{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@siva los cambios de la V.2.1 fueron: \n",
    "-Se elimina el filtrado que se quedó en el Timestamp Eliminator.\n",
    "-Se actualizan los códigos de pesajes para que trabajen con el formato actual (07Oct24) de los archivos\n",
    "-Se actualizan los códigos de comparación estadística para que trabajen con una sola fecha para futuras medicioens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de Entradas\n",
    "\n",
    "1. [Timestamp Eliminator](#1)\n",
    "2. [Curl File Generator](#2)\n",
    "3. [Moduli Curl File Generator](#3)\n",
    "4. [General Velocity File Generator](#4)\n",
    "5. [Velocity Generator by Triad](#5)\n",
    "6. [Vorticity File Generator](#6)\n",
    "7. [Helicity Generator by Triads](#7)\n",
    "8. [First Integral of Helicity Generator](#8)\n",
    "9. [First Integral of Absolute Values of Helicity Generator](#9)\n",
    "10. [General Enstrophy File Generator](#10)\n",
    "11. [First Integral of Enstrophy](#11)\n",
    "12. [First Integral Moduli Baseline vs Experimental Color](#12)\n",
    "13. [Second Integral of Absolute Helicity Baseline vs Experimental Color](#13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este primer código se encarga de determinar la ruta en la que se encuentran los archivos que se van a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = r\"D:\\Code\\Hypatia\\Fechas\\2025\\25Ene25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def buscar_labs_en_carpetas(root_folder):\n",
    "    # Expresión regular para encontrar laboratorios en formato \"Lab_algo\" o \"LabAlgo\",\n",
    "    # y asegurar que la carpeta comience con un número\n",
    "    patron = re.compile(r\"^\\d.*(Lab(?:_|)\\w+)$\")\n",
    "    labs_encontrados = []\n",
    "\n",
    "    for carpeta in os.listdir(root_folder):\n",
    "        ruta_completa = os.path.join(root_folder, carpeta)\n",
    "        if os.path.isdir(ruta_completa):\n",
    "            # Buscar coincidencia con la expresión regular\n",
    "            coincidencia = patron.search(carpeta)\n",
    "            if coincidencia:\n",
    "                # Guardar el nombre del laboratorio tal como se encuentra\n",
    "                labs_encontrados.append(coincidencia.group(1))\n",
    "\n",
    "    return labs_encontrados\n",
    "\n",
    "labs = buscar_labs_en_carpetas(root_folder)\n",
    "print(\"Laboratorios encontrados:\", labs)\n",
    "\n",
    "if len(labs) > 1:\n",
    "    chooser = True\n",
    "else:\n",
    "    chooser = False\n",
    "\n",
    "triad_names = {\n",
    "    \"FRT\": \"Frontal-Derecho-Superior\",  \n",
    "    \"PLB\": \"Trasero-Izquierdo-Inferior\",  \n",
    "    \"FLT\": \"Frontal-Izquierdo-Superior\",  \n",
    "    \"PRB\": \"Trasero-Derecho-Inferior\",  \n",
    "    \"FRB\": \"Frontal-Derecho-Inferior\",  \n",
    "    \"PLT\": \"Trasero-Izquierdo-Superior\",  \n",
    "    \"FLB\": \"Frontal-Izquierdo-Inferior\",  \n",
    "    \"PRT\": \"Trasero-Derecho-Superior\",  \n",
    "    \"RTB\": \"Derecho-Superior-Inferior\",  \n",
    "    \"FLP\": \"Frontal-Izquierdo-Trasero\",  \n",
    "    \"LTB\": \"Izquierdo-Superior-Inferior\",  \n",
    "    \"FRP\": \"Frontal-Derecho-Trasero\"\n",
    "}\n",
    "\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Define the triads\n",
    "triads = [\n",
    "    (0, 2, 4),  # T1 (FRONTAL, DERECHA, SUPERIOR)\n",
    "    (1, 3, 5),  # T2 (TRASERO, IZQUIERDA, INFERIOR)\n",
    "    (0, 3, 4),  # T3 (FRONTAL, IZQUIERDA, SUPERIOR)\n",
    "    (1, 2, 5),  # T4 (TRASERO, DERECHA, INFERIOR)\n",
    "    (0, 2, 5),  # T5 (FRONTAL, DERECHA, INFERIOR)\n",
    "    (1, 3, 4),  # T6 (TRASERO, IZQUIERDA, SUPERIOR)\n",
    "    (0, 3, 5),  # T7 (FRONTAL, IZQUIERDA, INFERIOR)\n",
    "    (1, 2, 4),  # T8 (TRASERO, DERECHA, SUPERIOR)\n",
    "    (2, 4, 5),  # T9 (DERECHA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 3),  # T10 (IZQUIERDA, FRONTAL, TRASERO)\n",
    "    (3, 4, 5),  # T11 (IZQUIERDA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 2)   # T12 (DERECHA, FRONTAL, TRASERO)\n",
    "]\n",
    "\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednaranja.txt\"]\n",
    "\n",
    "color_map = {\n",
    "    \"amarilla\": \"yellow\",\n",
    "    \"roja\": \"red\",\n",
    "    \"azul\": \"blue\",\n",
    "    \"verde\": \"green\",\n",
    "    \"morada\": \"purple\",\n",
    "    \"negra\": \"black\",\n",
    "    \"naranja\": \"orange\"\n",
    "}\n",
    "\n",
    "colores = ['Amarilla', 'Roja', 'Verde', 'Morada', 'Azul', 'Negra', 'Naranja']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Second Timestamp Eliminator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Función para actualizar el formato de las líneas\n",
    "def update_file_format(filepath):\n",
    "    updated_lines = []\n",
    "    print(f\"Actualizando formato del archivo: {filepath}\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            #print(f\"Procesando línea: {line.strip()}\")  # Mostrar cada línea procesada\n",
    "            # Verificar si la línea contiene dos estampas de tiempo\n",
    "            if re.match(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> \\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', line):\n",
    "                # Extraer y conservar sólo desde la segunda estampa de tiempo hasta el final de la línea\n",
    "                new_line = re.sub(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', '', line.strip())\n",
    "                #print(f\"Formato actualizado: {new_line}\")  # Mostrar la línea actualizada\n",
    "                updated_lines.append(new_line)\n",
    "            else:\n",
    "                updated_lines.append(line.strip())\n",
    "    \n",
    "    # Sobrescribir el archivo con el nuevo formato\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        for line in updated_lines:\n",
    "            file.write(line + '\\n')\n",
    "    print(f\"Formato actualizado en: {filepath}\")\n",
    "\n",
    "# Función para escanear un directorio y procesar archivos con sufijos específicos\n",
    "def scan_directory(root_folder, file_suffixes):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        dirs[:] = [d for d in dirs if d not in [\"Data Analysis\", \"Programas\"]]  # Excluir directorios\n",
    "        for filename in files:\n",
    "            if any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                update_file_format(file_path)  # Actualizar formato del archivo\n",
    "\n",
    "# Realiza la búsqueda y procesa los archivos\n",
    "scan_directory(root_folder, file_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Curl File Generator}\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee los datos del sensor desde un archivo.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_curl(data):\n",
    "    \"\"\"\n",
    "    Calcula la rotacional de los datos proporcionados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Dx = Dy = Dz = 1.0\n",
    "        curl_x = (data[:, 1] - data[:, 0]) / Dx  # (Trasero - Frontal) / Dx\n",
    "        curl_y = (data[:, 3] - data[:, 2]) / Dy  # (Derecha - Izquierda) / Dy\n",
    "        curl_z = (data[:, 5] - data[:, 4]) / Dz  # (Inferior - Superior) / Dz\n",
    "        return np.array([curl_x, curl_y, curl_z]).T\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular la rotacional: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def generate_new_timestamps(timestamps):\n",
    "    \"\"\"\n",
    "    Genera nuevos timestamps comenzando desde 00:00:00.000.\n",
    "    \"\"\"\n",
    "    base_time = datetime.strptime(\"00:00:00.000\", '%H:%M:%S.%f')\n",
    "    new_timestamps = []\n",
    "    initial_time = datetime.strptime(timestamps[0], '%H:%M:%S.%f')\n",
    "    \n",
    "    for ts in timestamps:\n",
    "        current_time = datetime.strptime(ts, '%H:%M:%S.%f')\n",
    "        delta = current_time - initial_time\n",
    "        new_time = base_time + delta\n",
    "        new_timestamps.append(new_time.strftime('%H:%M:%S.%f')[:-3])  # Mantener el formato exacto\n",
    "    return new_timestamps\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    \"\"\"\n",
    "    Encuentra la carpeta del experimento según el nombre del laboratorio y el turno.\n",
    "    \"\"\"\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')  # Regex para detectar fechas\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder)\n",
    "    return None\n",
    "\n",
    "def process_files(input_folder, output_folder, file_suffixes=None):\n",
    "    \"\"\"\n",
    "    Procesa los archivos dentro de la carpeta de entrada y genera archivos con los cálculos de rotacional.\n",
    "    \"\"\"\n",
    "    generated_files = []\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        # Excluir carpetas no deseadas\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]\n",
    "        \n",
    "        for filename in files:\n",
    "            if \"00\" in filename:  # Saltar archivos con \"00\" en el nombre\n",
    "                continue\n",
    "            if file_suffixes and not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                # Leer datos del archivo\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                if data.size == 0:\n",
    "                    continue\n",
    "                # Calcular rotacional\n",
    "                curls = calculate_curl(data)\n",
    "                if curls.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Generar nuevos timestamps comenzando desde 00:00:00.000\n",
    "                adjusted_timestamps = generate_new_timestamps(timestamps)\n",
    "\n",
    "                # Generar el nombre del archivo de salida\n",
    "                output_filename = f\"curl_{filename}\"\n",
    "                output_file_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "                # Guardar el archivo con los nuevos datos\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    for i, curl in enumerate(curls):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> ({curl[0]:.10f}, {curl[1]:.10f}, {curl[2]:.10f})\\n\")\n",
    "                generated_files.append(output_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar el archivo {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "def process_lab(root_folder, lab_name):\n",
    "    \"\"\"\n",
    "    Procesa los archivos de un laboratorio específico.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró la carpeta de experimentos para {lab_name}, turno {shift_name}.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Procesando directorio: {experiment_folder}\")\n",
    "\n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Baseline\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        file_suffixes_baseline = [\"negra.txt\"]  # Procesar solo archivos \"*negra.txt\"\n",
    "        generated_files = process_files(experiment_folder, output_folder, file_suffixes_baseline)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        filtered_suffixes = [suffix for suffix in file_suffixes if suffix != \"mednegra.txt\"]\n",
    "        generated_files = process_files(experiment_folder, output_folder, filtered_suffixes)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Procesar cada laboratorio\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from textwrap import wrap\n",
    "\n",
    "\n",
    "\n",
    "# Asignación de colores según sufijo\n",
    "color_map = {\n",
    "    \"medroja\": \"red\",\n",
    "    \"medmorada\": \"purple\",\n",
    "    \"medazul\": \"blue\",\n",
    "    \"medverde\": \"green\",\n",
    "    \"medamarilla\": \"yellow\",\n",
    "    \"mednaranja\": \"orange\",\n",
    "    \"mednegra\": \"black\"\n",
    "}\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee los datos del sensor desde un archivo.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1 and len(parts[1].strip('()').split(', ')) > 0:\n",
    "                    timestamps.append(parts[0])\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        # Validar dimensiones consistentes\n",
    "        if len(timestamps) != len(data):\n",
    "            min_length = min(len(timestamps), len(data))\n",
    "            timestamps = timestamps[:min_length]\n",
    "            data = data[:min_length]\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def assign_color(filename, color_map):\n",
    "    \"\"\"\n",
    "    Asigna el color basado en el sufijo del archivo.\n",
    "    \"\"\"\n",
    "    import re  # Importar aquí en caso de problemas contextuales\n",
    "    try:\n",
    "        match = re.search(r\"med[a-z]+\", filename)  # Busca el sufijo 'med{color}'\n",
    "        if match:\n",
    "            suffix = match.group()  # Extrae el sufijo completo como 'mednegra', 'medroja', etc.\n",
    "            if suffix in color_map:\n",
    "                return color_map[suffix]  # Devuelve el color correspondiente\n",
    "            else:\n",
    "                print(f\"Sufijo no reconocido en el archivo: {filename}\")\n",
    "                return \"black\"\n",
    "        else:\n",
    "            print(f\"No se pudo extraer el sufijo del archivo: {filename}\")\n",
    "            return \"black\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error asignando color para el archivo {filename}: {e}\")\n",
    "        return \"black\"\n",
    "\n",
    "def detect_abrupt_changes(curls):\n",
    "    derivatives = np.diff(curls, axis=0)\n",
    "    max_derivative = np.max(np.abs(derivatives), axis=0)\n",
    "    num_peaks = np.sum(np.any(np.abs(derivatives) > 0.1 * np.ptp(curls, axis=0), axis=1))\n",
    "    return max_derivative, num_peaks\n",
    "\n",
    "def calculate_distances(curls_baseline, curls_mei):\n",
    "    distances = np.linalg.norm(curls_mei[:, None, :] - curls_baseline[None, :, :], axis=2)\n",
    "    avg_distance = np.mean(distances)\n",
    "    max_distance = np.max(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    return avg_distance, max_distance, std_distance\n",
    "\n",
    "def calculate_trajectory_similarity(curls_baseline, curls_mei):\n",
    "    baseline_norm = np.linalg.norm(curls_baseline, axis=1, keepdims=True)\n",
    "    mei_norm = np.linalg.norm(curls_mei, axis=1, keepdims=True)\n",
    "    baseline_normed = curls_baseline / baseline_norm\n",
    "    mei_normed = curls_mei / mei_norm\n",
    "    similarities = np.sum(baseline_normed[:, None, :] * mei_normed[None, :, :], axis=2)\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return avg_similarity\n",
    "\n",
    "def visualize_time_series(curls_before, curls_mei, curls_after, timestamps_before, timestamps_mei, timestamps_after, lab_name, shift_name, output_folder, pair_name, mei_color):\n",
    "    \"\"\"\n",
    "    Genera gráficos de series de tiempo para comparar la línea base antes de la MEI, la MEI y la línea base después, \n",
    "    superpuestas en el mismo rango temporal.\n",
    "    \"\"\"\n",
    "    # Asegurar que los datos son bidimensionales, si no, crear arrays vacíos con la forma correcta\n",
    "    curls_before = curls_before if curls_before.ndim == 2 else np.zeros((0, 3))\n",
    "    curls_mei = curls_mei if curls_mei.ndim == 2 else np.zeros((0, 3))\n",
    "    curls_after = curls_after if curls_after.ndim == 2 else np.zeros((0, 3))\n",
    "\n",
    "    # Ajustar dimensiones si no coinciden\n",
    "    min_length_before = min(len(timestamps_before), len(curls_before))\n",
    "    min_length_mei = min(len(timestamps_mei), len(curls_mei))\n",
    "    min_length_after = min(len(timestamps_after), len(curls_after))\n",
    "\n",
    "    timestamps_before = timestamps_before[:min_length_before]\n",
    "    curls_before = curls_before[:min_length_before]\n",
    "\n",
    "    timestamps_mei = timestamps_mei[:min_length_mei]\n",
    "    curls_mei = curls_mei[:min_length_mei]\n",
    "\n",
    "    timestamps_after = timestamps_after[:min_length_after]\n",
    "    curls_after = curls_after[:min_length_after]\n",
    "\n",
    "    # Usar el rango temporal más amplio para alinear las series\n",
    "    time_indices = np.arange(max(len(timestamps_before), len(timestamps_mei), len(timestamps_after)))\n",
    "\n",
    "    # Componentes del rotacional\n",
    "    components = ['curl_x', 'curl_y', 'curl_z']\n",
    "\n",
    "    # Crear figura y ejes\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        # Superponer las tres series de tiempo\n",
    "        axes[i].plot(\n",
    "            time_indices[:len(curls_before)], curls_before[:, i], color=\"black\", label=f'Línea Base Antes ({component})', alpha=0.7\n",
    "        )\n",
    "        axes[i].plot(\n",
    "            time_indices[:len(curls_mei)], curls_mei[:, i], color=mei_color, label=f'MEI ({component})', alpha=0.7\n",
    "        )\n",
    "        axes[i].plot(\n",
    "            time_indices[:len(curls_after)], curls_after[:, i], color=\"gray\", label=f'Línea Base Después ({component})', alpha=0.7\n",
    "        )\n",
    "\n",
    "        # Configuración de ejes\n",
    "        axes[i].set_ylabel(f\"{component}\")\n",
    "        axes[i].legend(loc='upper right')\n",
    "\n",
    "    # Configuración del eje x\n",
    "    axes[-1].set_xlabel(\"Índice Temporal\")\n",
    "\n",
    "    # Títulos y diseño\n",
    "    fig.suptitle(f\"Comparativa de Series de Tiempo - {lab_name} ({shift_name})\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar la figura\n",
    "    output_path = os.path.join(output_folder, f\"{pair_name}_time_series_full_comparison.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def detect_abrupt_changes(curls):\n",
    "    if curls.size == 0:\n",
    "        print(\"Advertencia: Datos vacíos proporcionados a detect_abrupt_changes.\")\n",
    "        return np.array([]), 0\n",
    "    derivatives = np.diff(curls, axis=0)\n",
    "    max_derivative = np.max(np.abs(derivatives), axis=0)\n",
    "    num_peaks = np.sum(np.any(np.abs(derivatives) > 0.1 * np.ptp(curls, axis=0), axis=1))\n",
    "    return max_derivative, num_peaks\n",
    "\n",
    "def calculate_persistent_homology(curls, output_folder, pair_name, description):\n",
    "    \"\"\"\n",
    "    Calcula la homología persistente de los datos y genera un diagrama de persistencia.\n",
    "\n",
    "    Parámetros:\n",
    "    - curls: Datos del espacio de fases para análisis.\n",
    "    - output_folder: Carpeta donde se guardará el diagrama de persistencia.\n",
    "    - pair_name: Nombre del par de comparación.\n",
    "    - description: Descripción del conjunto analizado (e.g., \"Baseline\", \"MEI\").\n",
    "    \"\"\"\n",
    "    try:\n",
    "        diagrams = ripser(curls)['dgms']\n",
    "        h0_count = len(diagrams[0])  # Número de componentes conexas\n",
    "        h1_count = len(diagrams[1])  # Número de ciclos\n",
    "        return diagrams, h0_count, h1_count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular homología persistente para {pair_name} ({description}): {e}\")\n",
    "        return None, 0, 0\n",
    "\n",
    "def calculate_comparative_persistence(curls_baseline, curls_mei, output_folder, pair_name):\n",
    "    \"\"\"\n",
    "    Genera diagramas comparativos de persistencia entre Línea Base y MEI.\n",
    "\n",
    "    Parámetros:\n",
    "    - curls_baseline: Datos de la Línea Base.\n",
    "    - curls_mei: Datos de la Medición Experimental.\n",
    "    - output_folder: Carpeta donde se guardará el diagrama de comparación.\n",
    "    - pair_name: Nombre del par de comparación.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calcular homología persistente para cada conjunto\n",
    "        diagrams_baseline, h0_baseline, h1_baseline = calculate_persistent_homology(curls_baseline, output_folder, pair_name, \"Baseline\")\n",
    "        diagrams_mei, h0_mei, h1_mei = calculate_persistent_homology(curls_mei, output_folder, pair_name, \"MEI\")\n",
    "\n",
    "        if diagrams_baseline is None or diagrams_mei is None:\n",
    "            return 0, 0, 0, 0\n",
    "\n",
    "        # Crear el grid comparativo\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True, sharex=True)\n",
    "\n",
    "        # Graficar diagramas de persistencia\n",
    "        plot_diagrams(diagrams_baseline, ax=axes[0], show=False)\n",
    "        axes[0].set_title(\"Baseline\")\n",
    "        \n",
    "        plot_diagrams(diagrams_mei, ax=axes[1], show=False)\n",
    "        axes[1].set_title(\"MEI\")\n",
    "\n",
    "        # Obtener límites comunes para ambos ejes\n",
    "        x_min = min(axes[0].get_xlim()[0], axes[1].get_xlim()[0])\n",
    "        x_max = max(axes[0].get_xlim()[1], axes[1].get_xlim()[1])\n",
    "        y_min = min(axes[0].get_ylim()[0], axes[1].get_ylim()[0])\n",
    "        y_max = max(axes[0].get_ylim()[1], axes[1].get_ylim()[1])\n",
    "\n",
    "        # Ajustar límites en ambos ejes\n",
    "        for ax in axes:\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "\n",
    "        # Configuración y guardado\n",
    "        fig.suptitle(f\"Comparación de Diagramas de Persistencia ({pair_name})\")\n",
    "        comparison_path = os.path.join(output_folder, f\"{pair_name}_Comparative_Persistence.png\")\n",
    "        plt.savefig(comparison_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return h0_baseline, h1_baseline, h0_mei, h1_mei\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular homología persistente comparativa para {pair_name}: {e}\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "\n",
    "def hierarchical_analysis(curls_baseline, curls_mei, timestamps, lab_name, shift_name, output_folder, pair_name, baseline_color, mei_color, curls_before=None, curls_after=None, timestamps_before=None, timestamps_after=None):\n",
    "    \"\"\"\n",
    "    Realiza un análisis jerárquico y escribe la conclusión directamente en el archivo de resultados.\n",
    "    \"\"\"\n",
    "    if curls_baseline.size == 0 or curls_mei.size == 0:\n",
    "        print(f\"Advertencia: Datos vacíos detectados en {pair_name}. Saltando análisis.\")\n",
    "        return\n",
    "\n",
    "    result_path = os.path.join(output_folder, f\"{pair_name}_Results.txt\")\n",
    "    conclusion = \"Prueba no concluyente, realizar más análisis\"  # Conclusión predeterminada\n",
    "\n",
    "    with open(result_path, \"w\") as f:\n",
    "\n",
    "        # Nivel 4: Histograma Temporalizados\n",
    "        f.write(\"Nivel 1: Analisis de Series de Tiempo.\\n\")\n",
    "        if curls_before is not None and curls_after is not None:\n",
    "            visualize_time_series(curls_before, curls_mei, curls_after, timestamps_before, timestamps, timestamps_after, lab_name, shift_name, output_folder, pair_name, mei_color)\n",
    "\n",
    "        # Nivel 2: Cambios abruptos\n",
    "        max_derivative, num_peaks = detect_abrupt_changes(curls_mei)\n",
    "        f.write(f\"Nivel 2: Max derivative: {max_derivative}, Num peaks: {num_peaks}\\n\")\n",
    "        if num_peaks < 10:\n",
    "            conclusion = \"No hubo cambio significativo\"\n",
    "            f.write(f\"Conclusión: {conclusion}\\n\")\n",
    "            return\n",
    "        \n",
    "        # Nivel 3: Distancias temporalizadas\n",
    "        avg_distance, max_distance, std_distance = calculate_distances(curls_baseline, curls_mei)\n",
    "        f.write(f\"Nivel 3: Avg distance: {avg_distance}, Max distance: {max_distance}, Std distance: {std_distance}\\n\")\n",
    "        if avg_distance < 0.05:\n",
    "            conclusion = \"No hubo cambio significativo\"\n",
    "            f.write(f\"Conclusión: {conclusion}\\n\")\n",
    "            return\n",
    "        \n",
    "        # Nivel 4: Similitud de trayectorias\n",
    "        avg_similarity = calculate_trajectory_similarity(curls_baseline, curls_mei)\n",
    "        f.write(f\"Nivel 4: Avg trajectory similarity: {avg_similarity}\\n\")\n",
    "        if avg_similarity > 0.9:\n",
    "            conclusion = \"No hubo cambio significativo\"\n",
    "            f.write(f\"Conclusión: {conclusion}\\n\")\n",
    "            return\n",
    "        \n",
    "        # Nivel 5: Homología persistente\n",
    "        f.write(\"Nivel 5: Homología persistente.\\n\")\n",
    "        h0_baseline, h1_baseline, h0_mei, h1_mei = calculate_comparative_persistence(curls_baseline, curls_mei, output_folder, pair_name)\n",
    "\n",
    "        # Guardar los resultados en el archivo\n",
    "        f.write(f\"Nivel 5: H0 (LB - componentes conexas): {h0_baseline}, H1 (LB - ciclos): {h1_baseline}\\n\")\n",
    "        f.write(f\"Nivel 5: H0 (MEI - componentes conexas): {h0_mei}, H1 (MEI - ciclos): {h1_mei}\\n\")\n",
    "\n",
    "        # Determinar conclusión final\n",
    "        if h1_mei > 1500:\n",
    "            conclusion = \"Si hubo un cambio significativo\"\n",
    "        elif h1_mei > 1000:\n",
    "            conclusion = \"Hubo un cambio significativo ligero\"\n",
    "        elif h1_mei <= 1000:\n",
    "            conclusion = \"No hubo cambio significativo\"\n",
    "\n",
    "        # Escribir conclusión al archivo\n",
    "        f.write(f\"Conclusión: {conclusion}\\n\")\n",
    "\n",
    "        \n",
    "def consolidate_results(root_folder, labs):\n",
    "    \"\"\"\n",
    "    Barre todos los resultados generados en los laboratorios y consolida en un archivo único.\n",
    "    \"\"\"\n",
    "    consolidated_file = os.path.join(root_folder, \"Consolidated_Results.txt\")\n",
    "    \n",
    "    with open(consolidated_file, \"w\") as output_file:\n",
    "        for lab_name in labs:\n",
    "            output_file.write(f\"Laboratorio: {lab_name}\\n\")\n",
    "            output_file.write(\"=\" * 50 + \"\\n\")\n",
    "            \n",
    "            for shift in ['V', 'M']:\n",
    "                shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "                experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "                \n",
    "                if not experiment_folder:\n",
    "                    output_file.write(f\"No se encontraron experimentos para {lab_name}, turno {shift_name}.\\n\")\n",
    "                    continue\n",
    "                \n",
    "                results_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Topological Data Analysis\")\n",
    "                if not os.path.exists(results_folder):\n",
    "                    output_file.write(f\"No se encontró la carpeta de resultados para {lab_name}, turno {shift_name}.\\n\")\n",
    "                    continue\n",
    "                \n",
    "                # Leer todos los archivos de resultados\n",
    "                result_files = sorted([f for f in os.listdir(results_folder) if f.endswith(\"_Results.txt\")])\n",
    "                if not result_files:\n",
    "                    output_file.write(f\"No se encontraron archivos de resultados en {results_folder}.\\n\")\n",
    "                    continue\n",
    "                \n",
    "                output_file.write(f\"Turno: {shift_name}\\n\")\n",
    "                output_file.write(\"-\" * 50 + \"\\n\")\n",
    "                for result_file in result_files:\n",
    "                    result_path = os.path.join(results_folder, result_file)\n",
    "                    with open(result_path, \"r\") as rf:\n",
    "                        output_file.write(f\"Resultados de {result_file}:\\n\")\n",
    "                        output_file.write(rf.read() + \"\\n\")\n",
    "                        output_file.write(\"-\" * 50 + \"\\n\")\n",
    "            \n",
    "            output_file.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Resultados consolidados en: {consolidated_file}\")\n",
    "\n",
    "def process_lab(root_folder, lab_name):\n",
    "    \"\"\"\n",
    "    Procesa los datos de un laboratorio específico para turnos matutino y vespertino.\n",
    "    Realiza análisis jerárquicos hasta el nivel requerido, generando resultados y gráficos según sea necesario.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró la carpeta de experimentos para {lab_name}, turno {shift_name}.\")\n",
    "            continue\n",
    "\n",
    "        baseline_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Baseline\")\n",
    "        mei_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Experimental_Color\")\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Topological Data Analysis\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(baseline_folder) or not os.path.exists(mei_folder):\n",
    "            print(f\"Carpetas faltantes en {experiment_folder}.\")\n",
    "            continue\n",
    "\n",
    "        baseline_files = sorted([f for f in os.listdir(baseline_folder) if \"curl\" in f])\n",
    "        mei_files = sorted([f for f in os.listdir(mei_folder) if \"curl\" in f])\n",
    "\n",
    "        # Nivel 1-5: Generar combinaciones y procesar todos los niveles para cada archivo MEI\n",
    "        for i, mei_file in enumerate(mei_files):\n",
    "            timestamps_mei, curls_mei = read_sensor_data(os.path.join(mei_folder, mei_file))\n",
    "            mei_color = assign_color(mei_file, color_map)\n",
    "\n",
    "            # Comparar con `Baseline n` y `Baseline n+1`\n",
    "            for baseline_index in [i, i + 1]:\n",
    "                if baseline_index >= len(baseline_files):\n",
    "                    continue\n",
    "\n",
    "                baseline_file = baseline_files[baseline_index]\n",
    "                timestamps_baseline, curls_baseline = read_sensor_data(os.path.join(baseline_folder, baseline_file))\n",
    "                pair_name = f\"{baseline_file.split('.')[0]}_VS_{mei_file.split('.')[0]}\"\n",
    "\n",
    "                # Nivel 1: Visualización de series de tiempo\n",
    "                visualize_time_series(\n",
    "                    curls_baseline if baseline_index == i else np.array([]),\n",
    "                    curls_mei,\n",
    "                    curls_baseline if baseline_index == i + 1 else np.array([]),\n",
    "                    timestamps_baseline if baseline_index == i else [],\n",
    "                    timestamps_mei,\n",
    "                    timestamps_baseline if baseline_index == i + 1 else [],\n",
    "                    lab_name,\n",
    "                    shift_name,\n",
    "                    output_folder,\n",
    "                    pair_name,\n",
    "                    mei_color,\n",
    "                )\n",
    "\n",
    "                # Nivel 2-4: Análisis jerárquico\n",
    "                hierarchical_analysis(\n",
    "                    curls_baseline,\n",
    "                    curls_mei,\n",
    "                    timestamps_mei,\n",
    "                    lab_name,\n",
    "                    shift_name,\n",
    "                    output_folder,\n",
    "                    pair_name,\n",
    "                    \"black\",\n",
    "                    mei_color,\n",
    "                )\n",
    "\n",
    "                # Nivel 5: Análisis topológico\n",
    "                diagrams_baseline, h0_baseline, h1_baseline = calculate_persistent_homology(\n",
    "                    curls_baseline, output_folder, pair_name, \"Baseline\"\n",
    "                )\n",
    "                diagrams_mei, h0_mei, h1_mei = calculate_persistent_homology(\n",
    "                    curls_mei, output_folder, pair_name, \"MEI\"\n",
    "                )\n",
    "                calculate_comparative_persistence(curls_baseline, curls_mei, output_folder, pair_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Procesa cada laboratorio\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name)\n",
    "\n",
    "\n",
    "# Consolidar resultados\n",
    "consolidate_results(root_folder, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ripser import ripser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gudhi.wasserstein import wasserstein_distance\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee los datos del sensor desde un archivo, manejando posibles errores de lectura.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer {filepath}: {e}\")\n",
    "    return timestamps, np.array(data)\n",
    "\n",
    "def calculate_geometric_dispersion(curls):\n",
    "    \"\"\"\n",
    "    Calcula métricas de dispersión geométrica a partir de los datos.\n",
    "    \"\"\"\n",
    "    distances = pdist(curls)\n",
    "    mean_distance = np.mean(distances)\n",
    "    max_distance = np.max(distances)\n",
    "    return {\n",
    "        \"Mean_Distance\": mean_distance,\n",
    "        \"Max_Distance\": max_distance\n",
    "    }\n",
    "\n",
    "def calculate_trajectory_curvature(curls):\n",
    "    \"\"\"\n",
    "    Calcula la curvatura promedio de las trayectorias.\n",
    "    \"\"\"\n",
    "    deltas = np.diff(curls, axis=0)\n",
    "    norms = np.linalg.norm(deltas, axis=1)\n",
    "    curvatures = np.abs(np.diff(norms)) / (norms[:-1] + 1e-8)\n",
    "    return {\n",
    "        \"Average_Curvature\": np.mean(curvatures),\n",
    "        \"Max_Curvature\": np.max(curvatures)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def calculate_density_metrics(curls, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Calcula métricas de densidad usando DBSCAN.\n",
    "    \"\"\"\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(curls)\n",
    "    num_clusters = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n",
    "    noise_points = np.sum(clustering.labels_ == -1)\n",
    "    return {\n",
    "        \"Num_Clusters\": num_clusters,\n",
    "        \"Noise_Points\": noise_points\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_fractal_dimension(data, epsilon_range):\n",
    "    \"\"\"\n",
    "    Calcula la dimensión fractal de los datos mediante el método de conteo de cajas (box-counting).\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    dists = squareform(pdist(data))\n",
    "    counts = []\n",
    "    for epsilon in epsilon_range:\n",
    "        counts.append(np.sum(dists < epsilon))\n",
    "    coeffs = np.polyfit(np.log(epsilon_range), np.log(counts), 1)\n",
    "    return -coeffs[0]\n",
    "\n",
    "def calculate_persistence_entropy(diagram):\n",
    "    \"\"\"\n",
    "    Calcula la entropía de persistencia a partir de un diagrama de persistencia.\n",
    "    \"\"\"\n",
    "    lifetimes = [d[1] - d[0] for d in diagram if d[1] < np.inf]\n",
    "    if len(lifetimes) > 0:\n",
    "        probs = np.array(lifetimes) / sum(lifetimes)\n",
    "        return entropy(probs)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def calculate_average_curvature(curls):\n",
    "    \"\"\"\n",
    "    Calcula la curvatura promedio de las trayectorias.\n",
    "    \"\"\"\n",
    "    deltas = np.diff(curls, axis=0)\n",
    "    norms = np.linalg.norm(deltas, axis=1)\n",
    "    curvatures = np.abs(np.diff(norms)) / (norms[:-1] + 1e-8)\n",
    "    return np.mean(curvatures)\n",
    "\n",
    "def calculate_local_density(data, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Calcula la densidad local de puntos en el espacio usando DBSCAN.\n",
    "    \"\"\"\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(data)\n",
    "    return len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)\n",
    "\n",
    "\n",
    "def generate_persistence_diagram(diagrams, output_folder, filename):\n",
    "    \"\"\"\n",
    "    Genera y guarda un diagrama de persistencia en un archivo de imagen.\n",
    "    \n",
    "    Parameters:\n",
    "        diagrams (list): Diagramas de persistencia generados por Ripser.\n",
    "        output_folder (str): Carpeta donde se guardará la imagen.\n",
    "        filename (str): Nombre del archivo para el diagrama.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for dim, diagram in enumerate(diagrams):\n",
    "        plt.scatter(\n",
    "            [p[0] for p in diagram if p[1] < np.inf],\n",
    "            [p[1] for p in diagram if p[1] < np.inf],\n",
    "            label=f\"H{dim}\"\n",
    "        )\n",
    "    plt.xlabel(\"Birth\")\n",
    "    plt.ylabel(\"Death\")\n",
    "    plt.title(f\"Diagrama de Persistencia: {filename}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Crear la carpeta si no existe\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    diagram_path = os.path.join(output_folder, f\"{filename}_persistence_diagram.png\")\n",
    "    plt.savefig(diagram_path)\n",
    "    plt.close()\n",
    "    print(f\"Diagrama de persistencia guardado en: {diagram_path}\")\n",
    "\n",
    "\n",
    "def calculate_homology(curls):\n",
    "    \"\"\"\n",
    "    Calcula la homología persistente de los datos transformados exponencialmente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        transformed_curls = transform_exponential_abs(curls)\n",
    "        results = ripser(transformed_curls, maxdim=1, thresh=0.5)\n",
    "        diagrams = results.get('dgms', [])\n",
    "        \n",
    "        if not diagrams or len(diagrams[0]) == 0:\n",
    "            print(\"Diagramas de persistencia vacíos después de la transformación exponencial con valor absoluto.\")\n",
    "            return {\n",
    "                \"H0_count\": 0,\n",
    "                \"H1_count\": 0,\n",
    "                \"Max_Persistence_H0\": 0,\n",
    "                \"Max_Persistence_H1\": 0,\n",
    "                \"Diagrams\": diagrams\n",
    "            }\n",
    "\n",
    "        # Calcular métricas\n",
    "        h0 = len(diagrams[0])\n",
    "        h1 = len(diagrams[1]) if len(diagrams) > 1 else 0\n",
    "        max_persistence_h0 = max([d[1] - d[0] for d in diagrams[0] if d[1] < np.inf], default=0)\n",
    "        max_persistence_h1 = max([d[1] - d[0] for d in diagrams[1] if d[1] < np.inf], default=0)\n",
    "\n",
    "        return {\n",
    "            \"H0_count\": h0,\n",
    "            \"H1_count\": h1,\n",
    "            \"Max_Persistence_H0\": max_persistence_h0,\n",
    "            \"Max_Persistence_H1\": max_persistence_h1,\n",
    "            \"Diagrams\": diagrams\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular homología: {e}\")\n",
    "        return {\n",
    "            \"H0_count\": 0,\n",
    "            \"H1_count\": 0,\n",
    "            \"Max_Persistence_H0\": 0,\n",
    "            \"Max_Persistence_H1\": 0,\n",
    "            \"Diagrams\": []\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def transform_exponential_abs(curls):\n",
    "    \"\"\"\n",
    "    Aplica una transformación exponencial a cada entrada de curls.\n",
    "    \"\"\"\n",
    "    return np.exp(np.abs(curls))\n",
    "\n",
    "\n",
    "def calculate_wasserstein_distance(diagram1, diagram2):\n",
    "    \"\"\"\n",
    "    Calcula la distancia Wasserstein entre dos diagramas de persistencia.\n",
    "    \"\"\"\n",
    "    return wasserstein_distance(diagram1, diagram2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_baseline_file(filepath):\n",
    "    \"\"\"\n",
    "    Procesa un archivo de línea base para calcular métricas clave y homología persistente.\n",
    "    \"\"\"\n",
    "    timestamps, curls = read_sensor_data(filepath)\n",
    "    if curls.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Derivadas y picos\n",
    "    derivatives = np.abs(np.diff(curls, axis=0))\n",
    "    max_derivative = np.max(derivatives, axis=0)\n",
    "    median_derivative = np.median(derivatives, axis=0)\n",
    "    num_peaks = np.sum(derivatives > 0.1 * np.ptp(curls, axis=0), axis=0)\n",
    "\n",
    "    # Homología Persistente\n",
    "    homology_metrics = calculate_homology(curls)\n",
    "\n",
    "    diagrams = homology_metrics.pop(\"Diagrams\", [])\n",
    "\n",
    "    # Generar y guardar diagrama de persistencia\n",
    "    output_folder = os.path.join(os.path.dirname(filepath), \"Persistence_Diagrams\")\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    generate_persistence_diagram(diagrams, output_folder, filename)\n",
    "\n",
    "    # Métricas avanzadas\n",
    "    epsilon_range = np.logspace(-2, 0, 10)\n",
    "    fractal_dimension = calculate_fractal_dimension(curls, epsilon_range)\n",
    "\n",
    "    # Validar y calcular entropía de persistencia\n",
    "    if 'dgms' in homology_metrics:\n",
    "        diagrams = homology_metrics['dgms']\n",
    "        persistence_entropy_h0 = calculate_persistence_entropy(diagrams[0]) if len(diagrams) > 0 else 0\n",
    "        persistence_entropy_h1 = calculate_persistence_entropy(diagrams[1]) if len(diagrams) > 1 else 0\n",
    "    else:\n",
    "        persistence_entropy_h0 = 0\n",
    "        persistence_entropy_h1 = 0\n",
    "\n",
    "    # Calcular otras métricas avanzadas\n",
    "    average_curvature = calculate_average_curvature(curls)\n",
    "    geometric_dispersion = calculate_geometric_dispersion(curls)\n",
    "    curvature_metrics = calculate_trajectory_curvature(curls)\n",
    "    density_metrics = calculate_density_metrics(curls)\n",
    "\n",
    "    # Consolidar métricas utilizando `update`\n",
    "    metrics = {\n",
    "        \"Max_Derivative_X\": max_derivative[0],\n",
    "        \"Max_Derivative_Y\": max_derivative[1],\n",
    "        \"Max_Derivative_Z\": max_derivative[2],\n",
    "        \"Median_Derivative_X\": median_derivative[0],\n",
    "        \"Median_Derivative_Y\": median_derivative[1],\n",
    "        \"Median_Derivative_Z\": median_derivative[2],\n",
    "        \"Num_Peaks_X\": num_peaks[0],\n",
    "        \"Num_Peaks_Y\": num_peaks[1],\n",
    "        \"Num_Peaks_Z\": num_peaks[2],\n",
    "        \"Fractal_Dimension\": fractal_dimension,\n",
    "        \"Persistence_Entropy_H0\": persistence_entropy_h0,\n",
    "        \"Persistence_Entropy_H1\": persistence_entropy_h1,\n",
    "        \"Average_Curvature\": average_curvature\n",
    "    }\n",
    "    # Agregar los diccionarios adicionales\n",
    "    metrics.update(homology_metrics)\n",
    "    metrics.update(geometric_dispersion)\n",
    "    metrics.update(curvature_metrics)\n",
    "    metrics.update(density_metrics)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def process_mei_file(filepath):\n",
    "    \"\"\"\n",
    "    Procesa un archivos MEI para calcular métricas clave y homología persistente.\n",
    "    \"\"\"\n",
    "    timestamps, curls = read_sensor_data(filepath)\n",
    "    if curls.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Derivadas y picos\n",
    "    derivatives = np.abs(np.diff(curls, axis=0))\n",
    "    max_derivative = np.max(derivatives, axis=0)\n",
    "    median_derivative = np.median(derivatives, axis=0)\n",
    "    num_peaks = np.sum(derivatives > 0.1 * np.ptp(curls, axis=0), axis=0)\n",
    "\n",
    "    # Homología Persistente\n",
    "    homology_metrics = calculate_homology(curls)\n",
    "\n",
    "    diagrams = homology_metrics.pop(\"Diagrams\", [])\n",
    "\n",
    "    # Generar y guardar diagrama de persistencia\n",
    "    output_folder = os.path.join(os.path.dirname(filepath), \"Persistence_Diagrams\")\n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    generate_persistence_diagram(diagrams, output_folder, filename)\n",
    "\n",
    "    # Métricas avanzadas\n",
    "    epsilon_range = np.logspace(-2, 0, 10)\n",
    "    fractal_dimension = calculate_fractal_dimension(curls, epsilon_range)\n",
    "\n",
    "    # Validar y calcular entropía de persistencia\n",
    "    if 'dgms' in homology_metrics:\n",
    "        diagrams = homology_metrics['dgms']\n",
    "        persistence_entropy_h0 = calculate_persistence_entropy(diagrams[0]) if len(diagrams) > 0 else 0\n",
    "        persistence_entropy_h1 = calculate_persistence_entropy(diagrams[1]) if len(diagrams) > 1 else 0\n",
    "    else:\n",
    "        persistence_entropy_h0 = 0\n",
    "        persistence_entropy_h1 = 0\n",
    "\n",
    "    # Calcular otras métricas avanzadas\n",
    "    average_curvature = calculate_average_curvature(curls)\n",
    "    geometric_dispersion = calculate_geometric_dispersion(curls)\n",
    "    curvature_metrics = calculate_trajectory_curvature(curls)\n",
    "    density_metrics = calculate_density_metrics(curls)\n",
    "\n",
    "    # Consolidar métricas utilizando `update`\n",
    "    metrics = {\n",
    "        \"Max_Derivative_X\": max_derivative[0],\n",
    "        \"Max_Derivative_Y\": max_derivative[1],\n",
    "        \"Max_Derivative_Z\": max_derivative[2],\n",
    "        \"Median_Derivative_X\": median_derivative[0],\n",
    "        \"Median_Derivative_Y\": median_derivative[1],\n",
    "        \"Median_Derivative_Z\": median_derivative[2],\n",
    "        \"Num_Peaks_X\": num_peaks[0],\n",
    "        \"Num_Peaks_Y\": num_peaks[1],\n",
    "        \"Num_Peaks_Z\": num_peaks[2],\n",
    "        \"Fractal_Dimension\": fractal_dimension,\n",
    "        \"Persistence_Entropy_H0\": persistence_entropy_h0,\n",
    "        \"Persistence_Entropy_H1\": persistence_entropy_h1,\n",
    "        \"Average_Curvature\": average_curvature\n",
    "    }\n",
    "    # Agregar los diccionarios adicionales\n",
    "    metrics.update(homology_metrics)\n",
    "    metrics.update(geometric_dispersion)\n",
    "    metrics.update(curvature_metrics)\n",
    "    metrics.update(density_metrics)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def generate_distributions(metrics_list, output_folder, lab_name, shift_name):\n",
    "    \"\"\"\n",
    "    Genera distribuciones para las métricas por laboratorio y turno y las guarda como gráficos.\n",
    "    \"\"\"\n",
    "    if not metrics_list:\n",
    "        print(f\"No hay métricas para generar distribuciones en {lab_name}, turno {shift_name}.\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(metrics_list)\n",
    "    \n",
    "    # Seleccionar solo columnas numéricas\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    distribution_folder = os.path.join(output_folder, f\"Metric_Distributions_{lab_name}_{shift_name}\")\n",
    "    os.makedirs(distribution_folder, exist_ok=True)\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        df[column].plot(kind='hist', bins=20, alpha=0.75, title=f\"Distribución de {column} - {lab_name} ({shift_name})\")\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel(\"Frecuencia\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(distribution_folder, f\"{column}_distribution_{lab_name}_{shift_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"Distribuciones guardadas en: {distribution_folder}\")\n",
    "\n",
    "\n",
    "def calculate_local_fractal_dimension(data, epsilon_range):\n",
    "    \"\"\"\n",
    "    Calcula la dimensión fractal local para cada punto en los datos.\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "    # Calculamos las distancias pairwise\n",
    "    dists = squareform(pdist(data))\n",
    "\n",
    "    dimensions = []\n",
    "    for epsilon in epsilon_range:\n",
    "        # Contar cuántos puntos están dentro de cada epsilon para cada punto\n",
    "        counts = np.sum(dists < epsilon, axis=1)\n",
    "        \n",
    "        # Filtrar para asegurarnos de no tener valores vacíos o inválidos\n",
    "        valid_counts = counts[counts > 1]  # Excluir casos donde el conteo es 0 o 1 (sin sentido para log)\n",
    "\n",
    "        if len(valid_counts) == 0:  # Evitar errores si no hay valores válidos\n",
    "            continue\n",
    "        \n",
    "        # Ajustar una línea solo si hay suficientes valores válidos\n",
    "        try:\n",
    "            coeffs = np.polyfit(np.log(epsilon_range[:len(valid_counts)]), np.log(valid_counts), 1)\n",
    "            dimensions.append(-coeffs[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error en ajuste de polinomio para epsilon {epsilon}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Retornar la dimensión fractal promedio\n",
    "    if len(dimensions) > 0:\n",
    "        return np.mean(dimensions)\n",
    "    else:\n",
    "        print(\"No se pudo calcular la dimensión fractal local: todos los valores fueron inválidos.\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "def process_lab(root_folder, lab_name):\n",
    "    \"\"\"\n",
    "    Procesa las líneas base de un laboratorio y genera tablas para cada archivo.\n",
    "    También consolida los resultados en un archivo CSV único por laboratorio y turno.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró la carpeta de experimentos para {lab_name}, turno {shift_name}.\")\n",
    "            continue\n",
    "\n",
    "        baseline_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Baseline\")\n",
    "        mei_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Experimental_Color\")\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Topological Data Analysis Baseline\")\n",
    "\n",
    "        if not os.path.exists(baseline_folder):\n",
    "            print(f\"No se encontró la carpeta de líneas base en {baseline_folder}.\")\n",
    "            continue\n",
    "        if not os.path.exists(mei_folder):\n",
    "            print(f\"No se encontró la carpeta experimental en {mei_folder}.\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"Procesando laboratorio: {lab_name}, turno: {shift_name}\")\n",
    "        baseline_files = sorted([f for f in os.listdir(baseline_folder) if \"curl\" in f])\n",
    "        mei_files = sorted([f for f in os.listdir(mei_folder) if \"curl\" in f])\n",
    "        all_metrics = []\n",
    "\n",
    "        for file in baseline_files:\n",
    "            filepath = os.path.join(baseline_folder, file)\n",
    "            metrics = process_baseline_file(filepath)\n",
    "            if metrics:\n",
    "                metrics['Lab'] = lab_name\n",
    "                metrics['Shift'] = shift_name\n",
    "                metrics['File'] = file\n",
    "                all_metrics.append(metrics)\n",
    "                print(f\"Métricas procesadas para {file}: {metrics}\")\n",
    "\n",
    "        for file in mei_files:\n",
    "            filepath = os.path.join(mei_folder, file)\n",
    "            metrics = process_mei_file(filepath)\n",
    "            if metrics:\n",
    "                metrics['Lab'] = lab_name\n",
    "                metrics['Shift'] = shift_name\n",
    "                metrics['File'] = file\n",
    "                all_metrics.append(metrics)\n",
    "                print(f\"Métricas procesadas para {file}: {metrics}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Guardar métricas consolidadas por laboratorio y turno\n",
    "        consolidated_csv_path = os.path.join(root_folder, f\"Consolidated_Metrics_{lab_name}_{shift_name}.csv\")\n",
    "        pd.DataFrame(all_metrics).to_csv(consolidated_csv_path, index=False)\n",
    "        print(f\"Métricas consolidadas guardadas en: {consolidated_csv_path}\")\n",
    "\n",
    "        # Generar distribuciones para las métricas por laboratorio y turno\n",
    "        generate_distributions(all_metrics, root_folder, lab_name, shift_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ejecutar para todos los laboratorios\n",
    "for lab in labs:\n",
    "    process_lab(root_folder, lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Moduli Curl File Generator}\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee los datos del sensor desde un archivo.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_magnitude(data):\n",
    "    \"\"\"\n",
    "    Calcula la magnitud de los datos proporcionados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #print(np.linalg.norm(data, axis=1))\n",
    "        return np.linalg.norm(data, axis=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al calcular la magnitud: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def generate_new_timestamps(timestamps):\n",
    "    \"\"\"\n",
    "    Genera nuevos timestamps comenzando desde 00:00:00.000.\n",
    "    \"\"\"\n",
    "    base_time = datetime.strptime(\"00:00:00.000\", '%H:%M:%S.%f')\n",
    "    new_timestamps = []\n",
    "    initial_time = datetime.strptime(timestamps[0], '%H:%M:%S.%f')\n",
    "    \n",
    "    for ts in timestamps:\n",
    "        current_time = datetime.strptime(ts, '%H:%M:%S.%f')\n",
    "        delta = current_time - initial_time\n",
    "        new_time = base_time + delta\n",
    "        new_timestamps.append(new_time.strftime('%H:%M:%S.%f')[:-3])  # Mantener el formato exacto\n",
    "    return new_timestamps\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    \"\"\"\n",
    "    Encuentra la carpeta del experimento según el nombre del laboratorio y el turno.\n",
    "    \"\"\"\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')  # Regex para detectar fechas\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder)\n",
    "    return None\n",
    "\n",
    "def process_files(input_folder, output_folder, file_suffixes=None):\n",
    "    \"\"\"\n",
    "    Procesa los archivos dentro de la carpeta de entrada y genera archivos con las magnitudes de rotacional.\n",
    "    \"\"\"\n",
    "    generated_files = []\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        # Excluir carpetas no deseadas\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]\n",
    "        \n",
    "        for filename in files:\n",
    "            if \"00\" in filename:  # Saltar archivos con \"00\" en el nombre\n",
    "                continue\n",
    "            if file_suffixes and not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                # Leer datos del archivo\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                if data.size == 0:\n",
    "                    continue\n",
    "                # Calcular magnitud de rotacional\n",
    "                magnitudes = calculate_magnitude(data)\n",
    "                if magnitudes.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Generar nuevos timestamps comenzando desde 00:00:00.000\n",
    "                adjusted_timestamps = generate_new_timestamps(timestamps)\n",
    "\n",
    "                # Generar el nombre del archivo de salida\n",
    "                magnitude_filename = f\"magnitude_{filename}\"\n",
    "                magnitude_file_path = os.path.join(output_folder, magnitude_filename)\n",
    "\n",
    "                # Guardar el archivo con los nuevos datos\n",
    "                with open(magnitude_file_path, \"w\") as f:\n",
    "                    for i, magnitude in enumerate(magnitudes):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> {magnitude:.10f}\\n\")\n",
    "                generated_files.append(magnitude_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar el archivo {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "def process_lab(root_folder, lab_name):\n",
    "    \"\"\"\n",
    "    Procesa los archivos de un laboratorio específico para ambos turnos: Matutino y Vespertino.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró la carpeta de experimentos para {lab_name}, turno {shift_name}.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Procesando directorio: {experiment_folder}\")\n",
    "\n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Moduli_Curl_Baseline\")\n",
    "        file_suffixes_baseline = [\"negra.txt\"]  # Procesar solo archivos \"*negra.txt\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        generated_files = process_files(experiment_folder, output_folder,file_suffixes_baseline)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Moduli_Curl_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        #file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\", \"mednaranja.txt\"]\n",
    "        generated_files = process_files(experiment_folder, output_folder, file_suffixes)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Procesar cada laboratorio\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{General Velocity File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee los datos del sensor desde un archivo.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])  # Guardar marca de tiempo\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_velocities(data, delta_t):\n",
    "    \"\"\"\n",
    "    Calcula las velocidades usando diferencias hacia adelante.\n",
    "    \"\"\"\n",
    "    velocities = (data[1:] - data[:-1]) / delta_t\n",
    "    return velocities\n",
    "\n",
    "def generate_new_timestamps(timestamps):\n",
    "    \"\"\"\n",
    "    Genera nuevos timestamps comenzando desde 00:00:00.000.\n",
    "    \"\"\"\n",
    "    base_time = datetime.strptime(\"00:00:00.000\", '%H:%M:%S.%f')\n",
    "    new_timestamps = []\n",
    "    initial_time = datetime.strptime(timestamps[0], '%H:%M:%S.%f')\n",
    "    \n",
    "    for ts in timestamps[:-1]:  # Excluir la última marca de tiempo\n",
    "        current_time = datetime.strptime(ts, '%H:%M:%S.%f')\n",
    "        delta = current_time - initial_time\n",
    "        new_time = base_time + delta\n",
    "        new_timestamps.append(new_time.strftime('%H:%M:%S.%f')[:-3])  # Mantener el formato exacto\n",
    "    return new_timestamps\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    \"\"\"\n",
    "    Encuentra la carpeta del experimento según el nombre del laboratorio y el turno.\n",
    "    \"\"\"\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder)\n",
    "    return None\n",
    "\n",
    "def process_files(input_folder, output_folder, delta_t, file_suffixes=None):\n",
    "    \"\"\"\n",
    "    Procesa los archivos dentro de la carpeta de entrada y genera archivos con las velocidades calculadas.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Asegurar que el directorio de salida exista\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        # Excluir carpetas que contienen 'Data Analysis'\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]\n",
    "        for filename in files:\n",
    "            if \"00\" in filename:  # Omitir archivos con \"00\" en el nombre\n",
    "                continue\n",
    "            if file_suffixes and not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                if data.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calcular velocidades\n",
    "                velocities = calculate_velocities(data, delta_t)\n",
    "                if velocities.size == 0:\n",
    "                    continue\n",
    "\n",
    "                # Generar nuevos timestamps comenzando desde 00:00:00.000\n",
    "                adjusted_timestamps = generate_new_timestamps(timestamps)\n",
    "\n",
    "                # Generar el nombre del archivo de salida\n",
    "                output_filename = f\"velocity_{filename}\"\n",
    "                output_file_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "                # Guardar el archivo con los nuevos datos\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    for i, velocity in enumerate(velocities):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> ({velocity[0]:.10f}, {velocity[1]:.10f}, {velocity[2]:.10f}, {velocity[3]:.10f}, {velocity[4]:.10f}, {velocity[5]:.10f})\\n\")\n",
    "                generated_files.append(output_filename)  # Agregar el nombre del archivo generado a la lista\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar el archivo {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def process_lab(root_folder, lab_name, delta_t):\n",
    "    \"\"\"\n",
    "    Procesa los archivos de un laboratorio específico para ambos turnos: Matutino y Vespertino.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, turno {shift_name}.\")\n",
    "            continue\n",
    "\n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Velocity_Baseline\")\n",
    "        file_suffixes_baseline = [\"negra.txt\"]  # Procesar solo archivos \"*negra.txt\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        generated_files = process_files(experiment_folder, output_folder, delta_t,file_suffixes_baseline)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Velocity_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        #file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednaranja.txt\"]\n",
    "        generated_files = process_files(experiment_folder, output_folder, delta_t, file_suffixes)\n",
    "        \n",
    "        print(f\"Archivos generados para {lab_name}, turno {shift_name} Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Paso de tiempo para el cálculo de velocidades\n",
    "delta_t = 0.01  # Paso de tiempo (segundos) entre cada muestra\n",
    "\n",
    "# Procesar cada laboratorio\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name, delta_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Velocity Generator by Triad}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de velocidades por triada .txt\n",
    "import os\n",
    "\n",
    "def extract_velocity_components(root_folder):\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        if \"Velocity_Baseline\" in root or \"Velocity_Experimental_Color\" in root:\n",
    "            # Usar la carpeta actual, sin agregar '_Component'\n",
    "            new_folder = root\n",
    "            \n",
    "            # Iterar sobre cada archivo en la carpeta original\n",
    "            for filename in files:\n",
    "                if filename.startswith(\"v\") and filename.endswith(\".txt\"):\n",
    "                    path = os.path.join(root, filename)\n",
    "                    \n",
    "                    with open(path, 'r') as file:\n",
    "                        file_content = file.readlines()  # Leer todo el contenido una vez\n",
    "                        \n",
    "                        # Generar archivos por cada triada y usando su código del diccionario\n",
    "                        for triad, code in triad_codes.items():\n",
    "                            triad_folder = os.path.join(new_folder, code)\n",
    "                            os.makedirs(triad_folder, exist_ok=True)  # Crear la carpeta de la triada si no existe\n",
    "                            new_path = os.path.join(triad_folder, f\"Velocity_{code}_\" + filename)\n",
    "                            \n",
    "                            with open(new_path, 'w') as triad_file:\n",
    "                                for line in file_content:\n",
    "                                    time_stamp, velocities = line.strip().split(' -> ')\n",
    "                                    v = eval(velocities)\n",
    "                                    selected_components = tuple(v[j] for j in triad)\n",
    "                                    triad_file.write(f\"{time_stamp} -> {selected_components}\\n\")\n",
    "                            \n",
    "                            # Almacenar solo el nombre de la carpeta de cabecera y los nombres de los archivos generados\n",
    "                            header_folder = os.path.basename(triad_folder)\n",
    "                            file_name = os.path.basename(new_path)\n",
    "                            generated_files.append(f\"{header_folder}/{file_name}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "# La ruta base donde están las carpetas de origen\n",
    "generated_files = extract_velocity_components(root_folder)\n",
    "\n",
    "# Imprimir los nombres de los archivos generados\n",
    "print(\"Generated files:\")\n",
    "for file in generated_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Vorticity File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorticidad .txt\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_velocity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])  # Guardar timestamps directamente del archivo\n",
    "                    velocity_components = parts[1].strip('()').split(', ')\n",
    "                    if len(velocity_components) == 6:  # Asegurarse de que haya 6 componentes\n",
    "                        data.append([float(v) for v in velocity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_vorticity(velocities, indices):\n",
    "    if velocities.ndim != 2 or velocities.shape[1] != 3:\n",
    "        print(f\"velocities.ndim: {velocities.ndim}, velocities.shape: {velocities.shape}\")\n",
    "        raise ValueError(\"El array de velocidades debe ser bidimensional con tres columnas.\")\n",
    "    \n",
    "    Vx, Vy, Vz = indices\n",
    "    dvz_dy = np.gradient(velocities[:, Vz], axis=0)\n",
    "    dvy_dz = np.gradient(velocities[:, Vy], axis=0)\n",
    "    dvx_dz = np.gradient(velocities[:, Vx], axis=0)\n",
    "    dvz_dx = np.gradient(velocities[:, Vz], axis=0)\n",
    "    dvy_dx = np.gradient(velocities[:, Vy], axis=0)\n",
    "    dvx_dy = np.gradient(velocities[:, Vx], axis=0)\n",
    "\n",
    "    omega_x = dvz_dy - dvy_dz\n",
    "    omega_y = dvx_dz - dvz_dx\n",
    "    omega_z = dvy_dx - dvx_dy\n",
    "\n",
    "    return np.array([omega_x, omega_y, omega_z]).T\n",
    "\n",
    "def save_vorticity_with_timestamps(timestamps, vorticities, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)  # Crear el directorio si no existe\n",
    "    print(f\"Saving vorticity data to: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            for ts, vort in zip(timestamps, vorticities):\n",
    "                formatted_output = f\"{ts} -> ({vort[0]:.10f}, {vort[1]:.10f}, {vort[2]:.10f})\\n\"\n",
    "                f.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files(input_folder, output_folder, file_suffixes, triads):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return []\n",
    "    \n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing file: {filepath}\")\n",
    "            timestamps, velocities = read_velocity_data(filepath)\n",
    "            \n",
    "            if velocities.ndim != 2 or velocities.shape[1] != 6:\n",
    "                print(f\"El archivo {filename} no contiene datos de velocidad válidos. Dimensiones: {velocities.shape}\")\n",
    "                continue\n",
    "            \n",
    "            for triad in triads:\n",
    "                if triad in triad_codes:\n",
    "                    code = triad_codes[triad]\n",
    "                    triad_folder = os.path.join(output_folder, code)\n",
    "                    os.makedirs(triad_folder, exist_ok=True)  # Crear carpeta para la triada si no existe\n",
    "                    velocities_triad = velocities[:, triad]\n",
    "                    \n",
    "                    print(f\"Calculating vorticity for triad {code}: {filename}\")\n",
    "                    vorticity = calculate_vorticity(velocities_triad, [0, 1, 2])\n",
    "                    \n",
    "                    output_file = f\"vorticity_{code}_{filename}\"\n",
    "                    save_vorticity_with_timestamps(timestamps, vorticity, os.path.join(triad_folder, output_file))\n",
    "                    \n",
    "                    generated_files.append(f\"{os.path.basename(triad_folder)}/{output_file}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab(root_folder, lab_name, file_suffixes, triads):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "\n",
    "        input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "        \n",
    "        # Procesar la línea base\n",
    "        input_folder = os.path.join(input_folder_base, \"Velocity_Baseline\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Vorticity_Baseline\")\n",
    "        file_suffixes_baseline = [\"negra.txt\"]  # Procesar solo archivos \"*negra.txt\"\n",
    "        generated_files = process_files(input_folder, output_folder, file_suffixes_baseline, triads)\n",
    "        print(f\"Generated vorticity files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        input_folder = os.path.join(input_folder_base, \"Velocity_Experimental_Color\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\")\n",
    "        generated_files = process_files(input_folder, output_folder, file_suffixes, triads)\n",
    "        print(f\"Generated vorticity files for {lab_name}, {shift_name} shift Experimental Color:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Define the root folder and suffixes\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name, file_suffixes, triads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Helicity Generator by Triads}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_velocity_data(filepath):\n",
    "    \"\"\"\n",
    "    Lee datos de velocidad desde un archivo.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    velocity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in velocity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_helicity(velocities, vorticities, indices, volume):\n",
    "    \"\"\"\n",
    "    Calcula la helicidad en base a las velocidades, vorticidades y volumen.\n",
    "    \"\"\"\n",
    "    selected_velocities = velocities[:, indices]\n",
    "    helicity = np.einsum('ij,ij->i', selected_velocities, vorticities) * volume\n",
    "    return helicity\n",
    "\n",
    "def format_and_save_helicities(timestamps, helicities, filename):\n",
    "    \"\"\"\n",
    "    Formatea y guarda los resultados de helicidad en un archivo.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, helicity in zip(timestamps, helicities):\n",
    "                formatted_output = f\"{timestamp} -> {helicity:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al escribir el archivo {filename}: {e}\")\n",
    "\n",
    "def process_files(input_folder_velocity, input_folder_vorticity, output_folder, file_suffixes, volume, triads):\n",
    "    \"\"\"\n",
    "    Procesa archivos de velocidad y vorticidad para calcular la helicidad y guardarla.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    generated_files = []\n",
    "    \n",
    "    for suffix in file_suffixes:\n",
    "        for filename in os.listdir(input_folder_velocity):\n",
    "            if filename.endswith(suffix):\n",
    "                base_filename = filename.split('.')[0]\n",
    "                filepath_velocity = os.path.join(input_folder_velocity, filename)\n",
    "                \n",
    "                for triad in triads:\n",
    "                    if triad in triad_codes:\n",
    "                        code = triad_codes[triad]\n",
    "                        triad_folder = os.path.join(input_folder_vorticity, code)\n",
    "                        filepath_vorticity = os.path.join(triad_folder, f\"vorticity_{code}_{base_filename}.txt\")\n",
    "                        \n",
    "                        timestamps, velocities = read_velocity_data(filepath_velocity)\n",
    "                        if velocities.size == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        _, vorticity = read_velocity_data(filepath_vorticity)\n",
    "\n",
    "                        if vorticity.size == 0:\n",
    "                            print(f\"Skipping file {filename} due to missing vorticity data for triad {code}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        helicity = calculate_helicity(velocities, vorticity, triad, volume)\n",
    "                        \n",
    "                        output_filename = f\"helicity_{code}_{base_filename}.txt\"\n",
    "                        output_path = os.path.join(output_folder, code, output_filename)\n",
    "                        \n",
    "                        format_and_save_helicities(timestamps, helicity, output_path)\n",
    "                        \n",
    "                        generated_files.append(f\"{os.path.basename(output_folder)}/{code}/{output_filename}\")\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    \"\"\"\n",
    "    Encuentra la carpeta de experimentos para un laboratorio y turno específico.\n",
    "    \"\"\"\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab(root_folder, lab_name, file_suffixes, volume, triads):\n",
    "    \"\"\"\n",
    "    Procesa los archivos de helicidad para un laboratorio específico.\n",
    "    \"\"\"\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "\n",
    "        input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "        \n",
    "        if not os.path.exists(input_folder_base):\n",
    "            print(f\"No se puede acceder al directorio: {input_folder_base}\")\n",
    "            continue\n",
    "\n",
    "        # Procesar la línea base\n",
    "        input_folder_velocity = os.path.join(input_folder_base, \"Velocity_Baseline\")\n",
    "        input_folder_vorticity = os.path.join(input_folder_base, \"Vorticity_Baseline\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Helicity_Baseline\")\n",
    "        file_suffixes_baseline = [\"negra.txt\"]  # Procesar solo archivos \"*negra.txt\"\n",
    "        generated_files = process_files(input_folder_velocity, input_folder_vorticity, output_folder,file_suffixes_baseline, volume, triads)\n",
    "        print(f\"Archivos de helicidad generados para {lab_name}, turno {shift_name} Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        input_folder_velocity = os.path.join(input_folder_base, \"Velocity_Experimental_Color\")\n",
    "        input_folder_vorticity = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Helicity_Experimental_Color\")\n",
    "        generated_files = process_files(input_folder_velocity, input_folder_vorticity, output_folder, file_suffixes, volume, triads)\n",
    "        print(f\"Archivos de helicidad generados para {lab_name}, turno {shift_name} Experimental Color:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Definir los sufijos de archivos para procesar\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "# Verificar y asignar volúmenes si existen los laboratorios\n",
    "volumes = {}\n",
    "if len(labs) > 0:\n",
    "    volumes[labs[0]] = 7.3195  # Volumen para Lab A (Lab_Betta)\n",
    "if len(labs) > 1:\n",
    "    volumes[labs[1]] = 6.8342  # Volumen para Lab B (Lab_Gamma)\n",
    "\n",
    "# Procesar los laboratorios si existen los volúmenes asignados\n",
    "for lab_name in labs:\n",
    "    if lab_name in volumes:  # Verificar que haya volumen asignado para el laboratorio\n",
    "        process_lab(root_folder, lab_name, file_suffixes, volumes[lab_name], triads)\n",
    "    else:\n",
    "        print(f\"No se encontró volumen para {lab_name}. Omitiendo laboratorio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Helicity Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral Helicidad .txt\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_file(file_path, output_file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "#@siva se retira la función que normaliza los datos\n",
    "    \n",
    "    accumulated_helicity = 0\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        time_stamp, value = line.split('->')\n",
    "        value = float(value.strip())\n",
    "        accumulated_helicity += value\n",
    "        results.append(f\"{time_stamp.strip()} -> {accumulated_helicity:.10f}\\n\")\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.writelines(results)\n",
    "\n",
    "def generate_accumulated_helicity(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_{filename}\"\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                process_file(file_path, output_file_path)\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar los acumulados de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Helicity_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_helicity(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated helicity files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar los acumulados de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Helicity_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_helicity(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated helicity files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes)\n",
    "\n",
    "print(\"Processing complete. All accumulated helicity results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Absolute Values of Helicity}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral absoluta helicidad .txt\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_file_absolute(file_path, output_file_path):\n",
    "    # Leer las líneas del archivo\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    #@siva se retira la función que normaliza los datos\n",
    "    \n",
    "    # Extraer los valores de helicidad y calcular la suma de los valores absolutos\n",
    "    absolute_helicity_sum = 0\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        time_stamp, value = line.split('->')\n",
    "        value = abs(float(value.strip()))  # Uso de valor absoluto\n",
    "        absolute_helicity_sum += value\n",
    "        results.append(f\"{time_stamp.strip()} -> {absolute_helicity_sum:.10f}\\n\")\n",
    "\n",
    "    # Escribir los resultados de la suma de valores absolutos en un nuevo archivo\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.writelines(results)\n",
    "\n",
    "def generate_absolute_helicity_sum(input_directory, output_directory):\n",
    "    # Crear el directorio de salida si no existe\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Leer todos los archivos en el directorio especificado\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_absolute_{filename}\"  # Cambio en el nombre del archivo de salida para reflejar los valores absolutos\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                # Procesar el archivo\n",
    "                process_file_absolute(file_path, output_file_path)\n",
    "                # Imprimir en consola el nombre del archivo generado\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_absolute_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar los acumulados absolutos de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Absolute_Helicity_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_absolute_helicity_sum(input_directory, output_directory)\n",
    "            print(f\"Generated absolute helicity sum files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar los acumulados absolutos de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Absolute_Helicity_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_absolute_helicity_sum(input_directory, output_directory)\n",
    "            print(f\"Generated absolute helicity sum files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_absolute_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes)\n",
    "\n",
    "print(\"Processing complete. All absolute helicity sum results are saved in their respective directories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{General Enstrophy File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enstrofía general .txt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_vorticity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    vorticity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in vorticity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_enstrophy(vorticities, volume):\n",
    "    #@siva se retira la función que normaliza los datos\n",
    "    # Enstrophy calculation: sum of the squares of vorticity components, multiplied by volume\n",
    "    enstrophy = np.sum(vorticities**2, axis=1) * volume\n",
    "    return enstrophy\n",
    "\n",
    "def format_and_save_enstrophies(timestamps, enstrophies, filename):\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, enstrophy in zip(timestamps, enstrophies):\n",
    "                formatted_output = f\"{timestamp} -> {enstrophy:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files_enstrophy(input_folder, output_folder, file_suffixes, volume):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for filename in files:\n",
    "            for file_suffix in file_suffixes:\n",
    "                if filename.endswith(file_suffix):\n",
    "                    base_filename = filename[:-len(file_suffix)]  # Correctly removing the suffix\n",
    "                    \n",
    "                    filepath_vorticity = os.path.join(root, filename)\n",
    "                    \n",
    "                    timestamps, vorticity = read_vorticity_data(filepath_vorticity)\n",
    "                    if vorticity.size == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    enstrophy = calculate_enstrophy(vorticity, volume)\n",
    "                    \n",
    "                    output_filename = f\"enstrophy_{base_filename}{file_suffix}\"\n",
    "                    output_filepath = os.path.join(output_folder, output_filename)\n",
    "                    \n",
    "                    format_and_save_enstrophies(timestamps, enstrophy, output_filepath)\n",
    "                    generated_files.append(f\"{os.path.basename(output_folder)}/{output_filename}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_enstrophy(root_folder, lab_name, shift, file_suffixes, volume, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar enstrofia de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Vorticity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Enstrophy_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generated_files = process_files_enstrophy(input_directory, output_directory, file_suffixes, volume)\n",
    "            print(f\"Generated enstrophy files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}:\")\n",
    "            for file in generated_files:\n",
    "                print(f\"  - {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar enstrofia de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Enstrophy_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generated_files = process_files_enstrophy(input_directory, output_directory, file_suffixes, volume)\n",
    "            print(f\"Generated enstrophy files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}:\")\n",
    "            for file in generated_files:\n",
    "                print(f\"  - {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "# Verificar y asignar volúmenes si existen los laboratorios\n",
    "volumes = {}\n",
    "if len(labs) > 0:\n",
    "    volumes[labs[0]] = 7.3195  # Volumen para Lab A (Lab_Betta)\n",
    "if len(labs) > 1:\n",
    "    volumes[labs[1]] = 6.8342  # Volumen para Lab B (Lab_Gamma)\n",
    "    \n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_enstrophy(root_folder, lab_name, shift, file_suffixes, volumes[lab_name], triad_codes)\n",
    "\n",
    "print(\"Processing complete. All enstrophy results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Enstrophy}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral Enstrofía\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_vorticity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    vorticity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in vorticity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_enstrophy(vorticities, volume):\n",
    "    # Enstrophy calculation: sum of the squares of vorticity components, multiplied by volume\n",
    "    enstrophy = np.sum(vorticities**2, axis=1) * volume\n",
    "    return enstrophy\n",
    "\n",
    "def format_and_save_enstrophies(timestamps, enstrophies, filename):\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, enstrophy in zip(timestamps, enstrophies):\n",
    "                formatted_output = f\"{timestamp} -> {enstrophy:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files_enstrophy(input_folder, output_folder, file_suffixes, volume):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        for file_suffix in file_suffixes:\n",
    "            if filename.endswith(file_suffix):\n",
    "                base_filename = filename[:-len(file_suffix)]  # Correctly removing the suffix\n",
    "                \n",
    "                filepath_vorticity = os.path.join(input_folder, filename)\n",
    "                \n",
    "                timestamps, vorticity = read_vorticity_data(filepath_vorticity)\n",
    "                if vorticity.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                enstrophy = calculate_enstrophy(vorticity, volume)\n",
    "                \n",
    "                output_filename = f\"enstrophy_{base_filename}{file_suffix}\"\n",
    "                output_filepath = os.path.join(output_folder, output_filename)\n",
    "                \n",
    "                format_and_save_enstrophies(timestamps, enstrophy, output_filepath)\n",
    "                generated_files.append(f\"{os.path.basename(output_folder)}/{output_filename}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def accumulate_enstrophy(file_path, output_file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        accumulated_enstrophy = 0\n",
    "        results = []\n",
    "        for line in lines:\n",
    "            time_stamp, value = line.split('->')\n",
    "            value = float(value.strip())\n",
    "            accumulated_enstrophy += value\n",
    "            results.append(f\"{time_stamp.strip()} -> {accumulated_enstrophy:.10f}\\n\")\n",
    "\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.writelines(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def generate_accumulated_enstrophy(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_{filename}\"\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                accumulate_enstrophy(file_path, output_file_path)\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_accumulated_enstrophy(root_folder, lab_name, shift, file_suffixes, volume, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar enstrofia acumulada de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Enstrophy_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Enstrophy_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_enstrophy(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated enstrophy files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar enstrofia acumulada de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Enstrophy_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Enstrophy_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_enstrophy(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated enstrophy files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "#file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "# Verificar y asignar volúmenes si existen los laboratorios\n",
    "volumes = {}\n",
    "if len(labs) > 0:\n",
    "    volumes[labs[0]] = 7.3195  # Volumen para Lab A (Lab_Betta)\n",
    "if len(labs) > 1:\n",
    "    volumes[labs[1]] = 6.8342  # Volumen para Lab B (Lab_Gamma)\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_accumulated_enstrophy(root_folder, lab_name, shift, file_suffixes, volumes[lab_name], triad_codes)\n",
    "\n",
    "print(\"Processing complete. All accumulated enstrophy results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Códigos pesajes\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import re\n",
    "\n",
    "def extraer_fecha(ruta):\n",
    "    # Definir el patrón para coincidir con el formato de fecha ddMmmYY (como 12Ago24)\n",
    "    patron_fecha = r\"\\d{2}[A-Za-z]{3}\\d{2}\"\n",
    "    \n",
    "    # Buscar la fecha en la cadena\n",
    "    coincidencia = re.search(patron_fecha, ruta)\n",
    "    \n",
    "    if coincidencia:\n",
    "        return coincidencia.group()  # Devolver la fecha encontrada\n",
    "    else:\n",
    "        return None  # Si no se encuentra, devolver None\n",
    "    \n",
    "# Función para decodificar el texto hexadecimal\n",
    "def decodificar_hex(hex_string):\n",
    "    try:\n",
    "        hex_cleaned = hex_string.strip().replace(' ', '')\n",
    "        decoded_string = bytes.fromhex(hex_cleaned).decode('ascii', errors='ignore').strip()\n",
    "        return decoded_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error decodificando: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para procesar el archivo y extraer la hora y el valor limpio\n",
    "def procesar_archivo_para_hora_y_texto(ruta_archivo):\n",
    "    resultados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            match_hora = re.search(r'(\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}\\.\\d{3})', linea)\n",
    "            if match_hora:\n",
    "                hora = match_hora.group(1).split(' ')[1]  # Obtener solo la parte de la hora\n",
    "                partes = linea.strip().split(',')\n",
    "                if len(partes) > 3:\n",
    "                    hex_data = partes[-1]\n",
    "                    decoded_value = decodificar_hex(hex_data)\n",
    "\n",
    "                    if decoded_value:\n",
    "                        resultados.append(f\"{hora}, {decoded_value}\")\n",
    "                    else:\n",
    "                        resultados.append(f\"{hora}, .\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Función para guardar los resultados en un archivo de salida\n",
    "def guardar_resultados(ruta_salida, resultados):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        for resultado in resultados:\n",
    "            archivo_salida.write(resultado + '\\n')\n",
    "\n",
    "# Función para construir la ruta del archivo a procesar\n",
    "def construir_ruta(root_folder, fecha, lab, turno, color, numero):\n",
    "    colorin = color.lower()\n",
    "    return os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", f\"{numero:02d}med{colorin}.log\")\n",
    "\n",
    "# Función para filtrar resultados\n",
    "def filtrar_resultados(resultados):\n",
    "    # Expresión regular para el formato deseado\n",
    "    pattern = r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}, \\d+\\.\\d{2}g$'\n",
    "    return [resultado for resultado in resultados if re.match(pattern, resultado.strip())]\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    #print(root_folder)\n",
    "    fecha = extraer_fecha(root_folder)  # Asegúrate de definir esta función\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\", \"Naranja\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    ruta_archivo = construir_ruta(root_folder, fecha, lab, turno, color, numero)\n",
    "                    print(ruta_archivo)\n",
    "                    \n",
    "                    colorin = color.lower()\n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        ruta_salida = os.path.join(os.path.dirname(ruta_archivo), f\"{numero:02d}med{colorin}.txt\")\n",
    "\n",
    "                        print(f\"\\nProcesando para Lab: {lab}, Turno: {turno}, Color: {color}, Archivo: {numero:02d}\")\n",
    "                        print(f\"Archivo a procesar: {ruta_archivo}\")\n",
    "                        \n",
    "                        # Procesar el archivo para extraer hora y texto limpio\n",
    "                        resultados = procesar_archivo_para_hora_y_texto(ruta_archivo)\n",
    "\n",
    "                        # Filtrar resultados\n",
    "                        resultados_filtrados = filtrar_resultados(resultados)\n",
    "\n",
    "                        # Guardar los resultados filtrados en el archivo de salida\n",
    "                        guardar_resultados(ruta_salida, resultados_filtrados)\n",
    "                        print(f\"Archivo encontrado: {ruta_archivo}\")\n",
    "                    #else:\n",
    "                     #   print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Función para extraer la fecha del nombre del directorio raíz\n",
    "def extraer_fecha(root_folder):\n",
    "    nombre_directorio = os.path.basename(root_folder)\n",
    "    match = re.search(r'(\\d{2}[A-Za-z]{3}\\d{2})', nombre_directorio)\n",
    "    \n",
    "    if match:\n",
    "        fecha_str = match.group(1)  # Captura la fecha en formato '16Sep24'\n",
    "        return fecha_str\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontró una fecha válida en el nombre del directorio {root_folder}.\")\n",
    "\n",
    "# Función para cargar y procesar los datos\n",
    "def cargar_y_procesar_datos(ruta_archivo):\n",
    "    datos_procesados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            partes = linea.split(',')\n",
    "            hora = partes[0].strip()  # Extrae la hora\n",
    "            valor = partes[1].strip()  # Extrae el valor con 'g'\n",
    "            \n",
    "            # Elimina la 'g' y convierte a float\n",
    "            valor_numero = float(valor.replace('g', ''))\n",
    "            # Aplica el tratamiento Dato * 9.81 / 5.00\n",
    "            valor_procesado = valor_numero * 9.81 / 5.00\n",
    "            \n",
    "            # Guarda la hora y el valor procesado\n",
    "            datos_procesados.append(f\"{hora}, {valor_procesado:.2f}\\n\")\n",
    "    \n",
    "    return datos_procesados\n",
    "\n",
    "# Función para guardar los datos procesados en un archivo .txt\n",
    "def guardar_datos_txt(datos_procesados, ruta_salida):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        archivo_salida.writelines(datos_procesados)\n",
    "    print(f\"Datos procesados guardados en: {ruta_salida}\")\n",
    "\n",
    "# Función principal para procesar y guardar archivos\n",
    "def procesar_archivos(root_folder):\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\", \"Naranja\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    fecha = extraer_fecha(root_folder)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    colorin = color.lower()\n",
    "                    ruta_archivo = os.path.join(root_folder, f\"{fecha} - Lab_{lab}\", f\"{fecha}.{turno} - Lab_{lab}\", \"Pesajes\", f\"0{numero}med{colorin}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        # Cargar y procesar los datos del archivo\n",
    "                        datos_procesados = cargar_y_procesar_datos(ruta_archivo)\n",
    "\n",
    "                        # Definir la carpeta y el nombre del archivo de salida\n",
    "                        output_folder = os.path.join(root_folder, f\"{fecha} - Lab_{lab}\", f\"{fecha}.{turno} - Lab_{lab}\", \"Pesajes\", \"procesados\")\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ruta_salida = os.path.join(output_folder, f\"resultados_0{numero}_{colorin}.txt\")\n",
    "\n",
    "                        print(ruta_salida)\n",
    "\n",
    "                        # Guardar los datos procesados en un archivo .txt\n",
    "                        guardar_datos_txt(datos_procesados, ruta_salida)\n",
    "                    #else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_archivos(root_folder)\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir un diccionario para mapear los nombres de los colores a los colores de matplotlib\n",
    "color_map = {\n",
    "    \"amarilla\": \"yellow\",\n",
    "    \"roja\": \"red\",\n",
    "    \"azul\": \"blue\",\n",
    "    \"verde\": \"green\",\n",
    "    \"morada\": \"purple\",\n",
    "    \"negra\": \"black\",\n",
    "    \"naranja\": \"orange\"\n",
    "}\n",
    "\n",
    "# Función para cargar datos y crear DataFrame\n",
    "def cargar_datos(ruta_archivo):\n",
    "    # Leer archivo en un DataFrame\n",
    "    df = pd.read_csv(ruta_archivo, header=None, names=[\"Timestamp\", \"Valor\"])\n",
    "    \n",
    "    # Convertir el timestamp a formato de tiempo de pandas para facilitar el graficado\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S.%f')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para graficar serie de tiempo como scatter plot con color especificado\n",
    "def graficar_serie_tiempo(df, titulo, ruta_salida, color):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['Timestamp'], df['Valor'], color=color)\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.title(titulo)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Guardar la gráfica en la ruta especificada\n",
    "    plt.savefig(ruta_salida)\n",
    "    plt.close()  # Cierra la gráfica para no mostrarla en la terminal\n",
    "    print(f\"Gráfica guardada en: {ruta_salida}\")\n",
    "\n",
    "# Función principal para procesar y graficar\n",
    "def procesar_y_graficar_archivos(root_folder):\n",
    "    fecha = extraer_fecha(root_folder)  # Usar la función extraer_fecha definida previamente\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    #colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    colorin = color.lower()\n",
    "                    ruta_archivo = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", f\"0{numero}med{colorin}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        # Cargar los datos del archivo procesado\n",
    "                        df = cargar_datos(ruta_archivo)\n",
    "                        \n",
    "                        # Crear el título de la gráfica\n",
    "                        titulo = f\"Serie de Tiempo - Lab {lab} - Turno {turno} - {color} - Medición {numero}\"\n",
    "                        \n",
    "                        # Definir la carpeta y el nombre del archivo de salida para la gráfica\n",
    "                        output_folder = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", \"graficas\")\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ruta_salida_grafica = os.path.join(output_folder, f\"grafica_0{numero}_{colorin}.png\")\n",
    "                        \n",
    "                        # Obtener el color correspondiente a partir del nombre del archivo\n",
    "                        color_grafica = color_map.get(colorin, 'blue')  # Default a 'blue' si no se encuentra el color\n",
    "                        \n",
    "                        # Graficar la serie de tiempo como scatter plot con el color correspondiente\n",
    "                        graficar_serie_tiempo(df, titulo, ruta_salida_grafica, color_grafica)\n",
    "                    else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "\n",
    "# Ejecutar el procesamiento y graficado\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_y_graficar_archivos(root_folder)\n",
    "    print(\"\\nProcesamiento y graficado completo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Códigos de Comparación de Modulos\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@siva las siguientes celdas de código generan la comparación de modulos\n",
    "fechas = []\n",
    "\n",
    "# Función para extraer la fecha del nombre del directorio raíz\n",
    "def extraer_fecha(root_folder):\n",
    "    nombre_directorio = os.path.basename(root_folder)\n",
    "    match = re.search(r'(\\d{2}[A-Za-z]{3}\\d{2})', nombre_directorio)\n",
    "    \n",
    "    if match:\n",
    "        fecha_str = match.group(1)  # Captura la fecha en formato '16Sep24'\n",
    "        return fecha_str\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontró una fecha válida en el nombre del directorio {root_folder}.\")\n",
    "\n",
    "fechas.append(extraer_fecha(root_folder))\n",
    "\n",
    "print(fechas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re  # Para utilizar expresiones regulares\n",
    "\n",
    "# Listas para determinar laboratorio, turno y color\n",
    "turnos = ['M', 'V']\n",
    "colores = ['Amarilla', 'Roja', 'Verde', 'Morada', 'Azul', 'Negra', 'Naranja']\n",
    "\n",
    "# Diccionario para almacenar archivos según el criterio\n",
    "archivos_validos = {\n",
    "    lab: {turno: {'experimental': [], 'base': []} for turno in turnos} for lab in labs\n",
    "}\n",
    "\n",
    "# Función para extraer el número del archivo desde su nombre\n",
    "def extraer_numero_archivo(nombre_archivo):\n",
    "    match = re.match(r'(\\d+)', nombre_archivo)  # Busca el número al inicio del nombre\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "# Función para buscar y analizar archivos según los criterios específicos para cada fecha\n",
    "def buscar_archivos_y_analizar(root_folder, fechas):\n",
    "    archivos_info = []  # Lista para almacenar la información de todos los archivos procesados\n",
    "\n",
    "    for fecha in fechas:  # Iterar sobre cada fecha en la lista\n",
    "        for lab in labs:\n",
    "            for turno in turnos:\n",
    "                for color in colores:\n",
    "                    # Definir la ruta base específica para cada fecha con el subdirectorio de la fecha\n",
    "                    ruta_base = os.path.join(root_folder, f'{fecha} - {lab}', f'{fecha}.{turno} - {lab}', color)\n",
    "                    # Verificar la existencia de la ruta\n",
    "                    if not os.path.exists(ruta_base):\n",
    "                        continue\n",
    "                    \n",
    "                    # Recorrer los archivos en la ruta base\n",
    "                    for root, _, files in os.walk(ruta_base):\n",
    "                        if not files:  # Verificar si no hay archivos en la carpeta\n",
    "                            print(f\"No se encontraron archivos en: {ruta_base}\")\n",
    "                            continue\n",
    "\n",
    "                        for file in files:\n",
    "                            if file.endswith('.txt') and '00' not in file:\n",
    "                                filepath = os.path.join(root, file)\n",
    "\n",
    "                                # Extraer el número del archivo\n",
    "                                numero_archivo = extraer_numero_archivo(file)\n",
    "\n",
    "                                tipo = 'base' if color.lower() == 'negra' else 'experimental'\n",
    "                                archivos_validos[lab][turno][tipo].append({\n",
    "                                    'Número': numero_archivo,  # Almacena el número extraído del nombre del archivo\n",
    "                                    'Ruta': filepath,\n",
    "                                    'Fecha': fecha  # Añadir la fecha al diccionario\n",
    "                                })\n",
    "\n",
    "                                archivos_info.append({\n",
    "                                    'Número': numero_archivo,\n",
    "                                    'Archivo': file,\n",
    "                                    'Laboratorio': lab,\n",
    "                                    'Turno': turno,\n",
    "                                    'Tipo': 'Base' if color.lower() == 'negra' else 'Experimental',\n",
    "                                    'Fecha': fecha  # Añadir la fecha al diccionario\n",
    "                                })\n",
    "    return archivos_info\n",
    "\n",
    "# Llamada a la función de análisis para todas las fechas\n",
    "archivos_info = buscar_archivos_y_analizar(root_folder, fechas)\n",
    "\n",
    "# Mostrar la información detallada de cada archivo procesado (descomentar para visualización)\n",
    "# for info in archivos_info:\n",
    "#     print(info)\n",
    "\n",
    "# También puedes acceder a los archivos válidos por laboratorio, turno y tipo de archivo así:\n",
    "print(archivos_validos)\n",
    "# print(archivos_validos['Lab_Gamma']['M'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@siva en base a los archivos que sean adecuados, genera comparaciones de los datos crudos\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Función para calcular módulos y sumar utilizando el diccionario `archivos_validos` con fechas\n",
    "def calcular_modulos_y_sumar_con_fechas(archivos_validos):\n",
    "    resultados = {}\n",
    "    total_casos_satisfactorios = 0  # Inicializar el contador total de casos satisfactorios\n",
    "\n",
    "    for lab in archivos_validos.keys():  # Iterar sobre cada laboratorio\n",
    "        for turno in archivos_validos[lab].keys():  # Iterar sobre cada turno\n",
    "            experimental_archivos = archivos_validos[lab][turno]['experimental']\n",
    "            base_archivos = archivos_validos[lab][turno]['base']\n",
    "            \n",
    "            # Crear un diccionario para acceder a los archivos base por número\n",
    "            base_dict = {archivo['Número']: archivo['Ruta'] for archivo in base_archivos}\n",
    "\n",
    "            for exp_archivo in experimental_archivos:\n",
    "                exp_num = exp_archivo['Número']\n",
    "                exp_ruta = exp_archivo['Ruta']\n",
    "                exp_nombre = os.path.basename(exp_ruta)  # Obtener el nombre del archivo sin la ruta\n",
    "                exp_fecha = exp_archivo['Fecha']  # Obtener la fecha del archivo experimental\n",
    "\n",
    "                # Verificar si `exp_num` es válido\n",
    "                if exp_num is None:\n",
    "                    print(f\"Advertencia: 'Número' es None para el archivo {exp_ruta}. Saltando este archivo.\")\n",
    "                    continue\n",
    "\n",
    "                # Calcular la suma de módulos para el archivo experimental\n",
    "                mod_exp_suma = sumar_modulos(exp_ruta)\n",
    "\n",
    "                # Inicializar la lista de resultados para cada archivo experimental\n",
    "                if exp_fecha not in resultados:\n",
    "                    resultados[exp_fecha] = {}  # Crear entrada para la fecha si no existe\n",
    "\n",
    "                # Crear una lista para almacenar los resultados por laboratorio, turno y archivo\n",
    "                resultados[exp_fecha][f'{lab}_{turno}_Archivo_{exp_num}'] = []\n",
    "\n",
    "                # Verificar si hay un archivo base con el mismo número y el siguiente\n",
    "                for num in (exp_num, exp_num + 1):\n",
    "                    if num in base_dict:\n",
    "                        base_ruta = base_dict[num]  # Accedemos a la ruta correctamente\n",
    "                        #print(exp_ruta)\n",
    "                        mod_base_suma = sumar_modulos(base_ruta)\n",
    "                        dif_porcentual = (abs(mod_exp_suma - mod_base_suma) / mod_base_suma) * 100\n",
    "\n",
    "                        # Determinar el resultado basado en la diferencia porcentual\n",
    "                        if dif_porcentual < 10:\n",
    "                            resultado = \"No hay evidencia de intervención.\"\n",
    "                        elif 10 <= dif_porcentual <= 33.9:\n",
    "                            resultado = \"Resultados inconclusos.\"\n",
    "                        else:  # valor > 33.9\n",
    "                            resultado = \"Evidencia de intervención satisfactoria.\"\n",
    "                            total_casos_satisfactorios += 1  # Aumentar contador total de casos satisfactorios\n",
    "\n",
    "                        # Obtener el nombre del archivo base sin la ruta\n",
    "                        base_nombre = os.path.basename(base_ruta)\n",
    "\n",
    "                        # Almacenar el resultado en la lista de resultados para el archivo experimental\n",
    "                        resultados[exp_fecha][f'{lab}_{turno}_Archivo_{exp_num}'].append({\n",
    "                            'Laboratorio': lab,  # Agregar el nombre del laboratorio\n",
    "                            'Archivo_Experimental': exp_nombre,\n",
    "                            'Archivo_Base': base_nombre,\n",
    "                            'Modulo_Experimental': mod_exp_suma,\n",
    "                            'Modulo_Base': mod_base_suma,\n",
    "                            'Diferencia_Porcentual': dif_porcentual,\n",
    "                            'Resultado': resultado\n",
    "                        })\n",
    "\n",
    "    return resultados, total_casos_satisfactorios\n",
    "\n",
    "# Función para calcular la suma de módulos de los vectores en un archivo\n",
    "def sumar_modulos(filepath):\n",
    "    suma_modulos = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                vector = parts[1].strip('()').split(',')\n",
    "                valores = np.array([float(num.strip()) for num in vector])\n",
    "                modulo = np.linalg.norm(valores)  # Calcular el módulo del vector\n",
    "                suma_modulos += modulo\n",
    "\n",
    "    return suma_modulos\n",
    "\n",
    "# Función para guardar los resultados en un archivo Excel separado por cada fecha dentro de 'Resultados_Excel' en root_folder\n",
    "def guardar_resultados_por_fecha(resultados, root_folder):\n",
    "    # Crear la carpeta 'Resultados_Excel' dentro de `root_folder` si no existe\n",
    "    carpeta_resultados = os.path.join(root_folder, \"Excels\", 'Moduli Analysis')\n",
    "    os.makedirs(carpeta_resultados, exist_ok=True)\n",
    "\n",
    "    for fecha, datos_fecha in resultados.items():\n",
    "        # Crear una lista para almacenar las filas del DataFrame\n",
    "        filas = []\n",
    "\n",
    "        for archivo, lista_resultados in datos_fecha.items():\n",
    "            for resultado in lista_resultados:\n",
    "                filas.append({\n",
    "                    'Laboratorio': resultado['Laboratorio'],\n",
    "                    'Archivo_Experimental': resultado['Archivo_Experimental'],\n",
    "                    'Archivo_Base': resultado['Archivo_Base'],\n",
    "                    'Modulo_Experimental': resultado['Modulo_Experimental'],\n",
    "                    'Modulo_Base': resultado['Modulo_Base'],\n",
    "                    'Diferencia_Porcentual': resultado['Diferencia_Porcentual'],\n",
    "                    'Resultado': resultado['Resultado']\n",
    "                })\n",
    "\n",
    "        # Crear un DataFrame a partir de las filas\n",
    "        df = pd.DataFrame(filas)\n",
    "\n",
    "        # Definir el nombre del archivo Excel basado en la fecha y guardarlo en 'Resultados_Excel'\n",
    "        nombre_archivo = f'Moduli_intervencion_general_{fecha}.xlsx'\n",
    "        ruta_archivo = os.path.join(carpeta_resultados, nombre_archivo)\n",
    "\n",
    "        # Guardar el DataFrame en un archivo Excel\n",
    "        df.to_excel(ruta_archivo, index=False)\n",
    "        print(f\"Resultados guardados en {ruta_archivo}\")\n",
    "\n",
    "    return carpeta_resultados\n",
    "\n",
    "# Función para guardar el total de casos satisfactorios en un archivo .txt\n",
    "def guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados):\n",
    "    # Definir la ruta del archivo .txt para guardar el total de casos satisfactorios en la carpeta de resultados Excel\n",
    "    ruta_txt = os.path.join(carpeta_resultados, 'total_casos_satisfactorios.txt')\n",
    "\n",
    "    with open(ruta_txt, 'w') as file:\n",
    "        file.write(f'Total de casos satisfactorios: {total_casos_satisfactorios}\\n')\n",
    "        print(f\"Total de casos satisfactorios guardado en {ruta_txt}\")\n",
    "\n",
    "# Llamada a la función para calcular módulos y sumar utilizando `archivos_validos`\n",
    "resultados, total_casos_satisfactorios = calcular_modulos_y_sumar_con_fechas(archivos_validos)\n",
    "\n",
    "# Guardar los resultados en archivos Excel separados por fecha dentro de la carpeta `root_folder/Resultados_Excel`\n",
    "carpeta_resultados = guardar_resultados_por_fecha(resultados, root_folder)\n",
    "\n",
    "# Guardar el total de casos satisfactorios en un archivo .txt en la misma carpeta donde están los Excel\n",
    "guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados)\n",
    "\n",
    "print(\"Resultados guardados por fecha en archivos Excel separados dentro de 'Resultados_Excel'.\")\n",
    "print(\"Total de casos satisfactorios guardado en 'total_casos_satisfactorios.txt' en la misma carpeta de resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@siva crea comparacion de la primer integral helicidad\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la lista de triadas\n",
    "triads = [\n",
    "    \"FRT\", \"PLB\", \"FLT\", \"PRB\", \"FRB\",\n",
    "    \"PLT\", \"FLB\", \"PRT\", \"RTB\", \"FLP\",\n",
    "    \"LTB\", \"FRP\"\n",
    "]\n",
    "\n",
    "# Función para calcular módulos y sumar utilizando el diccionario `archivos_validos` con fechas\n",
    "def calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder):\n",
    "    resultados_por_fecha = {}  # Diccionario para almacenar los resultados por fecha\n",
    "    total_casos_satisfactorios = 0  # Contador total de casos satisfactorios\n",
    "\n",
    "    for lab in archivos_validos.keys():\n",
    "        for turno in archivos_validos[lab].keys():\n",
    "            for triad in triads:\n",
    "                experimental_archivos = archivos_validos[lab][turno]['experimental']\n",
    "                base_archivos = archivos_validos[lab][turno]['base']\n",
    "                \n",
    "                # Crear un diccionario para acceder a los archivos base por número\n",
    "                base_dict = {archivo['Número']: archivo['Ruta'] for archivo in base_archivos}\n",
    "\n",
    "                for exp_archivo in experimental_archivos:\n",
    "                    exp_num = exp_archivo['Número']\n",
    "                    exp_fecha = exp_archivo['Fecha']  # Obtener la fecha del archivo experimental\n",
    "                    exp_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Helicity_Experimental_Color\", triad, f\"sum_helicity_{triad}_velocity_0{exp_num}med*.txt\")\n",
    "                    \n",
    "                    # Usar glob para buscar el archivo experimental\n",
    "                    exp_archivos_encontrados = glob.glob(exp_ruta_pattern)\n",
    "\n",
    "                    if not exp_archivos_encontrados:\n",
    "                        continue\n",
    "                    \n",
    "                    exp_ruta = exp_archivos_encontrados[0]  # Toma el primer archivo encontrado\n",
    "                    exp_nombre = os.path.basename(exp_ruta)  # Obtener el nombre del archivo sin la ruta\n",
    "\n",
    "                    # Calcular la suma de módulos para el archivo experimental\n",
    "                    mod_exp_suma = sumar_modulos(exp_ruta)\n",
    "\n",
    "                    # Inicializar el diccionario para almacenar los resultados por fecha si no existe\n",
    "                    if exp_fecha not in resultados_por_fecha:\n",
    "                        resultados_por_fecha[exp_fecha] = {triad: [] for triad in triads}\n",
    "\n",
    "                    # Verificar si hay un archivo base con el mismo número y el siguiente\n",
    "                    for num in (exp_num, exp_num + 1):\n",
    "                        if num in base_dict:\n",
    "                            base_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Helicity_Baseline\", triad, f\"sum_helicity_{triad}_velocity_0{num}med*.txt\")\n",
    "                            \n",
    "                            # Usar glob para buscar el archivo base\n",
    "                            base_archivos_encontrados = glob.glob(base_ruta_pattern)\n",
    "                            #print(base_archivos_encontrados)\n",
    "                            if not base_archivos_encontrados:\n",
    "                                print(f\"No se encontró archivo base para el patrón: {base_ruta_pattern}\")\n",
    "                                continue\n",
    "\n",
    "                            base_ruta = base_archivos_encontrados[0]  # Toma el primer archivo base encontrado\n",
    "                            base_nombre = os.path.basename(base_ruta)  # Obtener el nombre del archivo base sin la ruta\n",
    "                            \n",
    "                            # Calcular la suma de módulos para el archivo base\n",
    "                            mod_base_suma = sumar_modulos(base_ruta)\n",
    "                            dif_porcentual = (abs(mod_exp_suma - mod_base_suma) / mod_base_suma) * 100\n",
    "\n",
    "                            # Determinar el resultado basado en la diferencia porcentual\n",
    "                            if dif_porcentual < 10:\n",
    "                                resultado = \"No hay evidencia de intervención.\"\n",
    "                            elif 10 <= dif_porcentual <= 33.9:\n",
    "                                resultado = \"Resultados inconclusos.\"\n",
    "                            else:  # valor > 33.9\n",
    "                                resultado = \"Evidencia de intervención satisfactoria.\"\n",
    "                                total_casos_satisfactorios += 1  # Aumentar contador total de casos satisfactorios\n",
    "\n",
    "                            # Almacenar el resultado en la lista de resultados para la triada y fecha\n",
    "                            #print(resultados_por_fecha[exp_fecha][triad])\n",
    "                            resultados_por_fecha[exp_fecha][triad].append({\n",
    "                                'Laboratorio': lab,\n",
    "                                'Turno': turno,\n",
    "                                'Archivo_Experimental': exp_nombre,\n",
    "                                'Archivo_Base': base_nombre,\n",
    "                                'Modulo_Experimental': mod_exp_suma,\n",
    "                                'Modulo_Base': mod_base_suma,\n",
    "                                'Diferencia_Porcentual': dif_porcentual,\n",
    "                                'Resultado': resultado\n",
    "                            })\n",
    "\n",
    "    return resultados_por_fecha, total_casos_satisfactorios\n",
    "\n",
    "# Función para calcular la suma de módulos de los vectores en un archivo\n",
    "def sumar_modulos(filepath):\n",
    "    suma_modulos = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                vector = parts[1].strip('()').split(',')\n",
    "                valores = np.array([float(num.strip()) for num in vector])\n",
    "                modulo = np.linalg.norm(valores)  # Calcular el módulo del vector\n",
    "                suma_modulos += modulo\n",
    "\n",
    "    return suma_modulos\n",
    "\n",
    "# Función para guardar los resultados en un archivo Excel separado por cada fecha dentro de `Resultados_Triadas` en `root_folder`\n",
    "def guardar_resultados_por_fecha(resultados_por_fecha, root_folder):\n",
    "    # Crear la carpeta 'Resultados_Triadas_Helicidad' dentro de `root_folder` si no existe\n",
    "    carpeta_resultados = os.path.join(root_folder, \"Excels\", 'Helicity Analysis')\n",
    "    os.makedirs(carpeta_resultados, exist_ok=True)\n",
    "\n",
    "    for fecha, resultados_triadas in resultados_por_fecha.items():\n",
    "        # Crear un archivo Excel con hojas por triada para cada fecha\n",
    "        nombre_archivo = f'resultados_triadas_{fecha}.xlsx'\n",
    "        ruta_archivo = os.path.join(carpeta_resultados, nombre_archivo)\n",
    "\n",
    "        with pd.ExcelWriter(ruta_archivo, engine='openpyxl') as writer:\n",
    "            for triad, resultados_triadas_data in resultados_triadas.items():\n",
    "                # Crear un DataFrame para cada triada y guardarlo en una hoja\n",
    "                df = pd.DataFrame(resultados_triadas_data)\n",
    "                if not df.empty:\n",
    "                    # Ordenar columnas según el código anterior\n",
    "                    df = df[['Laboratorio', 'Turno', 'Archivo_Experimental', 'Archivo_Base', 'Modulo_Experimental', 'Modulo_Base', 'Diferencia_Porcentual', 'Resultado']]\n",
    "                df.to_excel(writer, sheet_name=triad, index=False)\n",
    "\n",
    "        print(f\"Resultados guardados en {ruta_archivo}\")\n",
    "\n",
    "    return carpeta_resultados\n",
    "\n",
    "# Función para guardar el total de casos satisfactorios en un archivo .txt\n",
    "def guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados):\n",
    "    # Definir la ruta del archivo .txt para guardar el total de casos satisfactorios en la carpeta de resultados Excel\n",
    "    ruta_txt = os.path.join(carpeta_resultados, 'total_casos_satisfactorios.txt')\n",
    "\n",
    "    with open(ruta_txt, 'w') as file:\n",
    "        file.write(f'Total de casos satisfactorios: {total_casos_satisfactorios}\\n')\n",
    "        print(f\"Total de casos satisfactorios guardado en {ruta_txt}\")\n",
    "\n",
    "# Definir la ruta `root_folder` según tu entorno\n",
    "# root_folder = 'E:/Filter Data Agos-Sep'  # Reemplaza esta línea con tu `root_folder`\n",
    "\n",
    "# Llamada a la función para calcular módulos y sumar utilizando `archivos_validos`\n",
    "resultados_por_fecha, total_casos_satisfactorios = calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder)\n",
    "\n",
    "# Guardar los resultados en archivos Excel separados por fecha dentro de la carpeta `root_folder/Resultados_Triadas_Helicidad`\n",
    "carpeta_resultados = guardar_resultados_por_fecha(resultados_por_fecha, root_folder)\n",
    "\n",
    "# Guardar el total de casos satisfactorios en un archivo .txt en la misma carpeta donde están los Excel\n",
    "guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados)\n",
    "\n",
    "print(\"Resultados guardados por fecha en archivos Excel separados dentro de 'Resultados_Triadas_Helicidad'.\")\n",
    "print(\"Total de casos satisfactorios guardado en 'total_casos_satisfactorios.txt' en la misma carpeta de resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@siva compara los modulos de la integral absoluta de helicidad\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la lista de triadas\n",
    "triads = [\n",
    "    \"FRT\", \"PLB\", \"FLT\", \"PRB\", \"FRB\",\n",
    "    \"PLT\", \"FLB\", \"PRT\", \"RTB\", \"FLP\",\n",
    "    \"LTB\", \"FRP\"\n",
    "]\n",
    "\n",
    "# Función para calcular módulos y sumar utilizando el diccionario `archivos_validos` con fechas\n",
    "def calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder):\n",
    "    resultados_por_fecha = {}  # Diccionario para almacenar los resultados por fecha\n",
    "    total_casos_satisfactorios = 0  # Contador total de casos satisfactorios\n",
    "\n",
    "    for lab in archivos_validos.keys():\n",
    "        for turno in archivos_validos[lab].keys():\n",
    "            for triad in triads:\n",
    "                experimental_archivos = archivos_validos[lab][turno]['experimental']\n",
    "                base_archivos = archivos_validos[lab][turno]['base']\n",
    "                \n",
    "                # Crear un diccionario para acceder a los archivos base por número\n",
    "                base_dict = {archivo['Número']: archivo['Ruta'] for archivo in base_archivos}\n",
    "\n",
    "                for exp_archivo in experimental_archivos:\n",
    "                    exp_num = exp_archivo['Número']\n",
    "                    exp_fecha = exp_archivo['Fecha']  # Obtener la fecha del archivo experimental\n",
    "                    exp_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Absolute_Helicity_Experimental_Color\", triad, f\"sum_absolute_helicity_{triad}_velocity_0{exp_num}med*.txt\")\n",
    "                    \n",
    "                    # Usar glob para buscar el archivo experimental\n",
    "                    exp_archivos_encontrados = glob.glob(exp_ruta_pattern)\n",
    "\n",
    "                    if not exp_archivos_encontrados:\n",
    "                        continue\n",
    "                    \n",
    "                    exp_ruta = exp_archivos_encontrados[0]  # Toma el primer archivo encontrado\n",
    "                    exp_nombre = os.path.basename(exp_ruta)  # Obtener el nombre del archivo sin la ruta\n",
    "\n",
    "                    # Calcular la suma de módulos para el archivo experimental\n",
    "                    mod_exp_suma = sumar_modulos(exp_ruta)\n",
    "\n",
    "                    # Inicializar el diccionario para almacenar los resultados por fecha si no existe\n",
    "                    if exp_fecha not in resultados_por_fecha:\n",
    "                        resultados_por_fecha[exp_fecha] = {triad: [] for triad in triads}\n",
    "\n",
    "                    # Verificar si hay un archivo base con el mismo número y el siguiente\n",
    "                    for num in (exp_num, exp_num + 1):\n",
    "                        if num in base_dict:\n",
    "                            base_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Absolute_Helicity_Baseline\", triad, f\"sum_absolute_helicity_{triad}_velocity_0{num}med*.txt\")\n",
    "                            \n",
    "                            # Usar glob para buscar el archivo base\n",
    "                            base_archivos_encontrados = glob.glob(base_ruta_pattern)\n",
    "\n",
    "                            if not base_archivos_encontrados:\n",
    "                                print(f\"No se encontró archivo base para el patrón: {base_ruta_pattern}\")\n",
    "                                continue\n",
    "\n",
    "                            base_ruta = base_archivos_encontrados[0]  # Toma el primer archivo base encontrado\n",
    "                            base_nombre = os.path.basename(base_ruta)  # Obtener el nombre del archivo base sin la ruta\n",
    "                            \n",
    "                            # Calcular la suma de módulos para el archivo base\n",
    "                            mod_base_suma = sumar_modulos(base_ruta)\n",
    "                            dif_porcentual = (abs(mod_exp_suma - mod_base_suma) / mod_base_suma) * 100\n",
    "\n",
    "                            # Determinar el resultado basado en la diferencia porcentual\n",
    "                            if dif_porcentual < 10:\n",
    "                                resultado = \"No hay evidencia de intervención.\"\n",
    "                            elif 10 <= dif_porcentual <= 33.9:\n",
    "                                resultado = \"Resultados inconclusos.\"\n",
    "                            else:  # valor > 33.9\n",
    "                                resultado = \"Evidencia de intervención satisfactoria.\"\n",
    "                                total_casos_satisfactorios += 1  # Aumentar contador total de casos satisfactorios\n",
    "\n",
    "                            # Almacenar el resultado en la lista de resultados para la triada y fecha\n",
    "                            resultados_por_fecha[exp_fecha][triad].append({\n",
    "                                'Laboratorio': lab,\n",
    "                                'Turno': turno,\n",
    "                                'Archivo_Experimental': exp_nombre,\n",
    "                                'Archivo_Base': base_nombre,\n",
    "                                'Modulo_Experimental': mod_exp_suma,\n",
    "                                'Modulo_Base': mod_base_suma,\n",
    "                                'Diferencia_Porcentual': dif_porcentual,\n",
    "                                'Resultado': resultado\n",
    "                            })\n",
    "\n",
    "    return resultados_por_fecha, total_casos_satisfactorios\n",
    "\n",
    "# Función para calcular la suma de módulos de los vectores en un archivo\n",
    "def sumar_modulos(filepath):\n",
    "    suma_modulos = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                vector = parts[1].strip('()').split(',')\n",
    "                valores = np.array([float(num.strip()) for num in vector])\n",
    "                modulo = np.linalg.norm(valores)  # Calcular el módulo del vector\n",
    "                suma_modulos += modulo\n",
    "\n",
    "    return suma_modulos\n",
    "\n",
    "# Función para guardar los resultados en un archivo Excel separado por cada fecha dentro de `Resultados_Triadas` en `root_folder`\n",
    "def guardar_resultados_por_fecha(resultados_por_fecha, root_folder):\n",
    "    # Crear la carpeta 'Resultados_Triadas_Helicidad' dentro de `root_folder` si no existe\n",
    "    carpeta_resultados = os.path.join(root_folder, \"Excels\", 'Absolute Helicity Analysis')\n",
    "    os.makedirs(carpeta_resultados, exist_ok=True)\n",
    "\n",
    "    for fecha, resultados_triadas in resultados_por_fecha.items():\n",
    "        # Crear un archivo Excel con hojas por triada para cada fecha\n",
    "        nombre_archivo = f'resultados_triadas_{fecha}.xlsx'\n",
    "        ruta_archivo = os.path.join(carpeta_resultados, nombre_archivo)\n",
    "\n",
    "        with pd.ExcelWriter(ruta_archivo, engine='openpyxl') as writer:\n",
    "            for triad, resultados_triadas_data in resultados_triadas.items():\n",
    "                # Crear un DataFrame para cada triada y guardarlo en una hoja\n",
    "                df = pd.DataFrame(resultados_triadas_data)\n",
    "                if not df.empty:\n",
    "                    # Ordenar columnas según el código anterior\n",
    "                    df = df[['Laboratorio', 'Turno', 'Archivo_Experimental', 'Archivo_Base', 'Modulo_Experimental', 'Modulo_Base', 'Diferencia_Porcentual', 'Resultado']]\n",
    "                df.to_excel(writer, sheet_name=triad, index=False)\n",
    "\n",
    "        print(f\"Resultados guardados en {ruta_archivo}\")\n",
    "\n",
    "    return carpeta_resultados\n",
    "\n",
    "# Función para guardar el total de casos satisfactorios en un archivo .txt\n",
    "def guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados):\n",
    "    # Definir la ruta del archivo .txt para guardar el total de casos satisfactorios en la carpeta de resultados Excel\n",
    "    ruta_txt = os.path.join(carpeta_resultados, 'total_casos_satisfactorios.txt')\n",
    "\n",
    "    with open(ruta_txt, 'w') as file:\n",
    "        file.write(f'Total de casos satisfactorios: {total_casos_satisfactorios}\\n')\n",
    "        print(f\"Total de casos satisfactorios guardado en {ruta_txt}\")\n",
    "\n",
    "# Definir la ruta `root_folder` según tu entorno\n",
    "# root_folder = 'E:/Filter Data Agos-Sep'  # Reemplaza esta línea con tu `root_folder`\n",
    "\n",
    "# Llamada a la función para calcular módulos y sumar utilizando `archivos_validos`\n",
    "resultados_por_fecha, total_casos_satisfactorios = calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder)\n",
    "\n",
    "# Guardar los resultados en archivos Excel separados por fecha dentro de la carpeta `root_folder/Resultados_Triadas_Helicidad`\n",
    "carpeta_resultados = guardar_resultados_por_fecha(resultados_por_fecha, root_folder)\n",
    "\n",
    "# Guardar el total de casos satisfactorios en un archivo .txt en la misma carpeta donde están los Excel\n",
    "guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados)\n",
    "\n",
    "print(\"Resultados guardados por fecha en archivos Excel separados dentro de 'Resultados_Triadas_Helicidad_Absoluta'.\")\n",
    "print(\"Total de casos satisfactorios guardado en 'total_casos_satisfactorios.txt' en la misma carpeta de resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siva este codigo compara los modulos de la primera integral de enstrofia\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la lista de triadas\n",
    "triads = [\n",
    "    \"FRT\", \"PLB\", \"FLT\", \"PRB\", \"FRB\",\n",
    "    \"PLT\", \"FLB\", \"PRT\", \"RTB\", \"FLP\",\n",
    "    \"LTB\", \"FRP\"\n",
    "]\n",
    "\n",
    "# Función para calcular módulos y sumar utilizando el diccionario `archivos_validos` con fechas\n",
    "def calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder):\n",
    "    resultados_por_fecha = {}  # Diccionario para almacenar los resultados por fecha\n",
    "    total_casos_satisfactorios = 0  # Contador total de casos satisfactorios\n",
    "\n",
    "    for lab in archivos_validos.keys():\n",
    "        for turno in archivos_validos[lab].keys():\n",
    "            for triad in triads:\n",
    "                experimental_archivos = archivos_validos[lab][turno]['experimental']\n",
    "                base_archivos = archivos_validos[lab][turno]['base']\n",
    "                \n",
    "                # Crear un diccionario para acceder a los archivos base por número\n",
    "                base_dict = {archivo['Número']: archivo['Ruta'] for archivo in base_archivos}\n",
    "\n",
    "                for exp_archivo in experimental_archivos:\n",
    "                    exp_num = exp_archivo['Número']\n",
    "                    exp_fecha = exp_archivo['Fecha']  # Obtener la fecha del archivo experimental\n",
    "                    exp_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Enstrophy_Experimental_Color\", triad, f\"sum_enstrophy_vorticity_{triad}_velocity_0{exp_num}med*.txt\")\n",
    "                    \n",
    "                    # Usar glob para buscar el archivo experimental\n",
    "                    exp_archivos_encontrados = glob.glob(exp_ruta_pattern)\n",
    "\n",
    "                    if not exp_archivos_encontrados:\n",
    "                        continue\n",
    "                    \n",
    "                    exp_ruta = exp_archivos_encontrados[0]  # Toma el primer archivo encontrado\n",
    "                    exp_nombre = os.path.basename(exp_ruta)  # Obtener el nombre del archivo sin la ruta\n",
    "\n",
    "                    # Calcular la suma de módulos para el archivo experimental\n",
    "                    mod_exp_suma = sumar_modulos(exp_ruta)\n",
    "\n",
    "                    # Inicializar el diccionario para almacenar los resultados por fecha si no existe\n",
    "                    if exp_fecha not in resultados_por_fecha:\n",
    "                        resultados_por_fecha[exp_fecha] = {triad: [] for triad in triads}\n",
    "\n",
    "                    # Verificar si hay un archivo base con el mismo número y el siguiente\n",
    "                    for num in (exp_num, exp_num + 1):\n",
    "                        if num in base_dict:\n",
    "                            base_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Enstrophy_Baseline\", triad, f\"sum_enstrophy_vorticity_{triad}_velocity_0{num}med*.txt\")\n",
    "                            \n",
    "                            # Usar glob para buscar el archivo base\n",
    "                            base_archivos_encontrados = glob.glob(base_ruta_pattern)\n",
    "\n",
    "                            if not base_archivos_encontrados:\n",
    "                                print(f\"No se encontró archivo base para el patrón: {base_ruta_pattern}\")\n",
    "                                continue\n",
    "\n",
    "                            base_ruta = base_archivos_encontrados[0]  # Toma el primer archivo base encontrado\n",
    "                            base_nombre = os.path.basename(base_ruta)  # Obtener el nombre del archivo base sin la ruta\n",
    "                            \n",
    "                            # Calcular la suma de módulos para el archivo base\n",
    "                            mod_base_suma = sumar_modulos(base_ruta)\n",
    "                            dif_porcentual = (abs(mod_exp_suma - mod_base_suma) / mod_base_suma) * 100\n",
    "\n",
    "                            # Determinar el resultado basado en la diferencia porcentual\n",
    "                            if dif_porcentual < 10:\n",
    "                                resultado = \"No hay evidencia de intervención.\"\n",
    "                            elif 10 <= dif_porcentual <= 33.9:\n",
    "                                resultado = \"Resultados inconclusos.\"\n",
    "                            else:  # valor > 33.9\n",
    "                                resultado = \"Evidencia de intervención satisfactoria.\"\n",
    "                                total_casos_satisfactorios += 1  # Aumentar contador total de casos satisfactorios\n",
    "\n",
    "                            # Almacenar el resultado en la lista de resultados para la triada y fecha\n",
    "                            resultados_por_fecha[exp_fecha][triad].append({\n",
    "                                'Laboratorio': lab,\n",
    "                                'Turno': turno,\n",
    "                                'Archivo_Experimental': exp_nombre,\n",
    "                                'Archivo_Base': base_nombre,\n",
    "                                'Modulo_Experimental': mod_exp_suma,\n",
    "                                'Modulo_Base': mod_base_suma,\n",
    "                                'Diferencia_Porcentual': dif_porcentual,\n",
    "                                'Resultado': resultado\n",
    "                            })\n",
    "\n",
    "    return resultados_por_fecha, total_casos_satisfactorios\n",
    "\n",
    "# Función para calcular la suma de módulos de los vectores en un archivo\n",
    "def sumar_modulos(filepath):\n",
    "    suma_modulos = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                vector = parts[1].strip('()').split(',')\n",
    "                valores = np.array([float(num.strip()) for num in vector])\n",
    "                modulo = np.linalg.norm(valores)  # Calcular el módulo del vector\n",
    "                suma_modulos += modulo\n",
    "\n",
    "    return suma_modulos\n",
    "\n",
    "# Función para guardar los resultados en un archivo Excel separado por cada fecha dentro de `Resultados_Triadas` en `root_folder`\n",
    "def guardar_resultados_por_fecha(resultados_por_fecha, root_folder):\n",
    "    # Crear la carpeta 'Resultados_Triadas_Helicidad' dentro de `root_folder` si no existe\n",
    "    carpeta_resultados = os.path.join(root_folder, \"Excels\", 'Enstrophy Analysis')\n",
    "    os.makedirs(carpeta_resultados, exist_ok=True)\n",
    "\n",
    "    for fecha, resultados_triadas in resultados_por_fecha.items():\n",
    "        # Crear un archivo Excel con hojas por triada para cada fecha\n",
    "        nombre_archivo = f'resultados_triadas_{fecha}.xlsx'\n",
    "        ruta_archivo = os.path.join(carpeta_resultados, nombre_archivo)\n",
    "\n",
    "        with pd.ExcelWriter(ruta_archivo, engine='openpyxl') as writer:\n",
    "            for triad, resultados_triadas_data in resultados_triadas.items():\n",
    "                # Crear un DataFrame para cada triada y guardarlo en una hoja\n",
    "                df = pd.DataFrame(resultados_triadas_data)\n",
    "                if not df.empty:\n",
    "                    # Ordenar columnas según el código anterior\n",
    "                    df = df[['Laboratorio', 'Turno', 'Archivo_Experimental', 'Archivo_Base', 'Modulo_Experimental', 'Modulo_Base', 'Diferencia_Porcentual', 'Resultado']]\n",
    "                df.to_excel(writer, sheet_name=triad, index=False)\n",
    "\n",
    "        print(f\"Resultados guardados en {ruta_archivo}\")\n",
    "\n",
    "    return carpeta_resultados\n",
    "\n",
    "# Función para guardar el total de casos satisfactorios en un archivo .txt\n",
    "def guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados):\n",
    "    # Definir la ruta del archivo .txt para guardar el total de casos satisfactorios en la carpeta de resultados Excel\n",
    "    ruta_txt = os.path.join(carpeta_resultados, 'total_casos_satisfactorios.txt')\n",
    "\n",
    "    with open(ruta_txt, 'w') as file:\n",
    "        file.write(f'Total de casos satisfactorios: {total_casos_satisfactorios}\\n')\n",
    "        print(f\"Total de casos satisfactorios guardado en {ruta_txt}\")\n",
    "\n",
    "# Definir la ruta `root_folder` según tu entorno\n",
    "# root_folder = 'E:/Filter Data Agos-Sep'  # Reemplaza esta línea con tu `root_folder`\n",
    "\n",
    "# Llamada a la función para calcular módulos y sumar utilizando `archivos_validos`\n",
    "resultados_por_fecha, total_casos_satisfactorios = calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder)\n",
    "\n",
    "# Guardar los resultados en archivos Excel separados por fecha dentro de la carpeta `root_folder/Resultados_Triadas_Helicidad`\n",
    "carpeta_resultados = guardar_resultados_por_fecha(resultados_por_fecha, root_folder)\n",
    "\n",
    "# Guardar el total de casos satisfactorios en un archivo .txt en la misma carpeta donde están los Excel\n",
    "guardar_casos_satisfactorios_txt(total_casos_satisfactorios, carpeta_resultados)\n",
    "\n",
    "print(\"Resultados guardados por fecha en archivos Excel separados dentro de 'Resultados_Triadas_Enstrofia'.\")\n",
    "print(\"Total de casos satisfactorios guardado en 'total_casos_satisfactorios.txt' en la misma carpeta de resultados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Definir la lista de triadas\n",
    "triads = [\n",
    "    \"FRT\", \"PLB\", \"FLT\", \"PRB\", \"FRB\",\n",
    "    \"PLT\", \"FLB\", \"PRT\", \"RTB\", \"FLP\",\n",
    "    \"LTB\", \"FRP\"\n",
    "]\n",
    "\n",
    "# Lista de colores\n",
    "colores = [\"AMARILLA\", \"ROJA\", \"MORADA\", \"VERDE\", \"AZUL\", \"NARANJA\"]\n",
    "\n",
    "# Función para calcular módulos y sumar utilizando el diccionario `archivos_validos` con fechas\n",
    "def calcular_modulos_y_sumar_con_fechas(archivos_validos, root_folder):\n",
    "    resultados_por_fecha = {}  # Diccionario para almacenar los resultados por fecha\n",
    "    total_casos_satisfactorios = 0  # Contador total de casos satisfactorios\n",
    "\n",
    "    for lab in archivos_validos.keys():\n",
    "        for turno in archivos_validos[lab].keys():\n",
    "            for triad in triads:\n",
    "                experimental_archivos = archivos_validos[lab][turno]['experimental']\n",
    "                base_archivos = archivos_validos[lab][turno]['base']\n",
    "\n",
    "                # Crear un diccionario para acceder a los archivos base por número\n",
    "                base_dict = {archivo['Número']: archivo['Ruta'] for archivo in base_archivos}\n",
    "\n",
    "                for exp_archivo in experimental_archivos:\n",
    "                    exp_num = exp_archivo['Número']\n",
    "                    exp_color = exp_archivo['Color']  # Obtener el color del archivo experimental\n",
    "                    exp_fecha = exp_archivo['Fecha']  # Obtener la fecha del archivo experimental\n",
    "                    exp_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Enstrophy_Experimental_Color\", triad, f\"sum_enstrophy_vorticity_{triad}_velocity_0{exp_num}med{exp_color}.txt\")\n",
    "\n",
    "                    # Usar glob para buscar el archivo experimental\n",
    "                    exp_archivos_encontrados = glob.glob(exp_ruta_pattern)\n",
    "\n",
    "                    if not exp_archivos_encontrados:\n",
    "                        continue\n",
    "\n",
    "                    exp_ruta = exp_archivos_encontrados[0]  # Toma el primer archivo encontrado\n",
    "                    exp_nombre = os.path.basename(exp_ruta)  # Obtener el nombre del archivo sin la ruta\n",
    "\n",
    "                    # Calcular la suma de módulos para el archivo experimental\n",
    "                    mod_exp_suma = sumar_modulos(exp_ruta)\n",
    "\n",
    "                    # Inicializar el diccionario para almacenar los resultados por fecha si no existe\n",
    "                    if exp_fecha not in resultados_por_fecha:\n",
    "                        resultados_por_fecha[exp_fecha] = {triad: [] for triad in triads}\n",
    "\n",
    "                    # Verificar si hay un archivo base con el mismo número y el siguiente\n",
    "                    for num in (exp_num, exp_num + 1):\n",
    "                        if num in base_dict:\n",
    "                            base_ruta_pattern = os.path.join(root_folder, f\"{exp_fecha} - {lab}\", f\"{exp_fecha}.{turno} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Enstrophy_Baseline\", triad, f\"sum_enstrophy_vorticity_{triad}_velocity_0{num}med*.txt\")\n",
    "\n",
    "                            # Usar glob para buscar el archivo base\n",
    "                            base_archivos_encontrados = glob.glob(base_ruta_pattern)\n",
    "\n",
    "                            if not base_archivos_encontrados:\n",
    "                                print(f\"No se encontró archivo base para el patrón: {base_ruta_pattern}\")\n",
    "                                continue\n",
    "\n",
    "                            base_ruta = base_archivos_encontrados[0]  # Toma el primer archivo base encontrado\n",
    "                            base_nombre = os.path.basename(base_ruta)  # Obtener el nombre del archivo base sin la ruta\n",
    "\n",
    "                            # Calcular la suma de módulos para el archivo base\n",
    "                            mod_base_suma = sumar_modulos(base_ruta)\n",
    "                            dif_porcentual = (abs(mod_exp_suma - mod_base_suma) / mod_base_suma) * 100\n",
    "\n",
    "                            # Determinar el resultado basado en la diferencia porcentual\n",
    "                            if dif_porcentual < 10:\n",
    "                                resultado = \"No hay evidencia de intervención.\"\n",
    "                            elif 10 <= dif_porcentual <= 33.9:\n",
    "                                resultado = \"Resultados inconclusos.\"\n",
    "                            else:  # valor > 33.9\n",
    "                                resultado = \"Evidencia de intervención satisfactoria.\"\n",
    "                                total_casos_satisfactorios += 1  # Aumentar contador total de casos satisfactorios\n",
    "\n",
    "                            # Almacenar el resultado en la lista de resultados para la triada y fecha\n",
    "                            resultados_por_fecha[exp_fecha][triad].append({\n",
    "                                'Laboratorio': lab,\n",
    "                                'Turno': turno,\n",
    "                                'Archivo_Experimental': exp_nombre,\n",
    "                                'Archivo_Base': base_nombre,\n",
    "                                'Modulo_Experimental': mod_exp_suma,\n",
    "                                'Modulo_Base': mod_base_suma,\n",
    "                                'Diferencia_Porcentual': dif_porcentual,\n",
    "                                'Resultado': resultado\n",
    "                            })\n",
    "\n",
    "    return resultados_por_fecha, total_casos_satisfactorios\n",
    "\n",
    "# Función para calcular la suma de módulos de los vectores en un archivo\n",
    "def sumar_modulos(filepath):\n",
    "    suma_modulos = 0\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                vector = parts[1].strip('()').split(',')\n",
    "                valores = np.array([float(num.strip()) for num in vector])\n",
    "                modulo = np.linalg.norm(valores)  # Calcular el módulo del vector\n",
    "                suma_modulos += modulo\n",
    "\n",
    "    return suma_modulos\n",
    "\n",
    "# Función para guardar los resultados como PDFs separados por experimento y color\n",
    "def guardar_resultados_como_pdfs(resultados_por_fecha, root_folder):\n",
    "    carpeta_pdfs = os.path.join(root_folder, \"PDFs\")\n",
    "    os.makedirs(carpeta_pdfs, exist_ok=True)\n",
    "\n",
    "    for fecha, resultados_triadas in resultados_por_fecha.items():\n",
    "        for triad, resultados_triadas_data in resultados_triadas.items():\n",
    "            for resultado in resultados_triadas_data:\n",
    "                archivo_nombre = f\"{resultado['Archivo_Experimental'].split('_')[2]}_{resultado['Archivo_Experimental'].split('_')[4]}.pdf\"\n",
    "                archivo_ruta = os.path.join(carpeta_pdfs, archivo_nombre)\n",
    "\n",
    "                # Generar el PDF (ejemplo simple con pandas)\n",
    "                df = pd.DataFrame([resultado])\n",
    "                df.to_csv(archivo_ruta.replace('.pdf', '.csv'), index=False)\n",
    "\n",
    "                print(f\"PDF guardado en: {archivo_ruta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import re\n",
    "\n",
    "def extraer_fecha(ruta):\n",
    "    # Definir el patrón para coincidir con el formato de fecha ddMmmYY (como 12Ago24)\n",
    "    patron_fecha = r\"\\d{2}[A-Za-z]{3}\\d{2}\"\n",
    "    \n",
    "    # Buscar la fecha en la cadena\n",
    "    coincidencia = re.search(patron_fecha, ruta)\n",
    "    \n",
    "    if coincidencia:\n",
    "        return coincidencia.group()  # Devolver la fecha encontrada\n",
    "    else:\n",
    "        return None  # Si no se encuentra, devolver None\n",
    "    \n",
    "# Función para decodificar el texto hexadecimal\n",
    "def decodificar_hex(hex_string):\n",
    "    try:\n",
    "        hex_cleaned = hex_string.strip().replace(' ', '')\n",
    "        decoded_string = bytes.fromhex(hex_cleaned).decode('ascii', errors='ignore').strip()\n",
    "        return decoded_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error decodificando: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para procesar el archivo y extraer la hora y el valor limpio\n",
    "def procesar_archivo_para_hora_y_texto(ruta_archivo):\n",
    "    resultados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            match_hora = re.search(r'(\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}\\.\\d{3})', linea)\n",
    "            if match_hora:\n",
    "                hora = match_hora.group(1).split(' ')[1]  # Obtener solo la parte de la hora\n",
    "                partes = linea.strip().split(',')\n",
    "                if len(partes) > 3:\n",
    "                    hex_data = partes[-1]\n",
    "                    decoded_value = decodificar_hex(hex_data)\n",
    "\n",
    "                    if decoded_value:\n",
    "                        resultados.append(f\"{hora}, {decoded_value}\")\n",
    "                    else:\n",
    "                        resultados.append(f\"{hora}, .\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Función para guardar los resultados en un archivo de salida\n",
    "def guardar_resultados(ruta_salida, resultados):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        for resultado in resultados:\n",
    "            archivo_salida.write(resultado + '\\n')\n",
    "\n",
    "# Función para construir la ruta del archivo a procesar\n",
    "def construir_ruta(root_folder, fecha, lab, turno, color, numero):\n",
    "    colorin = color.lower()\n",
    "    return os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", f\"{numero:02d}med{colorin}.log\")\n",
    "\n",
    "# Función para filtrar resultados\n",
    "def filtrar_resultados(resultados):\n",
    "    # Expresión regular para el formato deseado\n",
    "    pattern = r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}, \\d+\\.\\d{2}g$'\n",
    "    return [resultado for resultado in resultados if re.match(pattern, resultado.strip())]\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    fecha = extraer_fecha(root_folder)  # Asegúrate de definir esta función\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\", \"Naranja\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    ruta_archivo = construir_ruta(root_folder, fecha, lab, turno, color, numero)\n",
    "                    #print(ruta_archivo)\n",
    "                    \n",
    "                    colorin = color.lower()\n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        ruta_salida = os.path.join(os.path.dirname(ruta_archivo), f\"{numero:02d}med{colorin}.txt\")\n",
    "\n",
    "                        print(f\"\\nProcesando para Lab: {lab}, Turno: {turno}, Color: {color}, Archivo: {numero:02d}\")\n",
    "                        print(f\"Archivo a procesar: {ruta_archivo}\")\n",
    "                        \n",
    "                        # Procesar el archivo para extraer hora y texto limpio\n",
    "                        resultados = procesar_archivo_para_hora_y_texto(ruta_archivo)\n",
    "\n",
    "                        # Filtrar resultados\n",
    "                        resultados_filtrados = filtrar_resultados(resultados)\n",
    "\n",
    "                        # Guardar los resultados filtrados en el archivo de salida\n",
    "                        guardar_resultados(ruta_salida, resultados_filtrados)\n",
    "                        print(f\"Archivo encontrado: {ruta_archivo}\")\n",
    "                    #else:\n",
    "                     #   print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Función para extraer la fecha del nombre del directorio raíz\n",
    "def extraer_fecha(root_folder):\n",
    "    nombre_directorio = os.path.basename(root_folder)\n",
    "    match = re.search(r'(\\d{2}[A-Za-z]{3}\\d{2})', nombre_directorio)\n",
    "    \n",
    "    if match:\n",
    "        fecha_str = match.group(1)  # Captura la fecha en formato '16Sep24'\n",
    "        return fecha_str\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontró una fecha válida en el nombre del directorio {root_folder}.\")\n",
    "\n",
    "# Función para cargar y procesar los datos\n",
    "def cargar_y_procesar_datos(ruta_archivo):\n",
    "    datos_procesados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            partes = linea.split(',')\n",
    "            hora = partes[0].strip()  # Extrae la hora\n",
    "            valor = partes[1].strip()  # Extrae el valor con 'g'\n",
    "            \n",
    "            # Elimina la 'g' y convierte a float\n",
    "            valor_numero = float(valor.replace('g', ''))\n",
    "            # Aplica el tratamiento Dato * 9.81 / 5.00\n",
    "            valor_procesado = valor_numero * 9.81 / 5.00\n",
    "            \n",
    "            # Guarda la hora y el valor procesado\n",
    "            datos_procesados.append(f\"{hora}, {valor_procesado:.2f}\\n\")\n",
    "    \n",
    "    return datos_procesados\n",
    "\n",
    "# Función para guardar los datos procesados en un archivo .txt\n",
    "def guardar_datos_txt(datos_procesados, ruta_salida):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        archivo_salida.writelines(datos_procesados)\n",
    "    print(f\"Datos procesados guardados en: {ruta_salida}\")\n",
    "\n",
    "# Función principal para procesar y guardar archivos\n",
    "def procesar_archivos(root_folder):\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\", \"Naranja\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    fecha = extraer_fecha(root_folder)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    colorin = color.lower()\n",
    "                    ruta_archivo = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", f\"0{numero}med{colorin}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        # Cargar y procesar los datos del archivo\n",
    "                        datos_procesados = cargar_y_procesar_datos(ruta_archivo)\n",
    "\n",
    "                        # Definir la carpeta y el nombre del archivo de salida\n",
    "                        output_folder = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", \"procesados\")\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ruta_salida = os.path.join(output_folder, f\"resultados_0{numero}_{colorin}.txt\")\n",
    "\n",
    "                        print(ruta_salida)\n",
    "\n",
    "                        # Guardar los datos procesados en un archivo .txt\n",
    "                        guardar_datos_txt(datos_procesados, ruta_salida)\n",
    "                    #else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_archivos(root_folder)\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir un diccionario para mapear los nombres de los colores a los colores de matplotlib\n",
    "color_map = {\n",
    "    \"amarilla\": \"yellow\",\n",
    "    \"roja\": \"red\",\n",
    "    \"azul\": \"blue\",\n",
    "    \"verde\": \"green\",\n",
    "    \"morada\": \"purple\",\n",
    "    \"negra\": \"black\",\n",
    "    \"naranja\": \"orange\"\n",
    "}\n",
    "\n",
    "# Función para cargar datos y crear DataFrame\n",
    "def cargar_datos(ruta_archivo):\n",
    "    # Leer archivo en un DataFrame\n",
    "    df = pd.read_csv(ruta_archivo, header=None, names=[\"Timestamp\", \"Valor\"])\n",
    "    \n",
    "    # Convertir el timestamp a formato de tiempo de pandas para facilitar el graficado\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H:%M:%S.%f')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para graficar serie de tiempo como scatter plot con color especificado\n",
    "def graficar_serie_tiempo(df, titulo, ruta_salida, color):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['Timestamp'], df['Valor'], color=color)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Gravity (m/s^2)')\n",
    "    plt.title(titulo)\n",
    "    plt.grid()\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Guardar la gráfica en la ruta especificada\n",
    "    plt.savefig(ruta_salida)\n",
    "    plt.close()  # Cierra la gráfica para no mostrarla en la terminal\n",
    "    print(f\"Gráfica guardada en: {ruta_salida}\")\n",
    "\n",
    "# Función principal para procesar y graficar\n",
    "def procesar_y_graficar_archivos(root_folder):\n",
    "    fecha = extraer_fecha(root_folder)  # Usar la función extraer_fecha definida previamente\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    #colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\", \"Negra\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    colorin = color.lower()\n",
    "                    ruta_archivo = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", \"procesados\", f\"resultados_0{numero}_{colorin}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        # Cargar los datos del archivo procesado\n",
    "                        df = cargar_datos(ruta_archivo)\n",
    "                        \n",
    "                        # Crear el título de la gráfica\n",
    "                        titulo = f\"Time Series - Lab {lab} - Shift {turno} - {color} - Measure {numero}\"\n",
    "                        \n",
    "                        # Definir la carpeta y el nombre del archivo de salida para la gráfica\n",
    "                        output_folder = os.path.join(root_folder, f\"{fecha} - {lab}\", f\"{fecha}.{turno} - {lab}\", \"Pesajes\", \"graficas\")\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ruta_salida_grafica = os.path.join(output_folder, f\"graph_0{numero}_{colorin}.png\")\n",
    "                        \n",
    "                        # Obtener el color correspondiente a partir del nombre del archivo\n",
    "                        color_grafica = color_map.get(colorin, 'blue')  # Default a 'blue' si no se encuentra el color\n",
    "                        \n",
    "                        # Graficar la serie de tiempo como scatter plot con el color correspondiente\n",
    "                        graficar_serie_tiempo(df, titulo, ruta_salida_grafica, color_grafica)\n",
    "                    else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "\n",
    "# Ejecutar el procesamiento y graficado\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_y_graficar_archivos(root_folder)\n",
    "    print(\"\\nProcesamiento y graficado completo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Función para crear una copia de respaldo del archivo\n",
    "def backup_raw_file(filepath, root_folder, backup_folder):\n",
    "    relative_path = os.path.relpath(filepath, root_folder)\n",
    "    backup_path = os.path.join(backup_folder, relative_path)\n",
    "    os.makedirs(os.path.dirname(backup_path), exist_ok=True)\n",
    "    shutil.copy(filepath, backup_path)\n",
    "    print(f\"Archivo respaldado en: {backup_path}\")\n",
    "\n",
    "# Función para extraer estampas de tiempo\n",
    "def extract_timestamps(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    if lines:\n",
    "        first_line = re.match(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}', lines[0])\n",
    "        last_line = re.match(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}', lines[-1])\n",
    "        first_time = first_line.group(0) if first_line else \"NA\"\n",
    "        last_time = last_line.group(0) if last_line else \"NA\"\n",
    "        return first_time, last_time\n",
    "    return \"NA\", \"NA\"\n",
    "\n",
    "# Función para actualizar el formato de las líneas y registrar en la bitácora\n",
    "def update_file_format(filepath):\n",
    "    updated_lines = []\n",
    "    print(f\"Actualizando formato del archivo: {filepath}\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if re.match(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> \\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', line):\n",
    "                new_line = re.sub(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', '', line.strip())\n",
    "                updated_lines.append(new_line)\n",
    "            else:\n",
    "                updated_lines.append(line.strip())\n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        for line in updated_lines:\n",
    "            file.write(line + '\\n')\n",
    "    print(f\"Formato actualizado en: {filepath}\")\n",
    "\n",
    "# Función para escanear un directorio y procesar archivos con sufijos específicos\n",
    "def scan_directory(root_folder, file_suffixes, backup_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        dirs[:] = [d for d in dirs if d not in [\"Data Analysis\", \"Programas\"]]  # Excluir carpetas\n",
    "        for filename in files:\n",
    "            if any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                try:\n",
    "                    backup_raw_file(file_path, root_folder, backup_folder)\n",
    "                    update_file_format(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error procesando el archivo {file_path}: {e}\")\n",
    "\n",
    "# Configuración inicial\n",
    "\n",
    "backup_folder = os.path.join(root_folder, \"Backup Raw Data\")\n",
    "file_suffixes = [\".txt\"]\n",
    "\n",
    "# Realiza la búsqueda y procesa los archivos\n",
    "scan_directory(root_folder, file_suffixes, backup_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

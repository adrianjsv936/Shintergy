{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de Entradas\n",
    "\n",
    "1. [Timestamp Eliminator](#1)\n",
    "2. [Curl File Generator](#2)\n",
    "3. [Moduli Curl File Generator](#3)\n",
    "4. [General Velocity File Generator](#4)\n",
    "5. [Velocity Generator by Triad](#5)\n",
    "6. [Vorticity File Generator](#6)\n",
    "7. [Helicity Generator by Triads](#7)\n",
    "8. [First Integral of Helicity Generator](#8)\n",
    "9. [First Integral of Absolute Values of Helicity Generator](#9)\n",
    "10. [General Enstrophy File Generator](#10)\n",
    "11. [First Integral of Enstrophy](#11)\n",
    "12. [First Integral Moduli Baseline vs Experimental Color](#12)\n",
    "13. [Second Integral of Absolute Helicity Baseline vs Experimental Color](#13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este primer código se encarga de determinar la ruta en la que se encuentran los archivos que se van a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_folder = r\"F:\\Datos\\16Sep24\"\n",
    "\n",
    "# Horarios ajustados de mediciones de línea base y de intervenciones experimentales\n",
    "\n",
    "# Horarios de mediciones de línea base\n",
    "lab_b_matutino_base_times = {\n",
    "    '01': \"10:39:59.065\",\n",
    "    '02': \"11:19:56.844\",\n",
    "    '03': \"12:00:00.896\",\n",
    "    '04': \"12:39:59.139\",\n",
    "    '05': \"13:19:59.461\"\n",
    "}\n",
    "\n",
    "# Horarios de intervenciones experimentales\n",
    "lab_b_matutino_intervention_times = {\n",
    "    '01': \"10:59:59.414\",\n",
    "    '02': \"11:39:59.104\",\n",
    "    '03': \"12:19:59.778\",\n",
    "    '04': \"13:00:01.295\"\n",
    "}\n",
    "\n",
    "# Horarios de mediciones de línea base para Lab_A en turno matutino\n",
    "lab_a_matutino_base_times = {\n",
    "    '01': \"10:39:59.065\",\n",
    "    '02': \"11:19:56.844\",\n",
    "    '03': \"12:00:00.896\",\n",
    "    '04': \"12:39:59.139\",\n",
    "    '05': \"13:19:59.461\"\n",
    "}\n",
    "\n",
    "# Horarios de intervenciones experimentales para Lab_A en turno matutino\n",
    "lab_a_matutino_intervention_times = {\n",
    "    '01': \"10:59:59.414\",\n",
    "    '02': \"11:39:59.104\",\n",
    "    '03': \"12:19:59.778\",\n",
    "    '04': \"13:00:01.295\"\n",
    "}\n",
    "\n",
    "n_change = {\"Lab_Betta\":\"Lab_Betta\", \"Lab_Gamma\":\"Lab_Gamma\"}\n",
    "\n",
    "triad_names = {\n",
    "    \"FRT\": \"Frontal-Derecho-Superior\",  \n",
    "    \"PLB\": \"Trasero-Izquierdo-Inferior\",  \n",
    "    \"FLT\": \"Frontal-Izquierdo-Superior\",  \n",
    "    \"PRB\": \"Trasero-Derecho-Inferior\",  \n",
    "    \"FRB\": \"Frontal-Derecho-Inferior\",  \n",
    "    \"PLT\": \"Trasero-Izquierdo-Superior\",  \n",
    "    \"FLB\": \"Frontal-Izquierdo-Inferior\",  \n",
    "    \"PRT\": \"Trasero-Derecho-Superior\",  \n",
    "    \"RTB\": \"Derecho-Superior-Inferior\",  \n",
    "    \"FLP\": \"Frontal-Izquierdo-Trasero\",  \n",
    "    \"LTB\": \"Izquierdo-Superior-Inferior\",  \n",
    "    \"FRP\": \"Frontal-Derecho-Trasero\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Second Timestamp Eliminator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Función para actualizar el formato de las líneas\n",
    "def update_file_format(filepath):\n",
    "    updated_lines = []\n",
    "    print(f\"Actualizando formato del archivo: {filepath}\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            print(f\"Procesando línea: {line.strip()}\")  # Mostrar cada línea procesada\n",
    "            # Verificar si la línea contiene dos estampas de tiempo\n",
    "            if re.match(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> \\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', line):\n",
    "                # Extraer y conservar sólo desde la segunda estampa de tiempo hasta el final de la línea\n",
    "                new_line = re.sub(r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3} -> ', '', line.strip())\n",
    "                print(f\"Formato actualizado: {new_line}\")  # Mostrar la línea actualizada\n",
    "                updated_lines.append(new_line)\n",
    "            else:\n",
    "                updated_lines.append(line.strip())\n",
    "    \n",
    "    # Sobrescribir el archivo con el nuevo formato\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        for line in updated_lines:\n",
    "            file.write(line + '\\n')\n",
    "    print(f\"Formato actualizado en: {filepath}\")\n",
    "\n",
    "# Función para eliminar filas con valores menores a 3.0\n",
    "def filter_file(filepath):\n",
    "    filtered_lines = []\n",
    "    print(f\"Filtrando archivo: {filepath}\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            # Buscar el vector entre paréntesis\n",
    "            match = re.search(r'\\((.*?)\\)', line)\n",
    "            if match:\n",
    "                # Convertir los valores del vector a float\n",
    "                values = list(map(float, match.group(1).split(',')))\n",
    "                # Si todos los valores son mayores o iguales a 3.0, conservar la línea\n",
    "                if all(value >= 3.0 for value in values):\n",
    "                    filtered_lines.append(line.strip())\n",
    "                else:\n",
    "                    print(f\"Línea eliminada: {line.strip()}\")  # Mostrar la línea eliminada\n",
    "            else:\n",
    "                filtered_lines.append(line.strip())\n",
    "\n",
    "    # Sobrescribir el archivo con las líneas filtradas\n",
    "    with open(filepath, 'w', encoding='utf-8') as outfile:\n",
    "        for line in filtered_lines:\n",
    "            outfile.write(line + '\\n')\n",
    "    print(f\"Archivo filtrado guardado: {filepath}\")\n",
    "\n",
    "# Función para escanear un directorio y procesar archivos con sufijos específicos\n",
    "def scan_directory(root_folder, file_suffixes):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        dirs[:] = [d for d in dirs if d not in [\"Data Analysis\", \"Programas\"]]  # Excluir directorios\n",
    "        for filename in files:\n",
    "            if any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                update_file_format(file_path)  # Actualizar formato del archivo\n",
    "                filter_file(file_path)  # Filtrar el archivo\n",
    "\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\", \"mednaranja.txt\"]\n",
    "\n",
    "# Realiza la búsqueda y procesa los archivos\n",
    "scan_directory(root_folder, file_suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Curl File Generator}\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotacional .txt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_curl(data):\n",
    "    try:\n",
    "        Dx = Dy = Dz = 1.0\n",
    "        curl_x = (data[:, 1] - data[:, 0]) / Dx # (Trasero - Frontal) / Dx\n",
    "        curl_y = (data[:, 3] - data[:, 2]) / Dy # (Derecha - Izquierda) / Dy\n",
    "        curl_z = (data[:, 5] - data[:, 4]) / Dz # (Inferior - Superior) / Dz\n",
    "        return np.array([curl_x, curl_y, curl_z]).T\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating curl: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def adjust_timestamps(base_time, timestamps):\n",
    "    try:\n",
    "        base_datetime = datetime.strptime(base_time, '%H:%M:%S.%f')\n",
    "        adjusted_timestamps = []\n",
    "        for ts in timestamps:\n",
    "            time_delta = datetime.strptime(ts, '%H:%M:%S.%f') - datetime.strptime('00:00:00.000', '%H:%M:%S.%f')\n",
    "            new_datetime = base_datetime + time_delta\n",
    "            adjusted_timestamps.append(new_datetime.strftime('%H:%M:%S.%f')[:-3])\n",
    "        return adjusted_timestamps\n",
    "    except Exception as e:\n",
    "        print(f\"Error adjusting timestamps: {e}\")\n",
    "        return []\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder)\n",
    "    return None\n",
    "\n",
    "def process_files(input_folder, output_folder, base_times, file_suffixes=None):\n",
    "    generated_files = []\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]\n",
    "        for filename in files:\n",
    "            if \"00\" in filename:\n",
    "                continue\n",
    "            if file_suffixes:\n",
    "                if not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                    continue\n",
    "            else:\n",
    "                if not filename.endswith('mednegra.txt'):\n",
    "                    continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                if data.size == 0:\n",
    "                    continue\n",
    "                curls = calculate_curl(data)\n",
    "                if curls.size == 0:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            file_number = filename.split('mednegra.txt')[0][-2:] if not file_suffixes else filename[:2]\n",
    "            adjusted_timestamps = adjust_timestamps(base_times.get(file_number, \"00:00:00.000\"), timestamps)\n",
    "\n",
    "            output_filename = f\"curl_{filename}\"\n",
    "            output_file_path = os.path.join(output_folder, output_filename)\n",
    "            try:\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    for i, curl in enumerate(curls):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> ({curl[0]:.10f}, {curl[1]:.10f}, {curl[2]:.10f})\\n\")\n",
    "                generated_files.append(output_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {output_file_path}: {e}\")\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "def process_lab(root_folder, lab_name, base_times, intervention_times):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No experiment folder found for {lab_name}, {shift_name} shift.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing directory: {experiment_folder}\")\n",
    "\n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Baseline\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        generated_files = process_files(experiment_folder, output_folder, base_times)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Curl_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\"]\n",
    "        generated_files = process_files(experiment_folder, output_folder, intervention_times, file_suffixes)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = {\n",
    "    \"Lab_Betta\": {\n",
    "        \"base_times\": lab_a_matutino_base_times,\n",
    "        \"intervention_times\": lab_a_matutino_intervention_times,\n",
    "    }  ,\n",
    "    \"Lab_Gamma\": {\n",
    "        \"base_times\": lab_b_matutino_base_times,\n",
    "        \"intervention_times\": lab_b_matutino_intervention_times,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for lab_name, times in labs.items():\n",
    "    process_lab(root_folder, lab_name, times[\"base_times\"], times[\"intervention_times\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Moduli Curl File Generator}\n",
    "\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Magnitud rotacional.txt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                timestamps.append(parts[0])\n",
    "                numbers = parts[1].strip('()').split(', ')\n",
    "                data.append([float(num) for num in numbers])\n",
    "    return timestamps, np.array(data)\n",
    "\n",
    "def calculate_magnitude(data):\n",
    "    return np.linalg.norm(data, axis=1)\n",
    "\n",
    "def adjust_timestamps(base_time, timestamps):\n",
    "    base_datetime = datetime.strptime(base_time, '%H:%M:%S.%f')\n",
    "    adjusted_timestamps = []\n",
    "    for ts in timestamps:\n",
    "        time_delta = datetime.strptime(ts, '%H:%M:%S.%f') - datetime.strptime('00:00:00.000', '%H:%M:%S.%f')\n",
    "        new_datetime = base_datetime + time_delta\n",
    "        adjusted_timestamps.append(new_datetime.strftime('%H:%M:%S.%f')[:-3])\n",
    "    return adjusted_timestamps\n",
    "\n",
    "def process_files(input_folder, output_folder, base_times, file_suffixes=None):\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]  # Excluir directorios que contienen 'Data Analysis'\n",
    "        for filename in files:\n",
    "            if \"00\" in filename:  # Omitir archivos con \"00\" en el nombre\n",
    "                continue\n",
    "            if file_suffixes:\n",
    "                if not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                    continue\n",
    "            else:\n",
    "                if not filename.endswith('mednegra.txt'):\n",
    "                    continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                magnitudes = calculate_magnitude(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            file_number = filename.split('mednegra.txt')[0][-2:] if not file_suffixes else filename[:2]\n",
    "            adjusted_timestamps = adjust_timestamps(base_times.get(file_number, \"00:00:00.000\"), timestamps)\n",
    "\n",
    "            magnitude_filename = f\"magnitude_{filename}\"\n",
    "            magnitude_file_path = os.path.join(output_folder, magnitude_filename)\n",
    "            try:\n",
    "                with open(magnitude_file_path, \"w\") as f:\n",
    "                    for i, magnitude in enumerate(magnitudes):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> {magnitude:.10f}\\n\")\n",
    "                generated_files.append(magnitude_filename)  # Agregar el nombre del archivo generado a la lista\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {magnitude_file_path}: {e}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab(root_folder, lab_name, base_times, intervention_times):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "\n",
    "        input_folder = experiment_folder\n",
    "        if not os.path.exists(input_folder):\n",
    "            continue\n",
    "        \n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(input_folder, \"Data Analysis\", \"Processing Data\", \"Moduli_Curl_Baseline\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        generated_files = process_files(input_folder, output_folder, base_times)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(input_folder, \"Data Analysis\", \"Processing Data\", \"Moduli_Curl_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednaranja.txt\"]\n",
    "        generated_files = process_files(input_folder, output_folder, intervention_times, file_suffixes)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = {\n",
    "    \"Lab_Betta\": {\n",
    "        \"base_times\": lab_a_matutino_base_times,\n",
    "        \"intervention_times\": lab_a_matutino_intervention_times,\n",
    "    }  \n",
    ",\n",
    "    \"Lab_Gamma\": {\n",
    "        \"base_times\": lab_b_matutino_base_times,\n",
    "        \"intervention_times\": lab_b_matutino_intervention_times,\n",
    "    }\n",
    "}\n",
    "\n",
    "for lab_name, times in labs.items():\n",
    "    process_lab(root_folder, lab_name, times[\"base_times\"], times[\"intervention_times\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{General Velocity File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Código requsito previo para correr el código de las 12 triadas de velocidad-------------- Velocidad.txt (no triadas)\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def read_sensor_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])  # Guardar marca de tiempo\n",
    "                    numbers = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(num) for num in numbers])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_velocities(data, delta_t):\n",
    "    # Calcular velocidades usando diferencias hacia adelante\n",
    "    velocities = (data[1:] - data[:-1]) / delta_t\n",
    "    return velocities\n",
    "\n",
    "def adjust_timestamps(base_time, timestamps):\n",
    "    try:\n",
    "        base_datetime = datetime.strptime(base_time, '%H:%M:%S.%f')\n",
    "        adjusted_timestamps = []\n",
    "        for ts in timestamps[:-1]:  # Excluir la última marca de tiempo\n",
    "            current_time = datetime.strptime(ts, '%H:%M:%S.%f')\n",
    "            adjusted_time = base_datetime + (current_time - datetime.strptime(timestamps[0], '%H:%M:%S.%f'))\n",
    "            adjusted_timestamps.append(adjusted_time.strftime('%H:%M:%S.%f')[:-3])\n",
    "        return adjusted_timestamps\n",
    "    except Exception as e:\n",
    "        print(f\"Error adjusting timestamps: {e}\")\n",
    "        return []\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_files(input_folder, output_folder, delta_t, base_times, file_suffixes=None):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Asegurar que el directorio de salida exista\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        dirs[:] = [d for d in dirs if \"Data Analysis\" not in os.path.join(root, d)]  # Excluir directorios que contienen 'Data Analysis'\n",
    "        for filename in files:\n",
    "            if \"00\" in filename:  # Omitir archivos con \"00\" en el nombre\n",
    "                continue\n",
    "            if file_suffixes:\n",
    "                if not any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "                    continue\n",
    "            else:\n",
    "                if not filename.endswith('mednegra.txt'):\n",
    "                    continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                timestamps, data = read_sensor_data(file_path)\n",
    "                if data.size == 0:\n",
    "                    continue\n",
    "                velocities = calculate_velocities(data, delta_t)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            file_number = filename.split('mednegra.txt')[0][-2:] if not file_suffixes else filename[:2]\n",
    "            adjusted_timestamps = adjust_timestamps(base_times.get(file_number, \"00:00:00.000\"), timestamps)\n",
    "\n",
    "            output_filename = f\"velocity_{filename}\"\n",
    "            output_file_path = os.path.join(output_folder, output_filename)\n",
    "            try:\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    for i, velocity in enumerate(velocities):\n",
    "                        f.write(f\"{adjusted_timestamps[i]} -> ({velocity[0]:.10f}, {velocity[1]:.10f}, {velocity[2]:.10f}, {velocity[3]:.10f}, {velocity[4]:.10f}, {velocity[5]:.10f})\\n\")\n",
    "                generated_files.append(output_filename)  # Agregar el nombre del archivo generado a la lista\n",
    "            except Exception as e:\n",
    "                print(f\"Error writing file {output_file_path}: {e}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def process_lab(root_folder, lab_name, base_times, intervention_times, delta_t):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "        \n",
    "        # Procesar la línea base\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Velocity_Baseline\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        generated_files = process_files(experiment_folder, output_folder, delta_t, base_times)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        output_folder = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\", \"Velocity_Experimental_Color\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\"]\n",
    "        generated_files = process_files(experiment_folder, output_folder, delta_t, intervention_times, file_suffixes)\n",
    "        \n",
    "        print(f\"Generated files for {lab_name}, {shift_name} shift Experimental:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = {\n",
    "    \"Lab_Betta\": {\n",
    "        \"base_times\": lab_a_matutino_base_times,\n",
    "        \"intervention_times\": lab_a_matutino_intervention_times,\n",
    "    }  \n",
    ",\n",
    "    \"Lab_Gamma\": {\n",
    "        \"base_times\": lab_b_matutino_base_times,\n",
    "        \"intervention_times\": lab_b_matutino_intervention_times,\n",
    "    }\n",
    "\n",
    "}\n",
    "delta_t = 0.01  # Paso de tiempo para el cálculo de velocidades\n",
    "\n",
    "for lab_name, times in labs.items():\n",
    "    process_lab(root_folder, lab_name, times[\"base_times\"], times[\"intervention_times\"], delta_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Velocity Generator by Triad}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de velocidades por triada .txt\n",
    "import os\n",
    "\n",
    "def extract_velocity_components(root_folder):\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "    \n",
    "    triad_codes = {\n",
    "        (0, 2, 4): \"FRT\",  \n",
    "        (1, 3, 5): \"PLB\",  \n",
    "        (0, 3, 4): \"FLT\",  \n",
    "        (1, 2, 5): \"PRB\",  \n",
    "        (0, 2, 5): \"FRB\",  \n",
    "        (1, 3, 4): \"PLT\",  \n",
    "        (0, 3, 5): \"FLB\",  \n",
    "        (1, 2, 4): \"PRT\",  \n",
    "        (2, 4, 5): \"RTB\",  \n",
    "        (0, 1, 3): \"FLP\",  \n",
    "        (3, 4, 5): \"LTB\",  \n",
    "        (0, 1, 2): \"FRP\"\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        if \"Velocity_Baseline\" in root or \"Velocity_Experimental_Color\" in root:\n",
    "            # Usar la carpeta actual, sin agregar '_Component'\n",
    "            new_folder = root\n",
    "            \n",
    "            # Iterar sobre cada archivo en la carpeta original\n",
    "            for filename in files:\n",
    "                if filename.startswith(\"v\") and filename.endswith(\".txt\"):\n",
    "                    path = os.path.join(root, filename)\n",
    "                    \n",
    "                    with open(path, 'r') as file:\n",
    "                        file_content = file.readlines()  # Leer todo el contenido una vez\n",
    "                        \n",
    "                        # Generar archivos por cada triada y usando su código del diccionario\n",
    "                        for triad, code in triad_codes.items():\n",
    "                            triad_folder = os.path.join(new_folder, code)\n",
    "                            os.makedirs(triad_folder, exist_ok=True)  # Crear la carpeta de la triada si no existe\n",
    "                            new_path = os.path.join(triad_folder, f\"Velocity_{code}_\" + filename)\n",
    "                            \n",
    "                            with open(new_path, 'w') as triad_file:\n",
    "                                for line in file_content:\n",
    "                                    time_stamp, velocities = line.strip().split(' -> ')\n",
    "                                    v = eval(velocities)\n",
    "                                    selected_components = tuple(v[j] for j in triad)\n",
    "                                    triad_file.write(f\"{time_stamp} -> {selected_components}\\n\")\n",
    "                            \n",
    "                            # Almacenar solo el nombre de la carpeta de cabecera y los nombres de los archivos generados\n",
    "                            header_folder = os.path.basename(triad_folder)\n",
    "                            file_name = os.path.basename(new_path)\n",
    "                            generated_files.append(f\"{header_folder}/{file_name}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "# La ruta base donde están las carpetas de origen\n",
    "generated_files = extract_velocity_components(root_folder)\n",
    "\n",
    "# Imprimir los nombres de los archivos generados\n",
    "print(\"Generated files:\")\n",
    "for file in generated_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Vorticity File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorticidad .txt\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_velocity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])  # Guardar timestamps directamente del archivo\n",
    "                    velocity_components = parts[1].strip('()').split(', ')\n",
    "                    if len(velocity_components) == 6:  # Asegurarse de que haya 6 componentes\n",
    "                        data.append([float(v) for v in velocity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_vorticity(velocities, indices):\n",
    "    if velocities.ndim != 2 or velocities.shape[1] != 3:\n",
    "        print(f\"velocities.ndim: {velocities.ndim}, velocities.shape: {velocities.shape}\")\n",
    "        raise ValueError(\"El array de velocidades debe ser bidimensional con tres columnas.\")\n",
    "    \n",
    "    Vx, Vy, Vz = indices\n",
    "    dvz_dy = np.gradient(velocities[:, Vz], axis=0)\n",
    "    dvy_dz = np.gradient(velocities[:, Vy], axis=0)\n",
    "    dvx_dz = np.gradient(velocities[:, Vx], axis=0)\n",
    "    dvz_dx = np.gradient(velocities[:, Vz], axis=0)\n",
    "    dvy_dx = np.gradient(velocities[:, Vy], axis=0)\n",
    "    dvx_dy = np.gradient(velocities[:, Vx], axis=0)\n",
    "\n",
    "    omega_x = dvz_dy - dvy_dz\n",
    "    omega_y = dvx_dz - dvz_dx\n",
    "    omega_z = dvy_dx - dvx_dy\n",
    "\n",
    "    return np.array([omega_x, omega_y, omega_z]).T\n",
    "\n",
    "def save_vorticity_with_timestamps(timestamps, vorticities, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)  # Crear el directorio si no existe\n",
    "    print(f\"Saving vorticity data to: {filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            for ts, vort in zip(timestamps, vorticities):\n",
    "                formatted_output = f\"{ts} -> ({vort[0]:.10f}, {vort[1]:.10f}, {vort[2]:.10f})\\n\"\n",
    "                f.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files(input_folder, output_folder, file_suffixes, triads):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return []\n",
    "    \n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if any(filename.endswith(suffix) for suffix in file_suffixes):\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing file: {filepath}\")\n",
    "            timestamps, velocities = read_velocity_data(filepath)\n",
    "            \n",
    "            if velocities.ndim != 2 or velocities.shape[1] != 6:\n",
    "                print(f\"El archivo {filename} no contiene datos de velocidad válidos. Dimensiones: {velocities.shape}\")\n",
    "                continue\n",
    "            \n",
    "            for triad in triads:\n",
    "                if triad in triad_codes:\n",
    "                    code = triad_codes[triad]\n",
    "                    triad_folder = os.path.join(output_folder, code)\n",
    "                    os.makedirs(triad_folder, exist_ok=True)  # Crear carpeta para la triada si no existe\n",
    "                    velocities_triad = velocities[:, triad]\n",
    "                    \n",
    "                    print(f\"Calculating vorticity for triad {code}: {filename}\")\n",
    "                    vorticity = calculate_vorticity(velocities_triad, [0, 1, 2])\n",
    "                    \n",
    "                    output_file = f\"vorticity_{code}_{filename}\"\n",
    "                    save_vorticity_with_timestamps(timestamps, vorticity, os.path.join(triad_folder, output_file))\n",
    "                    \n",
    "                    generated_files.append(f\"{os.path.basename(triad_folder)}/{output_file}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab(root_folder, lab_name, file_suffixes, triads):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "\n",
    "        input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "        \n",
    "        # Procesar la línea base\n",
    "        input_folder = os.path.join(input_folder_base, \"Velocity_Baseline\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Vorticity_Baseline\")\n",
    "        generated_files = process_files(input_folder, output_folder, file_suffixes, triads)\n",
    "        print(f\"Generated vorticity files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        input_folder = os.path.join(input_folder_base, \"Velocity_Experimental_Color\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\")\n",
    "        generated_files = process_files(input_folder, output_folder, file_suffixes, triads)\n",
    "        print(f\"Generated vorticity files for {lab_name}, {shift_name} shift Experimental Color:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Define the root folder and suffixes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "# Define the triads\n",
    "triads = [\n",
    "    (0, 2, 4),  # T1 (FRONTAL, DERECHA, SUPERIOR)\n",
    "    (1, 3, 5),  # T2 (TRASERO, IZQUIERDA, INFERIOR)\n",
    "    (0, 3, 4),  # T3 (FRONTAL, IZQUIERDA, SUPERIOR)\n",
    "    (1, 2, 5),  # T4 (TRASERO, DERECHA, INFERIOR)\n",
    "    (0, 2, 5),  # T5 (FRONTAL, DERECHA, INFERIOR)\n",
    "    (1, 3, 4),  # T6 (TRASERO, IZQUIERDA, SUPERIOR)\n",
    "    (0, 3, 5),  # T7 (FRONTAL, IZQUIERDA, INFERIOR)\n",
    "    (1, 2, 4),  # T8 (TRASERO, DERECHA, SUPERIOR)\n",
    "    (2, 4, 5),  # T9 (DERECHA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 3),  # T10 (IZQUIERDA, FRONTAL, TRASERO)\n",
    "    (3, 4, 5),  # T11 (IZQUIERDA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 2)   # T12 (DERECHA, FRONTAL, TRASERO)\n",
    "]\n",
    "\n",
    "triad_codes = {\n",
    "        (0, 2, 4): \"FRT\",  \n",
    "        (1, 3, 5): \"PLB\",  \n",
    "        (0, 3, 4): \"FLT\",  \n",
    "        (1, 2, 5): \"PRB\",  \n",
    "        (0, 2, 5): \"FRB\",  \n",
    "        (1, 3, 4): \"PLT\",  \n",
    "        (0, 3, 5): \"FLB\",  \n",
    "        (1, 2, 4): \"PRT\",  \n",
    "        (2, 4, 5): \"RTB\",  \n",
    "        (0, 1, 3): \"FLP\",  \n",
    "        (3, 4, 5): \"LTB\",  \n",
    "        (0, 1, 2): \"FRP\"\n",
    "    }\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name, file_suffixes, triads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Helicity Generator by Triads}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helicidad general .txt\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_velocity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    velocity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in velocity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_helicity(velocities, vorticities, indices, volume):\n",
    "    selected_velocities = velocities[:, indices]\n",
    "    helicity = np.einsum('ij,ij->i', selected_velocities, vorticities) * volume\n",
    "    return helicity\n",
    "\n",
    "def format_and_save_helicities(timestamps, helicities, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, helicity in zip(timestamps, helicities):\n",
    "                formatted_output = f\"{timestamp} -> {helicity:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files(input_folder_velocity, input_folder_vorticity, output_folder, file_suffixes, volume, triads):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    generated_files = []\n",
    "    \n",
    "    for suffix in file_suffixes:\n",
    "        for filename in os.listdir(input_folder_velocity):\n",
    "            if filename.endswith(suffix):\n",
    "                base_filename = filename.split('.')[0]\n",
    "                filepath_velocity = os.path.join(input_folder_velocity, filename)\n",
    "                \n",
    "                for triad in triads:\n",
    "                    if triad in triad_codes:\n",
    "                        code = triad_codes[triad]\n",
    "                        triad_folder = os.path.join(input_folder_vorticity, code)\n",
    "                        filepath_vorticity = os.path.join(triad_folder, f\"vorticity_{code}_{base_filename}.txt\")\n",
    "                        \n",
    "                        timestamps, velocities = read_velocity_data(filepath_velocity)\n",
    "                        if velocities.size == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        _, vorticity = read_velocity_data(filepath_vorticity)\n",
    "\n",
    "                        if vorticity.size == 0:\n",
    "                            print(f\"Skipping file {filename} due to missing vorticity data for triad {code}.\")\n",
    "                            continue\n",
    "                        \n",
    "                        helicity = calculate_helicity(velocities, vorticity, triad, volume)\n",
    "                        \n",
    "                        output_filename = f\"helicity_{code}_{base_filename}.txt\"\n",
    "                        output_path = os.path.join(output_folder, code, output_filename)\n",
    "                        \n",
    "                        format_and_save_helicities(timestamps, helicity, output_path)\n",
    "                        \n",
    "                        generated_files.append(f\"{os.path.basename(output_folder)}/{code}/{output_filename}\")\n",
    "\n",
    "    return generated_files\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab(root_folder, lab_name, file_suffixes, volume, triads):\n",
    "    for shift in ['V', 'M']:\n",
    "        shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "        experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "        if not experiment_folder:\n",
    "            print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "            continue\n",
    "\n",
    "        input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "        \n",
    "        if not os.path.exists(input_folder_base):\n",
    "            print(f\"Cannot access directory: {input_folder_base}\")\n",
    "            continue\n",
    "\n",
    "        # Procesar la línea base\n",
    "        input_folder_velocity = os.path.join(input_folder_base, \"Velocity_Baseline\")\n",
    "        input_folder_vorticity = os.path.join(input_folder_base, \"Vorticity_Baseline\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Helicity_Baseline\")\n",
    "        generated_files = process_files(input_folder_velocity, input_folder_vorticity, output_folder, file_suffixes, volume, triads)\n",
    "        print(f\"Generated helicity files for {lab_name}, {shift_name} shift Baseline:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Procesar las intervenciones\n",
    "        input_folder_velocity = os.path.join(input_folder_base, \"Velocity_Experimental_Color\")\n",
    "        input_folder_vorticity = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\")\n",
    "        output_folder = os.path.join(input_folder_base, \"Helicity_Experimental_Color\")\n",
    "        generated_files = process_files(input_folder_velocity, input_folder_vorticity, output_folder, file_suffixes, volume, triads)\n",
    "        print(f\"Generated helicity files for {lab_name}, {shift_name} shift Experimental Color:\")\n",
    "        for file in generated_files:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "# Define the root folder and suffixes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "\n",
    "# Volumes for each lab\n",
    "volumes = {\n",
    "    \"Lab_Betta\": 7.3195, # Volume for Lab A\n",
    "    \"Lab_Gamma\": 6.8342  # Volume for Lab B (example)\n",
    "}\n",
    "\n",
    "# Triads\n",
    "triads = [\n",
    "    (0, 2, 4),  # T1 (FRONTAL, DERECHA, SUPERIOR)\n",
    "    (1, 3, 5),  # T2 (TRASERO, IZQUIERDA, INFERIOR)\n",
    "    (0, 3, 4),  # T3 (FRONTAL, IZQUIERDA, SUPERIOR)\n",
    "    (1, 2, 5),  # T4 (TRASERO, DERECHA, INFERIOR)\n",
    "    (0, 2, 5),  # T5 (FRONTAL, DERECHA, INFERIOR)\n",
    "    (1, 3, 4),  # T6 (TRASERO, IZQUIERDA, SUPERIOR)\n",
    "    (0, 3, 5),  # T7 (FRONTAL, IZQUIERDA, INFERIOR)\n",
    "    (1, 2, 4),  # T8 (TRASERO, DERECHA, SUPERIOR)\n",
    "    (2, 4, 5),  # T9 (DERECHA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 3),  # T10 (IZQUIERDA, FRONTAL, TRASERO)\n",
    "    (3, 4, 5),  # T11 (IZQUIERDA, SUPERIOR, INFERIOR)\n",
    "    (0, 1, 2)   # T12 (DERECHA, FRONTAL, TRASERO)\n",
    "]\n",
    "\n",
    "# Triad codes\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    process_lab(root_folder, lab_name, file_suffixes, volumes[lab_name], triads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Helicity Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral Helicidad .txt\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_file(file_path, output_file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    accumulated_helicity = 0\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        time_stamp, value = line.split('->')\n",
    "        value = float(value.strip())\n",
    "        accumulated_helicity += value\n",
    "        results.append(f\"{time_stamp.strip()} -> {accumulated_helicity:.10f}\\n\")\n",
    "\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.writelines(results)\n",
    "\n",
    "def generate_accumulated_helicity(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_{filename}\"\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                process_file(file_path, output_file_path)\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar los acumulados de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Helicity_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_helicity(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated helicity files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar los acumulados de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Helicity_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_helicity(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated helicity files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes)\n",
    "\n",
    "print(\"Processing complete. All accumulated helicity results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Absolute Values of Helicity}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral absoluta helicidad .txt\n",
    "import os\n",
    "import re\n",
    "\n",
    "def process_file_absolute(file_path, output_file_path):\n",
    "    # Leer las líneas del archivo\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    # Extraer los valores de helicidad y calcular la suma de los valores absolutos\n",
    "    absolute_helicity_sum = 0\n",
    "    results = []\n",
    "    for line in lines:\n",
    "        time_stamp, value = line.split('->')\n",
    "        value = abs(float(value.strip()))  # Uso de valor absoluto\n",
    "        absolute_helicity_sum += value\n",
    "        results.append(f\"{time_stamp.strip()} -> {absolute_helicity_sum:.10f}\\n\")\n",
    "\n",
    "    # Escribir los resultados de la suma de valores absolutos en un nuevo archivo\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.writelines(results)\n",
    "\n",
    "def generate_absolute_helicity_sum(input_directory, output_directory):\n",
    "    # Crear el directorio de salida si no existe\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Leer todos los archivos en el directorio especificado\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_absolute_{filename}\"  # Cambio en el nombre del archivo de salida para reflejar los valores absolutos\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                # Procesar el archivo\n",
    "                process_file_absolute(file_path, output_file_path)\n",
    "                # Imprimir en consola el nombre del archivo generado\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_absolute_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar los acumulados absolutos de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Absolute_Helicity_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_absolute_helicity_sum(input_directory, output_directory)\n",
    "            print(f\"Generated absolute helicity sum files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar los acumulados absolutos de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Helicity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Absolute_Helicity_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_absolute_helicity_sum(input_directory, output_directory)\n",
    "            print(f\"Generated absolute helicity sum files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_absolute_helicity(root_folder, lab_name, shift, file_suffixes, triad_codes)\n",
    "\n",
    "print(\"Processing complete. All absolute helicity sum results are saved in their respective directories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{General Enstrophy File Generator}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enstrofía general .txt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_vorticity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    vorticity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in vorticity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_enstrophy(vorticities, volume):\n",
    "    # Enstrophy calculation: sum of the squares of vorticity components, multiplied by volume\n",
    "    enstrophy = np.sum(vorticities**2, axis=1) * volume\n",
    "    return enstrophy\n",
    "\n",
    "def format_and_save_enstrophies(timestamps, enstrophies, filename):\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, enstrophy in zip(timestamps, enstrophies):\n",
    "                formatted_output = f\"{timestamp} -> {enstrophy:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files_enstrophy(input_folder, output_folder, file_suffixes, volume):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for filename in files:\n",
    "            for file_suffix in file_suffixes:\n",
    "                if filename.endswith(file_suffix):\n",
    "                    base_filename = filename[:-len(file_suffix)]  # Correctly removing the suffix\n",
    "                    \n",
    "                    filepath_vorticity = os.path.join(root, filename)\n",
    "                    \n",
    "                    timestamps, vorticity = read_vorticity_data(filepath_vorticity)\n",
    "                    if vorticity.size == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    enstrophy = calculate_enstrophy(vorticity, volume)\n",
    "                    \n",
    "                    output_filename = f\"enstrophy_{base_filename}{file_suffix}\"\n",
    "                    output_filepath = os.path.join(output_folder, output_filename)\n",
    "                    \n",
    "                    format_and_save_enstrophies(timestamps, enstrophy, output_filepath)\n",
    "                    generated_files.append(f\"{os.path.basename(output_folder)}/{output_filename}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_enstrophy(root_folder, lab_name, shift, file_suffixes, volume, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar enstrofia de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Vorticity_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Enstrophy_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generated_files = process_files_enstrophy(input_directory, output_directory, file_suffixes, volume)\n",
    "            print(f\"Generated enstrophy files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}:\")\n",
    "            for file in generated_files:\n",
    "                print(f\"  - {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar enstrofia de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Vorticity_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Enstrophy_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generated_files = process_files_enstrophy(input_directory, output_directory, file_suffixes, volume)\n",
    "            print(f\"Generated enstrophy files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}:\")\n",
    "            for file in generated_files:\n",
    "                print(f\"  - {file}\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Volumes for each lab\n",
    "volumes = {\n",
    "    \"Lab_Betta\": 7.3195 , # Volume for Lab A\n",
    "    \"Lab_Gamma\": 6.8342   # Volume for Lab B (example)\n",
    "}\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_enstrophy(root_folder, lab_name, shift, file_suffixes, volumes[lab_name], triad_codes)\n",
    "\n",
    "print(\"Processing complete. All enstrophy results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral of Enstrophy}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Integral Enstrofía\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def read_vorticity_data(filepath):\n",
    "    data = []\n",
    "    timestamps = []\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split(' -> ')\n",
    "                if len(parts) > 1:\n",
    "                    timestamps.append(parts[0])\n",
    "                    vorticity_components = parts[1].strip('()').split(', ')\n",
    "                    data.append([float(v) for v in vorticity_components])\n",
    "        return timestamps, np.array(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return [], np.array([])\n",
    "\n",
    "def calculate_enstrophy(vorticities, volume):\n",
    "    # Enstrophy calculation: sum of the squares of vorticity components, multiplied by volume\n",
    "    enstrophy = np.sum(vorticities**2, axis=1) * volume\n",
    "    return enstrophy\n",
    "\n",
    "def format_and_save_enstrophies(timestamps, enstrophies, filename):\n",
    "    try:\n",
    "        with open(filename, \"w\") as file:\n",
    "            for timestamp, enstrophy in zip(timestamps, enstrophies):\n",
    "                formatted_output = f\"{timestamp} -> {enstrophy:.10f}\\n\"\n",
    "                file.write(formatted_output)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing file {filename}: {e}\")\n",
    "\n",
    "def process_files_enstrophy(input_folder, output_folder, file_suffixes, volume):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    generated_files = []  # Lista para almacenar los nombres de los archivos generados\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        for file_suffix in file_suffixes:\n",
    "            if filename.endswith(file_suffix):\n",
    "                base_filename = filename[:-len(file_suffix)]  # Correctly removing the suffix\n",
    "                \n",
    "                filepath_vorticity = os.path.join(input_folder, filename)\n",
    "                \n",
    "                timestamps, vorticity = read_vorticity_data(filepath_vorticity)\n",
    "                if vorticity.size == 0:\n",
    "                    continue\n",
    "                \n",
    "                enstrophy = calculate_enstrophy(vorticity, volume)\n",
    "                \n",
    "                output_filename = f\"enstrophy_{base_filename}{file_suffix}\"\n",
    "                output_filepath = os.path.join(output_folder, output_filename)\n",
    "                \n",
    "                format_and_save_enstrophies(timestamps, enstrophy, output_filepath)\n",
    "                generated_files.append(f\"{os.path.basename(output_folder)}/{output_filename}\")\n",
    "\n",
    "    return generated_files  # Devolver la lista de archivos generados\n",
    "\n",
    "def accumulate_enstrophy(file_path, output_file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "        accumulated_enstrophy = 0\n",
    "        results = []\n",
    "        for line in lines:\n",
    "            time_stamp, value = line.split('->')\n",
    "            value = float(value.strip())\n",
    "            accumulated_enstrophy += value\n",
    "            results.append(f\"{time_stamp.strip()} -> {accumulated_enstrophy:.10f}\\n\")\n",
    "\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.writelines(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "def generate_accumulated_enstrophy(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                output_filename = f\"sum_{filename}\"\n",
    "                output_file_path = os.path.join(output_directory, output_filename)\n",
    "                accumulate_enstrophy(file_path, output_file_path)\n",
    "                print(f\"Generated file: {output_filename}\")\n",
    "\n",
    "def find_experiment_folder(root_folder, lab, shift):\n",
    "    date_regex = re.compile(r'\\d{2}[A-Za-z]{3}\\d{2}')\n",
    "    for folder in os.listdir(root_folder):\n",
    "        if date_regex.search(folder) and lab in folder:\n",
    "            for subfolder in os.listdir(os.path.join(root_folder, folder)):\n",
    "                if date_regex.search(subfolder) and shift in subfolder:\n",
    "                    return os.path.join(root_folder, folder, subfolder), date_regex.search(folder).group(0)\n",
    "    return None, None\n",
    "\n",
    "def process_lab_accumulated_enstrophy(root_folder, lab_name, shift, file_suffixes, volume, triad_codes):\n",
    "    shift_name = \"Vespertino\" if shift == 'V' else \"Matutino\"\n",
    "    experiment_folder, date_str = find_experiment_folder(root_folder, lab_name, shift)\n",
    "    if not experiment_folder:\n",
    "        print(f\"No se encontró ningún directorio para {lab_name}, {shift}.\")\n",
    "        return\n",
    "\n",
    "    input_folder_base = os.path.join(experiment_folder, \"Data Analysis\", \"Processing Data\")\n",
    "    \n",
    "    if not os.path.exists(input_folder_base):\n",
    "        print(f\"Skipping {shift_name} shift for {lab_name} as directory does not exist: {input_folder_base}\")\n",
    "        return\n",
    "\n",
    "    # Procesar enstrofia acumulada de Baseline para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Enstrophy_Baseline\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Enstrophy_Baseline\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_enstrophy(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated enstrophy files for {lab_name}, {shift_name} shift Baseline, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Baseline Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "    # Procesar enstrofia acumulada de Experimental Color para cada triada\n",
    "    for triad, triad_code in triad_codes.items():\n",
    "        input_directory = os.path.join(input_folder_base, \"Enstrophy_Experimental_Color\", triad_code)\n",
    "        output_directory = os.path.join(input_folder_base, \"Sum_Enstrophy_Experimental_Color\", triad_code)\n",
    "        if os.path.exists(input_directory):\n",
    "            generate_accumulated_enstrophy(input_directory, output_directory)\n",
    "            print(f\"Generated accumulated enstrophy files for {lab_name}, {shift_name} shift Experimental Color, Triad {triad_code}.\")\n",
    "        else:\n",
    "            print(f\"Skipping {shift_name} shift for {lab_name} Experimental Color Triad {triad_code} as directory does not exist: {input_directory}\")\n",
    "\n",
    "# Define the root folder, suffixes, and triad codes\n",
    "file_suffixes = [\"medroja.txt\", \"medmorada.txt\", \"medazul.txt\", \"medverde.txt\", \"medamarilla.txt\", \"mednegra.txt\"]\n",
    "triad_codes = {\n",
    "    (0, 2, 4): \"FRT\",  \n",
    "    (1, 3, 5): \"PLB\",  \n",
    "    (0, 3, 4): \"FLT\",  \n",
    "    (1, 2, 5): \"PRB\",  \n",
    "    (0, 2, 5): \"FRB\",  \n",
    "    (1, 3, 4): \"PLT\",  \n",
    "    (0, 3, 5): \"FLB\",  \n",
    "    (1, 2, 4): \"PRT\",  \n",
    "    (2, 4, 5): \"RTB\",  \n",
    "    (0, 1, 3): \"FLP\",  \n",
    "    (3, 4, 5): \"LTB\",  \n",
    "    (0, 1, 2): \"FRP\"\n",
    "}\n",
    "\n",
    "# Volumes for each lab\n",
    "volumes = {\n",
    "    \"Lab_Betta\": 7.3195 , # Volume for Lab A\n",
    "    \"Lab_Gamma\": 6.8342   # Volume for Lab B (example)\n",
    "}\n",
    "\n",
    "# Procesar todos los laboratorios y turnos\n",
    "labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "\n",
    "for lab_name in labs:\n",
    "    for shift in ['V', 'M']:\n",
    "        process_lab_accumulated_enstrophy(root_folder, lab_name, shift, file_suffixes, volumes[lab_name], triad_codes)\n",
    "\n",
    "print(\"Processing complete. All accumulated enstrophy results are saved in their respective directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{First Integral Moduli Baseline vs Experimental Color}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "def read_data(filepath):\n",
    "    timestamps = []\n",
    "    values = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                try:\n",
    "                    timestamp = datetime.datetime.strptime(parts[0], \"%H:%M:%S.%f\")\n",
    "                    timestamps.append(timestamp)\n",
    "                    values.append([float(num) for num in parts[1].strip('()').split(',')])\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing line in file {filepath}: {line}\\n{e}\")\n",
    "    return timestamps, np.array(values)\n",
    "\n",
    "def generate_combinations(baseline_files, experimental_files):\n",
    "    combinations = []\n",
    "    for base_file in baseline_files:\n",
    "        base_num = int(base_file.split('med')[0][-2:])\n",
    "        for exp_file in experimental_files:\n",
    "            exp_num = int(exp_file.split('med')[0][-2:])\n",
    "            if exp_num == base_num or exp_num == base_num - 1:\n",
    "                combinations.append((base_file, exp_file))\n",
    "            elif exp_num == base_num + 1 and (base_num + 1) <= len(baseline_files):\n",
    "                combinations.append((baseline_files[base_num], exp_file))\n",
    "    return combinations\n",
    "\n",
    "def read_curl_file(file_path):\n",
    "    \"\"\"Reads a file containing curl components and returns a list of tuples (i, j, k).\"\"\"\n",
    "    curl_components = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                try:\n",
    "                    components = tuple(map(float, parts[1].strip('()').split(',')))\n",
    "                    if len(components) == 3:\n",
    "                        curl_components.append(components)\n",
    "                except ValueError:\n",
    "                    continue  # Ignore lines that cannot be converted to floats\n",
    "    return curl_components\n",
    "\n",
    "def calculate_modulus(curl_components):\n",
    "    \"\"\"Calculates the modulus of the curl components.\"\"\"\n",
    "    moduli = [np.sqrt(i**2 + j**2 + k**2) for i, j, k in curl_components]\n",
    "    return moduli\n",
    "\n",
    "def integrate_moduli(moduli):\n",
    "    \"\"\"Calculates the sum (integral) of the moduli.\"\"\"\n",
    "    return np.sum(moduli)\n",
    "\n",
    "def compare_integrals(baseline_integral, intervention_integral):\n",
    "    \"\"\"Compares the integrals and reports the result.\"\"\"\n",
    "    difference = abs(intervention_integral - baseline_integral)\n",
    "    percentage_difference = (difference / baseline_integral) * 100\n",
    "\n",
    "    if percentage_difference > 34:\n",
    "        return \"There is intervention\"\n",
    "    elif 10 <= percentage_difference <= 33.9:\n",
    "        return \"Inconclusive\"\n",
    "    else:\n",
    "        return \"No evidence\"\n",
    "\n",
    "def print_result(lab, shift, baseline_file, experimental_file, baseline_integral, intervention_integral, result):\n",
    "    shift_name = \"Matutino\" if shift == \"M\" else \"Vespertino\"\n",
    "    print(f\"Lab: {lab}, Shift: {shift_name}\")\n",
    "    print(f\"Comparison Result\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"Baseline File: {os.path.basename(baseline_file)}\")\n",
    "    print(f\"Experimental File: {os.path.basename(experimental_file)}\")\n",
    "    print(f\"Baseline Integral: {baseline_integral:.2f}\")\n",
    "    print(f\"Intervention Integral: {intervention_integral:.2f}\")\n",
    "    print(f\"Difference: {abs(intervention_integral - baseline_integral):.2f}\")\n",
    "    print(f\"Percentage Difference: {(abs(intervention_integral - baseline_integral) / baseline_integral) * 100:.2f}%\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "def save_intervention_result(results_file, lab, shift, baseline_file, experimental_file, baseline_integral, intervention_integral, result):\n",
    "    shift_name = \"Matutino\" if shift == \"M\" else \"Vespertino\"\n",
    "    with open(results_file, 'a') as f:\n",
    "        f.write(f\"Lab: {lab}, Shift: {shift_name}\\n\")\n",
    "        f.write(f\"Comparison Result\\n\")\n",
    "        f.write(f\"{'-'*50}\\n\")\n",
    "        f.write(f\"Baseline File: {os.path.basename(baseline_file)}\\n\")\n",
    "        f.write(f\"Experimental File: {os.path.basename(experimental_file)}\\n\")\n",
    "        f.write(f\"Baseline Integral: {baseline_integral:.2f}\\n\")\n",
    "        f.write(f\"Intervention Integral: {intervention_integral:.2f}\\n\")\n",
    "        f.write(f\"Difference: {abs(intervention_integral - baseline_integral):.2f}\\n\")\n",
    "        f.write(f\"Percentage Difference: {(abs(intervention_integral - baseline_integral) / baseline_integral) * 100:.2f}%\\n\")\n",
    "        f.write(f\"Result: {result}\\n\")\n",
    "        f.write(f\"{'='*50}\\n\\n\")\n",
    "\n",
    "def main():\n",
    "    results_file = os.path.join(root_folder, \"intervention_results.txt\")\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        os.remove(results_file)  # Remove the file if it already exists\n",
    "\n",
    "    labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "    shifts = [\"V\", \"M\"]\n",
    "\n",
    "    color_map = {\n",
    "        \"medroja.txt\": \"red\",\n",
    "        \"medmorada.txt\": \"purple\",\n",
    "        \"medazul.txt\": \"blue\",\n",
    "        \"medverde.txt\": \"green\",\n",
    "        \"medamarilla.txt\": \"yellow\"\n",
    "    }\n",
    "\n",
    "    for lab in labs:\n",
    "        for shift in shifts:\n",
    "            lab_dir = glob.glob(os.path.join(root_folder, f\"*{lab}\"))[0]\n",
    "            date_str = os.path.basename(lab_dir).split(' ')[0]\n",
    "\n",
    "            baseline_dir = os.path.join(lab_dir, f\"{date_str}.{shift} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Curl_Baseline\")\n",
    "            exp_dir = os.path.join(lab_dir, f\"{date_str}.{shift} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Curl_Experimental_Color\")\n",
    "            \n",
    "            if not os.path.exists(baseline_dir) or not os.path.exists(exp_dir):\n",
    "                print(f\"Error: Directory not found for {lab} {shift}\")\n",
    "                continue\n",
    "\n",
    "            baseline_files = [f for f in os.listdir(baseline_dir) if f.endswith('mednegra.txt') and \"00\" not in f]\n",
    "            experimental_files = [f for f in os.listdir(exp_dir) if any(f.endswith(suffix) for suffix in color_map.keys()) and \"00\" not in f]\n",
    "\n",
    "            combinations = generate_combinations(baseline_files, experimental_files)\n",
    "\n",
    "            for base_file, exp_file in combinations:\n",
    "                baseline_file = os.path.join(baseline_dir, base_file)\n",
    "                experimental_file = os.path.join(exp_dir, exp_file)\n",
    "                if os.path.exists(baseline_file) and os.path.exists(experimental_file):\n",
    "                    # Reading and processing baseline file\n",
    "                    baseline_curl = read_curl_file(baseline_file)\n",
    "                    baseline_moduli = calculate_modulus(baseline_curl)\n",
    "                    baseline_integral = integrate_moduli(baseline_moduli)\n",
    "\n",
    "                    # Reading and processing intervention file\n",
    "                    intervention_curl = read_curl_file(experimental_file)\n",
    "                    intervention_moduli = calculate_modulus(intervention_curl)\n",
    "                    intervention_integral = integrate_moduli(intervention_moduli)\n",
    "\n",
    "                    # Comparing integrals\n",
    "                    result = compare_integrals(baseline_integral, intervention_integral)\n",
    "                    print_result(lab, shift, baseline_file, experimental_file, baseline_integral, intervention_integral, result)\n",
    "                    \n",
    "                    if result == \"There is intervention\":\n",
    "                        save_intervention_result(results_file, lab, shift, baseline_file, experimental_file, baseline_integral, intervention_integral, result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13\n",
    "\n",
    "$$\n",
    "\n",
    "    \\Huge \\text{Second Integral of Absolute Helicity Baseline vs Experimental Color}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def read_data(filepath):\n",
    "    values = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(' -> ')\n",
    "            if len(parts) > 1:\n",
    "                values.append(float(parts[1]))\n",
    "    return np.array(values)\n",
    "\n",
    "def get_sum_helicity(files, dir_path):\n",
    "    sum_helicities = {\"sum_absolute_helicity_1\": {}, \"sum_absolute_helicity_2\": {}}\n",
    "    for file in files:\n",
    "        file_path = os.path.join(dir_path, file)\n",
    "        values = read_data(file_path)\n",
    "        sum_helicity = np.sum(values)\n",
    "        if \"sum_absolute_helicity_1\" in file:\n",
    "            sum_helicities[\"sum_absolute_helicity_1\"][file] = sum_helicity\n",
    "        elif \"sum_absolute_helicity_2\" in file:\n",
    "            sum_helicities[\"sum_absolute_helicity_2\"][file] = sum_helicity\n",
    "    return sum_helicities\n",
    "\n",
    "def generate_combinations(baseline_files, experimental_files):\n",
    "    combinations = []\n",
    "    for base_file in baseline_files:\n",
    "        base_num = int(base_file.split('velocity_')[1].split('med')[0])\n",
    "        for exp_file in experimental_files:\n",
    "            exp_num = int(exp_file.split('velocity_')[1].split('med')[0])\n",
    "            if exp_num == base_num or exp_num == base_num - 1:\n",
    "                combinations.append((base_file, exp_file))\n",
    "            elif exp_num == base_num + 1 and (base_num + 1) <= len(baseline_files):\n",
    "                combinations.append((baseline_files[base_num], exp_file))\n",
    "    return combinations\n",
    "\n",
    "def calculate_percentage_difference(baseline_value, experimental_value):\n",
    "    return 100 * (experimental_value - baseline_value) / baseline_value\n",
    "\n",
    "def format_comparison_results(combinations, baseline_sums, experimental_sums, comparison_type):\n",
    "    comparison_lines = [f\"*** Comparison - {comparison_type} ***\\n\"]\n",
    "    for base_file, exp_file in combinations:\n",
    "        if base_file in baseline_sums[comparison_type] and exp_file in experimental_sums[comparison_type]:\n",
    "            baseline_value = baseline_sums[comparison_type][base_file]\n",
    "            experimental_value = experimental_sums[comparison_type][exp_file]\n",
    "            percentage_diff = calculate_percentage_difference(baseline_value, experimental_value)\n",
    "            comparison_lines.append(f\"Baseline: {base_file} - {baseline_value:.2f}\\n\")\n",
    "            comparison_lines.append(f\"Experimental: {exp_file} - {experimental_value:.2f}\\n\")\n",
    "            comparison_lines.append(f\"*** Percentage Difference: {percentage_diff:.2f}% ***\\n\\n\")\n",
    "    return comparison_lines\n",
    "\n",
    "def format_statistical_summary(baseline_sums, experimental_sums):\n",
    "    summary_lines = [\"*** Statistical Summary ***\\n\"]\n",
    "    for helicity_type in baseline_sums:\n",
    "        baseline_values = list(baseline_sums[helicity_type].values())\n",
    "        experimental_values = list(experimental_sums[helicity_type].values())\n",
    "\n",
    "        # Verifica si las listas están vacías\n",
    "        if not baseline_values:\n",
    "            summary_lines.append(f\"{helicity_type} Baseline: No data\\n\")\n",
    "        else:\n",
    "            max_baseline_value = max(baseline_values)\n",
    "            min_baseline_value = min(baseline_values)\n",
    "            max_baseline_file = [k for k, v in baseline_sums[helicity_type].items() if v == max_baseline_value][0]\n",
    "            min_baseline_file = [k for k, v in baseline_sums[helicity_type].items() if v == min_baseline_value][0]\n",
    "            summary_lines.append(f\"{helicity_type}:\\n\")\n",
    "            summary_lines.append(f\"  Baseline - Mean: {np.mean(baseline_values):.2f}, Std Dev: {np.std(baseline_values):.2f}\\n\")\n",
    "            summary_lines.append(f\"    Max: {max_baseline_value:.2f} ({max_baseline_file})\\n\")\n",
    "            summary_lines.append(f\"    Min: {min_baseline_value:.2f} ({min_baseline_file})\\n\")\n",
    "\n",
    "        # Verifica si las listas están vacías para datos experimentales\n",
    "        if not experimental_values:\n",
    "            summary_lines.append(f\"{helicity_type} Experimental: No data\\n\")\n",
    "        else:\n",
    "            max_experimental_value = max(experimental_values)\n",
    "            min_experimental_value = min(experimental_values)\n",
    "            max_experimental_file = [k for k, v in experimental_sums[helicity_type].items() if v == max_experimental_value][0]\n",
    "            min_experimental_file = [k for k, v in experimental_sums[helicity_type].items() if v == min_experimental_value][0]\n",
    "            summary_lines.append(f\"  Experimental - Mean: {np.mean(experimental_values):.2f}, Std Dev: {np.std(experimental_values):.2f}\\n\")\n",
    "            summary_lines.append(f\"    Max: {max_experimental_value:.2f} ({max_experimental_file})\\n\")\n",
    "            summary_lines.append(f\"    Min: {min_experimental_value:.2f} ({min_experimental_file})\\n\")\n",
    "\n",
    "        summary_lines.append(\"\\n\")\n",
    "\n",
    "    return summary_lines\n",
    "\n",
    "def main():\n",
    "    root_folder = \"path/to/root_folder\"  # Define la ruta de la carpeta raíz\n",
    "    labs = [\"Lab_Betta\", \"Lab_Gamma\"]\n",
    "    shifts = [\"V\", \"M\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lab in labs:\n",
    "        for shift in shifts:\n",
    "            lab_dirs = glob.glob(os.path.join(root_folder, f\"*{lab}\"))\n",
    "            if not lab_dirs:\n",
    "                print(f\"No se encontró ningún directorio para {lab}.\")\n",
    "                continue\n",
    "            lab_dir = lab_dirs[0]\n",
    "            date_str = os.path.basename(lab_dir).split(' ')[0]\n",
    "            baseline_dir = os.path.join(lab_dir, f\"{date_str}.{shift} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Absolute_Helicity_Baseline\")\n",
    "            exp_dir = os.path.join(lab_dir, f\"{date_str}.{shift} - {lab}\", \"Data Analysis\", \"Processing Data\", \"Sum_Absolute_Helicity_Experimental_Color\")\n",
    "\n",
    "            if not os.path.exists(baseline_dir) or not os.path.exists(exp_dir):\n",
    "                print(f\"Omitiendo cálculo para {lab} {shift} ya que el directorio no existe.\")\n",
    "                continue\n",
    "\n",
    "            baseline_files = [f for f in os.listdir(baseline_dir) if f.endswith('mednegra.txt') and \"00\" not in f]\n",
    "            experimental_files = [f for f in os.listdir(exp_dir) if f.endswith('.txt') and \"00\" not in f]\n",
    "\n",
    "            baseline_sums = get_sum_helicity(baseline_files, baseline_dir)\n",
    "            experimental_sums = get_sum_helicity(experimental_files, exp_dir)\n",
    "\n",
    "            header = f\"\\n\\n*** Resultados para {lab} - Turno: {'Vespertino' if shift == 'V' else 'Matutino'} - Fecha: {date_str} ***\\n\"\n",
    "            results.append(header)\n",
    "            results.append(\"=\" * len(header) + \"\\n\\n\")\n",
    "\n",
    "            # Generar combinaciones y formatear resultados de comparación\n",
    "            combinations_1 = generate_combinations(baseline_files, experimental_files)\n",
    "            results.extend(format_comparison_results(combinations_1, baseline_sums, experimental_sums, \"sum_absolute_helicity_1\"))\n",
    "            results.extend(format_comparison_results(combinations_1, baseline_sums, experimental_sums, \"sum_absolute_helicity_2\"))\n",
    "\n",
    "            # Añadir resumen estadístico\n",
    "            results.append(\"*** Statistical Summary ***\\n\")\n",
    "            results.extend(format_statistical_summary(baseline_sums, experimental_sums))\n",
    "            results.append(\"=\" * len(header) + \"\\n\\n\")\n",
    "\n",
    "    output_filepath = os.path.join(root_folder, \"SUM TOTAL OF HELICITY BY TRIAD COMPONENTS.txt\")\n",
    "    with open(output_filepath, 'w') as output_file:\n",
    "        output_file.writelines(results)\n",
    "\n",
    "    print(f\"Resultados guardados en {output_filepath}\")\n",
    "\n",
    "    # Imprimir resultados en consola\n",
    "    for line in results:\n",
    "        print(line, end='')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando para Lab: Betta, Turno: M, Color: Amarilla, Archivo: 01\n",
      "Archivo a procesar: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\01medamarilla-pesajes.log\n",
      "Error decodificando: non-hexadecimal number found in fromhex() arg at position 0\n",
      "\n",
      "Procesando para Lab: Betta, Turno: M, Color: Amarilla, Archivo: 02\n",
      "Archivo a procesar: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\02medamarilla-pesajes.log\n",
      "Error decodificando: non-hexadecimal number found in fromhex() arg at position 0\n",
      "\n",
      "Procesando para Lab: Betta, Turno: M, Color: Amarilla, Archivo: 03\n",
      "Archivo a procesar: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\03medamarilla-pesajes.log\n",
      "Error decodificando: non-hexadecimal number found in fromhex() arg at position 0\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\04medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\05medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\06medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\07medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\08medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\09medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Amarilla\\pesajes\\10medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\01medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\02medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\03medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\04medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\05medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\06medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\07medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\08medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\09medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Roja\\pesajes\\10medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\01medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\02medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\03medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\04medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\05medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\06medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\07medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\08medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\09medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Azul\\pesajes\\10medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\01medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\02medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\03medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\04medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\05medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\06medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\07medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\08medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\09medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Verde\\pesajes\\10medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\01medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\02medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\03medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\04medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\05medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\06medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\07medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\08medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\09medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Morada\\pesajes\\10medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\01medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\02medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\03medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\04medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\05medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\06medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\07medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\08medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\09medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Amarilla\\pesajes\\10medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\01medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\02medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\03medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\04medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\05medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\06medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\07medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\08medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\09medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Roja\\pesajes\\10medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\01medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\02medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\03medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\04medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\05medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\06medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\07medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\08medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\09medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Azul\\pesajes\\10medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\01medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\02medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\03medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\04medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\05medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\06medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\07medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\08medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\09medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Verde\\pesajes\\10medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\01medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\02medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\03medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\04medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\05medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\06medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\07medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\08medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\09medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Morada\\pesajes\\10medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\01medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\02medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\03medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\04medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\05medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\06medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\07medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\08medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\09medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Amarilla\\pesajes\\10medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\01medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\02medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\03medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\04medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\05medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\06medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\07medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\08medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\09medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Roja\\pesajes\\10medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\01medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\02medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\03medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\04medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\05medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\06medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\07medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\08medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\09medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Azul\\pesajes\\10medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\01medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\02medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\03medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\04medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\05medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\06medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\07medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\08medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\09medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Verde\\pesajes\\10medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\01medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\02medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\03medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\04medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\05medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\06medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\07medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\08medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\09medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Morada\\pesajes\\10medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\01medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\02medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\03medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\04medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\05medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\06medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\07medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\08medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\09medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Amarilla\\pesajes\\10medamarilla-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\01medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\02medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\03medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\04medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\05medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\06medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\07medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\08medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\09medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Roja\\pesajes\\10medroja-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\01medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\02medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\03medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\04medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\05medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\06medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\07medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\08medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\09medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Azul\\pesajes\\10medazul-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\01medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\02medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\03medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\04medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\05medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\06medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\07medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\08medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\09medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Verde\\pesajes\\10medverde-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\01medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\02medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\03medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\04medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\05medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\06medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\07medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\08medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\09medmorada-pesajes.log\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Morada\\pesajes\\10medmorada-pesajes.log\n",
      "\n",
      "Procesamiento completo.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Función para decodificar el texto hexadecimal\n",
    "def decodificar_hex(hex_string):\n",
    "    try:\n",
    "        hex_cleaned = hex_string.strip().replace(' ', '')\n",
    "        decoded_string = bytes.fromhex(hex_cleaned).decode('ascii', errors='ignore').strip()\n",
    "        return decoded_string\n",
    "    except Exception as e:\n",
    "        print(f\"Error decodificando: {e}\")\n",
    "        return None\n",
    "\n",
    "# Función para procesar el archivo y extraer la hora y el valor limpio\n",
    "def procesar_archivo_para_hora_y_texto(ruta_archivo):\n",
    "    resultados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            match_hora = re.search(r'(\\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2}:\\d{2}\\.\\d{3})', linea)\n",
    "            if match_hora:\n",
    "                hora = match_hora.group(1).split(' ')[1]  # Obtener solo la parte de la hora\n",
    "                partes = linea.strip().split(',')\n",
    "                if len(partes) > 3:\n",
    "                    hex_data = partes[-1]\n",
    "                    decoded_value = decodificar_hex(hex_data)\n",
    "\n",
    "                    if decoded_value:\n",
    "                        resultados.append(f\"{hora}, {decoded_value}\")\n",
    "                    else:\n",
    "                        resultados.append(f\"{hora}, .\")\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Función para guardar los resultados en un archivo de salida\n",
    "def guardar_resultados(ruta_salida, resultados):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        for resultado in resultados:\n",
    "            archivo_salida.write(resultado + '\\n')\n",
    "\n",
    "# Función para construir la ruta del archivo a procesar\n",
    "def construir_ruta(root_folder, fecha, lab, turno, color, numero):\n",
    "    colorin = color.lower()\n",
    "    return os.path.join(root_folder, f\"{fecha} - Lab_{lab}\", f\"{fecha}.{turno} - Lab_{lab}\", f\"{color}\", \"pesajes\", f\"{numero:02d}med{colorin}-pesajes.log\")\n",
    "\n",
    "# Función para filtrar resultados\n",
    "def filtrar_resultados(resultados):\n",
    "    # Expresión regular para el formato deseado\n",
    "    pattern = r'^\\d{2}:\\d{2}:\\d{2}\\.\\d{3}, \\d+\\.\\d{2}g$'\n",
    "    return [resultado for resultado in resultados if re.match(pattern, resultado.strip())]\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = r\"F:\\Datos\\16Sep24\"\n",
    "    fecha = extraer_fecha(root_folder)  # Asegúrate de definir esta función\n",
    "    labs = [\"Betta\", \"Gamma\"]\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    ruta_archivo = construir_ruta(root_folder, fecha, lab, turno, color, numero)\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        ruta_salida = os.path.join(os.path.dirname(ruta_archivo), f\"resultados_{numero:02d}.txt\")\n",
    "\n",
    "                        print(f\"\\nProcesando para Lab: {lab}, Turno: {turno}, Color: {color}, Archivo: {numero:02d}\")\n",
    "                        print(f\"Archivo a procesar: {ruta_archivo}\")\n",
    "                        \n",
    "                        # Procesar el archivo para extraer hora y texto limpio\n",
    "                        resultados = procesar_archivo_para_hora_y_texto(ruta_archivo)\n",
    "\n",
    "                        # Filtrar resultados\n",
    "                        resultados_filtrados = filtrar_resultados(resultados)\n",
    "\n",
    "                        # Guardar los resultados filtrados en el archivo de salida\n",
    "                        guardar_resultados(ruta_salida, resultados_filtrados)\n",
    "                    else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\01medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\02medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\03medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\04medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\05medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\06medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\07medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\08medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\09medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\010medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\01medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\02medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\03medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\04medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\05medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\06medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\07medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\08medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\09medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\010medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\01medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\02medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\03medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\04medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\05medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\06medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\07medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\08medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\09medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\010medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\01medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\02medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\03medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\04medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\05medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\06medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\07medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\08medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\09medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\010medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\01medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\02medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\03medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\04medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\05medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\06medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\07medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\08medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\09medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.M - Lab_Betta\\Pesajes\\010medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\01medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\02medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\03medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\04medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\05medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\06medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\07medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\08medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\09medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\010medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\01medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\02medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\03medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\04medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\05medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\06medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\07medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\08medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\09medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\010medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\01medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\02medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\03medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\04medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\05medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\06medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\07medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\08medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\09medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\010medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\01medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\02medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\03medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\04medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\05medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\06medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\07medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\08medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\09medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\010medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\01medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\02medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\03medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\04medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\05medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\06medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\07medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\08medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\09medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Betta\\16Sep24.V - Lab_Betta\\Pesajes\\010medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\01medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\02medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\03medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\04medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\05medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\06medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\07medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\08medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\09medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\010medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\01medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\02medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\03medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\04medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\05medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\06medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\07medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\08medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\09medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\010medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\01medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\02medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\03medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\04medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\05medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\06medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\07medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\08medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\09medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\010medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\01medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\02medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\03medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\04medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\05medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\06medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\07medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\08medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\09medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\010medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\01medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\02medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\03medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\04medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\05medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\06medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\07medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\08medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\09medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.M - Lab_Gamma\\Pesajes\\010medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\01medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\02medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\03medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\04medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\05medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\06medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\07medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\08medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\09medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\010medamarilla.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\01medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\02medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\03medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\04medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\05medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\06medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\07medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\08medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\09medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\010medroja.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\01medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\02medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\03medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\04medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\05medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\06medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\07medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\08medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\09medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\010medazul.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\01medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\02medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\03medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\04medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\05medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\06medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\07medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\08medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\09medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\010medverde.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\01medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\02medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\03medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\04medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\05medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\06medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\07medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\08medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\09medmorada.txt\n",
      "Archivo no encontrado: F:\\Datos\\16Sep24\\16Sep24 - Lab_Gamma\\16Sep24.V - Lab_Gamma\\Pesajes\\010medmorada.txt\n",
      "\n",
      "Procesamiento completo.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Función para extraer la fecha del nombre del directorio raíz\n",
    "def extraer_fecha(root_folder):\n",
    "    nombre_directorio = os.path.basename(root_folder)\n",
    "    match = re.search(r'(\\d{2}[A-Za-z]{3}\\d{2})', nombre_directorio)\n",
    "    \n",
    "    if match:\n",
    "        fecha_str = match.group(1)  # Captura la fecha en formato '16Sep24'\n",
    "        return fecha_str\n",
    "    else:\n",
    "        raise ValueError(f\"No se encontró una fecha válida en el nombre del directorio {root_folder}.\")\n",
    "\n",
    "# Función para cargar y procesar los datos\n",
    "def cargar_y_procesar_datos(ruta_archivo):\n",
    "    datos_procesados = []\n",
    "    \n",
    "    with open(ruta_archivo, 'r') as archivo:\n",
    "        for linea in archivo:\n",
    "            partes = linea.split(',')\n",
    "            hora = partes[0].strip()  # Extrae la hora\n",
    "            valor = partes[1].strip()  # Extrae el valor con 'g'\n",
    "            \n",
    "            # Elimina la 'g' y convierte a float\n",
    "            valor_numero = float(valor.replace('g', ''))\n",
    "            # Aplica el tratamiento Dato * 9.81 / 5.00\n",
    "            valor_procesado = valor_numero * 9.81 / 5.00\n",
    "            \n",
    "            # Guarda la hora y el valor procesado\n",
    "            datos_procesados.append(f\"{hora}, {valor_procesado:.2f}\\n\")\n",
    "    \n",
    "    return datos_procesados\n",
    "\n",
    "# Función para guardar los datos procesados en un archivo .txt\n",
    "def guardar_datos_txt(datos_procesados, ruta_salida):\n",
    "    with open(ruta_salida, 'w') as archivo_salida:\n",
    "        archivo_salida.writelines(datos_procesados)\n",
    "    print(f\"Datos procesados guardados en: {ruta_salida}\")\n",
    "\n",
    "# Función principal para procesar y guardar archivos\n",
    "def procesar_archivos(root_folder):\n",
    "    labs = [\"Betta\", \"Gamma\"]\n",
    "    turnos = [\"M\", \"V\"]\n",
    "    colores = [\"Amarilla\", \"Roja\", \"Azul\", \"Verde\", \"Morada\"]\n",
    "    numeros_archivo = range(1, 11)\n",
    "\n",
    "    fecha = extraer_fecha(root_folder)\n",
    "\n",
    "    for lab in labs:\n",
    "        for turno in turnos:\n",
    "            for color in colores:\n",
    "                for numero in numeros_archivo:\n",
    "                    colorin = color.lower()\n",
    "                    ruta_archivo = os.path.join(root_folder, f\"{fecha} - Lab_{lab}\", f\"{fecha}.{turno} - Lab_{lab}\", \"Pesajes\", f\"0{numero}med{colorin}.txt\")\n",
    "                    \n",
    "                    if os.path.exists(ruta_archivo):\n",
    "                        # Cargar y procesar los datos del archivo\n",
    "                        datos_procesados = cargar_y_procesar_datos(ruta_archivo)\n",
    "\n",
    "                        # Definir la carpeta y el nombre del archivo de salida\n",
    "                        output_folder = os.path.join(root_folder, f\"{fecha} - Lab_{lab}\", f\"{fecha}.{turno} - Lab_{lab}\", \"pesajes\", \"procesados\")\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ruta_salida = os.path.join(output_folder, f\"resultados_0{numero}_{colorin}.txt\")\n",
    "\n",
    "                        # Guardar los datos procesados en un archivo .txt\n",
    "                        guardar_datos_txt(datos_procesados, ruta_salida)\n",
    "                    else:\n",
    "                        print(f\"Archivo no encontrado: {ruta_archivo}\")\n",
    "\n",
    "# Si este es el archivo principal\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_archivos(root_folder)\n",
    "    print(\"\\nProcesamiento completo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
